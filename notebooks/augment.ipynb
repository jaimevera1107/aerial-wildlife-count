{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f640110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import itertools\n",
    "import threading\n",
    "from concurrent.futures import as_completed\n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Got processor for bboxes, but no transform to process it\")\n",
    "\n",
    "class AugmentationPipeline:\n",
    "    def __init__(self, config: dict, split: str = \"train\", logger=None):\n",
    "        \"\"\"\n",
    "        Initialize configuration, directory structure, and parameters\n",
    "        for the modular augmentation pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # LOGGER SETUP\n",
    "        # ==========================================================\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.version = \"v2.4.0\"  # updated version marker\n",
    "\n",
    "        log_cfg = config.get(\"logging\", {})\n",
    "        log_level = getattr(logging, log_cfg.get(\"level\", \"INFO\").upper(), logging.INFO)\n",
    "\n",
    "        self.logger = logger or logging.getLogger(f\"AugmentationPipeline.{split}\")\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\", \"%H:%M:%S\")\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "        self.logger.propagate = False\n",
    "\n",
    "        # ==========================================================\n",
    "        # PATH CONFIGURATION\n",
    "        # ==========================================================\n",
    "        self.root_dir = Path(config.get(\"output_dir\", \"data/outputs\")).resolve()\n",
    "        self.mirror_dir = self.root_dir / \"mirror_clean\"\n",
    "\n",
    "        paths_cfg = config.get(\"paths\", {})\n",
    "\n",
    "        # --- Base clean dataset ---\n",
    "        self.clean_dir = Path(paths_cfg.get(\"clean_dir\", self.mirror_dir / f\"{split}_clean\")).resolve()\n",
    "        self.clean_json = Path(paths_cfg.get(\"clean_json\", self.clean_dir / f\"{split}_clean.json\")).resolve()\n",
    "\n",
    "        # --- JSON fallback detection ---\n",
    "        if not self.clean_json.exists():\n",
    "            fallback_candidates = [\n",
    "                self.clean_dir / f\"{split}_joined.json\",\n",
    "                self.clean_dir / f\"{split}.json\"\n",
    "            ]\n",
    "            found = next((p for p in fallback_candidates if p.exists()), None)\n",
    "            if found:\n",
    "                self.clean_json = found\n",
    "                self.logger.info(f\"[INIT] Using fallback JSON -> {found}\")\n",
    "            else:\n",
    "                json_candidates = list(self.clean_dir.glob(\"*.json\"))\n",
    "                if len(json_candidates) == 1:\n",
    "                    self.clean_json = json_candidates[0]\n",
    "                    self.logger.info(f\"[INIT] Auto-detected clean JSON -> {self.clean_json}\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"[INIT] Missing clean JSON file in {self.clean_dir}\")\n",
    "\n",
    "        if not self.clean_dir.exists():\n",
    "            raise FileNotFoundError(f\"[INIT] Missing clean image directory: {self.clean_dir}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # DIRECTORY STRUCTURE (Standardized for all stages)\n",
    "        # ==========================================================\n",
    "        self.prop_dir = self.mirror_dir / f\"{split}_prop\"\n",
    "        self.rebalance_dir = self.mirror_dir / f\"{split}_rebalance_1\"\n",
    "        self.zoom_dir = self.mirror_dir / f\"{split}_zoom\"\n",
    "        self.final_dir = self.mirror_dir / f\"{split}_final\"\n",
    "\n",
    "        for d in [self.prop_dir, self.rebalance_dir, self.zoom_dir, self.final_dir]:\n",
    "            d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --- Reports directory ---\n",
    "        self.output_dir = self.root_dir / \"reports\"\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ==========================================================\n",
    "        # JSON PATHS PER STAGE\n",
    "        # ==========================================================\n",
    "        self.prop_json = self.prop_dir / f\"{split}_prop.json\"\n",
    "        self.rebalance_json = self.rebalance_dir / f\"{split}_rebalance_1.json\"\n",
    "        self.zoom_json = self.zoom_dir / f\"{split}_zoom.json\"\n",
    "        self.final_json = self.final_dir / f\"{split}_final.json\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # AUGMENTATION PARAMETERS\n",
    "        # ==========================================================\n",
    "        aug_cfg = config.get(\"augmentations\", {})\n",
    "\n",
    "        def _to_float(val, default):\n",
    "            try:\n",
    "                return float(val)\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        def _to_int(val, default):\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        self.mode = aug_cfg.get(\"mode\", \"none\")\n",
    "        self.proportion = _to_float(aug_cfg.get(\"proportion\", 0.25), 0.25)\n",
    "        self.tolerance = _to_float(aug_cfg.get(\"tolerance\", 0.10), 0.10)\n",
    "        self.max_repeats = _to_int(aug_cfg.get(\"max_repeats\", 10), 10)\n",
    "        self.min_unique_ratio = _to_float(aug_cfg.get(\"min_unique_ratio\", 0.4), 0.4)\n",
    "        self.max_boxes_per_image = _to_int(aug_cfg.get(\"max_boxes_per_image\", 100), 100)\n",
    "        self.min_images_per_class = _to_int(aug_cfg.get(\"min_images_per_class\", 10), 10)\n",
    "        self.transforms_cfg = aug_cfg.get(\"transforms\", {})\n",
    "        self.seed = _to_int(config.get(\"seed\", 42), 42)\n",
    "        self.num_workers = _to_int(config.get(\"num_workers\", 4), 4)\n",
    "\n",
    "        # ==========================================================\n",
    "        # CLASS SETTINGS\n",
    "        # ==========================================================\n",
    "        self.classes = list(dict.fromkeys(config.get(\"classes\", [])))  # remove duplicates, preserve order\n",
    "        if not self.classes:\n",
    "            raise ValueError(\"[INIT] No classes defined in configuration (config['classes']).\")\n",
    "\n",
    "        self.minority_strategy = aug_cfg.get(\"minority_strategy\", \"auto\")  # \"auto\" or \"bottom_n\"\n",
    "        self.bottom_n = _to_int(aug_cfg.get(\"bottom_n\", 2), 2)\n",
    "\n",
    "        if not (0 <= self.proportion <= 1):\n",
    "            raise ValueError(f\"[INIT] Invalid proportion value: {self.proportion}\")\n",
    "        if not (0 <= self.tolerance <= 1):\n",
    "            raise ValueError(f\"[INIT] Invalid tolerance value: {self.tolerance}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # INTERNAL STATE\n",
    "        # ==========================================================\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.categories = []\n",
    "        self.class_counts = {}\n",
    "        self.pipeline = None\n",
    "        self.sampling_plan = {}\n",
    "        self.rebalance_plan = {}\n",
    "        self.history = {}\n",
    "\n",
    "        adaptive_flag = aug_cfg.get(\"adaptive_rebalance\", False)\n",
    "        self.logger.info(f\"Adaptive rebalance={'ON' if adaptive_flag else 'OFF'}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # FINAL VALIDATION LOG\n",
    "        # ==========================================================\n",
    "        self.logger.info(\"[INIT] Directory structure validated and configuration is consistent.\")\n",
    "        self.logger.info(f\"[INIT] AugmentationPipeline ({self.version}) initialized for split='{split}'.\")\n",
    "        self.logger.info(\n",
    "            f\"Mode={self.mode} | Proportion={self.proportion} | Tolerance={self.tolerance} | \"\n",
    "            f\"max_repeats={self.max_repeats} | min_unique_ratio={self.min_unique_ratio} | \"\n",
    "            f\"max_boxes_per_image={self.max_boxes_per_image}\"\n",
    "        )\n",
    "\n",
    "    def _filter_minority_images(self, anns: list[dict], cats: dict[int, str], minority_classes: list[str]) -> set[int]:\n",
    "        \"\"\"\n",
    "        Identify and return image IDs that contain at least one annotation\n",
    "        of any minority class. These are the only images eligible for the\n",
    "        rebalance augmentation stage.\n",
    "        \"\"\"\n",
    "        if not anns:\n",
    "            self.logger.warning(f\"[FILTER] ({self.split}) Empty annotation list. Returning empty set.\")\n",
    "            return set()\n",
    "        if not cats:\n",
    "            self.logger.warning(f\"[FILTER] ({self.split}) Empty category mapping. Returning empty set.\")\n",
    "            return set()\n",
    "        if not minority_classes:\n",
    "            self.logger.info(f\"[FILTER] ({self.split}) No minority classes provided. Nothing to filter.\")\n",
    "            return set()\n",
    "\n",
    "        # --- Normalize class names (case-insensitive match) ---\n",
    "        minority_norm = {c.lower().strip() for c in minority_classes}\n",
    "        minority_cat_ids = {\n",
    "            cid for cid, cname in cats.items()\n",
    "            if cname and cname.lower().strip() in minority_norm\n",
    "        }\n",
    "\n",
    "        if not minority_cat_ids:\n",
    "            self.logger.warning(f\"[FILTER] ({self.split}) No matching category IDs found for minority classes.\")\n",
    "            return set()\n",
    "\n",
    "        # --- Select eligible image IDs ---\n",
    "        eligible_img_ids = {\n",
    "            a[\"image_id\"]\n",
    "            for a in anns\n",
    "            if \"image_id\" in a and a.get(\"category_id\") in minority_cat_ids\n",
    "        }\n",
    "\n",
    "        # --- Reporting ---\n",
    "        n_imgs = len(eligible_img_ids)\n",
    "        n_min = len(minority_classes)\n",
    "\n",
    "        if n_imgs == 0:\n",
    "            self.logger.warning(f\"[FILTER] ({self.split}) No eligible images found for {n_min} minority classes.\")\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"[FILTER] ({self.split}) Eligible images for rebalance: {n_imgs} \"\n",
    "                f\"(contain â‰¥1 of {n_min} minority classes).\"\n",
    "            )\n",
    "\n",
    "        self.logger.debug(f\"[FILTER] Minority classes ({n_min}): {minority_classes}\")\n",
    "        return eligible_img_ids\n",
    "\n",
    "    def summarize_balance_from_json(self, json_path: Path) -> dict:\n",
    "        \"\"\"\n",
    "        Compute class balance summary directly from a COCO-style JSON file.\n",
    "        \"\"\"\n",
    "        if not Path(json_path).exists():\n",
    "            raise FileNotFoundError(f\"[SUMMARY] JSON file not found: {json_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"[SUMMARY] Failed to read {json_path}: {e}\")\n",
    "\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "        if not anns or not cats:\n",
    "            raise ValueError(\"[SUMMARY] Invalid or empty dataset structure.\")\n",
    "\n",
    "        # Count per class\n",
    "        counts = {cname: 0 for cname in cats.values()}\n",
    "        for ann in anns:\n",
    "            cname = cats.get(ann[\"category_id\"])\n",
    "            if cname:\n",
    "                counts[cname] += 1\n",
    "\n",
    "        total = sum(counts.values())\n",
    "        if total == 0:\n",
    "            raise RuntimeError(\"[SUMMARY] No annotations found in dataset.\")\n",
    "\n",
    "        # Stats\n",
    "        max_c = max(counts.values())\n",
    "        min_c = min(counts.values())\n",
    "        mean_c = np.mean(list(counts.values()))\n",
    "        std_c = np.std(list(counts.values()))\n",
    "        ratio = round(max_c / max(min_c, 1), 2)\n",
    "        cv = round(std_c / max(mean_c, 1e-9), 3)\n",
    "        deviation = {k: round((v - mean_c) / mean_c, 3) for k, v in counts.items()}\n",
    "        tol = float(getattr(self, \"tolerance\", 0.1))\n",
    "        within_tol = np.mean(np.abs(list(deviation.values()))) <= tol\n",
    "\n",
    "        # Detect minority / majority\n",
    "        threshold = mean_c * (1 - tol)\n",
    "        minority = [k for k, v in counts.items() if v < threshold]\n",
    "\n",
    "        self.logger.info(f\"[SUMMARY] Class balance summary for {json_path.name}:\")\n",
    "        self.logger.info(f\"       -> Max={max_c} | Min={min_c} | Mean={mean_c:.1f} | Ratio={ratio:.2f}\")\n",
    "        self.logger.info(f\"       -> Std={std_c:.1f} | CV={cv:.3f} | Within tol={within_tol}\")\n",
    "\n",
    "        for cls, count in sorted(counts.items(), key=lambda x: -x[1]):\n",
    "            delta = ((max_c - count) / max_c) * 100\n",
    "            mark = \"OK\" if cls not in minority else \"WARNING\"\n",
    "            self.logger.info(f\"   - {cls:<15} {count:>6} Î”={delta:5.1f}% {mark}\")\n",
    "\n",
    "        return {\n",
    "            \"counts\": counts,\n",
    "            \"max_count\": max_c,\n",
    "            \"min_count\": min_c,\n",
    "            \"mean_count\": round(mean_c, 2),\n",
    "            \"std_count\": round(std_c, 2),\n",
    "            \"cv\": cv,\n",
    "            \"ratio\": ratio,\n",
    "            \"minority_classes\": minority,\n",
    "            \"within_tolerance\": within_tol,\n",
    "            \"total_annotations\": total,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    def load_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load and validate COCO-style JSON with image and annotation metadata.\n",
    "        Ensures internal consistency between image IDs and annotations.\n",
    "        \"\"\"\n",
    "        if not self.clean_json.exists():\n",
    "            raise FileNotFoundError(f\"[LOAD] JSON not found: {self.clean_json}\")\n",
    "        try:\n",
    "            with open(self.clean_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            size_kb = round(Path(self.clean_json).stat().st_size / 1024, 1)\n",
    "            self.logger.info(f\"[LOAD] Loaded JSON ({size_kb} KB) from {self.clean_json}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"[LOAD] Invalid JSON format: {e}\")\n",
    "\n",
    "        # --- Extract core fields ---\n",
    "        self.images = data.get(\"images\", [])\n",
    "        self.annotations = data.get(\"annotations\", [])\n",
    "        self.categories = data.get(\"categories\", [])\n",
    "\n",
    "        if not self.images:\n",
    "            self.logger.warning(f\"[LOAD] No images found in JSON ({self.clean_json}).\")\n",
    "        if not self.categories:\n",
    "            self.logger.warning(f\"[LOAD] No categories found in JSON ({self.clean_json}).\")\n",
    "\n",
    "        # --- Validate internal links ---\n",
    "        valid_ids = {img.get(\"id\") for img in self.images if \"id\" in img}\n",
    "        before = len(self.annotations)\n",
    "        self.annotations = [a for a in self.annotations if a.get(\"image_id\") in valid_ids]\n",
    "        removed = before - len(self.annotations)\n",
    "\n",
    "        # --- Build class maps ---\n",
    "        self.id2class = {c[\"id\"]: c[\"name\"] for c in self.categories if \"id\" in c and \"name\" in c}\n",
    "        self.class2id = {v: k for k, v in self.id2class.items()}\n",
    "\n",
    "        if not self.id2class:\n",
    "            raise ValueError(\"[LOAD] No valid categories with 'id' and 'name' fields found.\")\n",
    "\n",
    "        # --- Detect inconsistencies between config and JSON ---\n",
    "        missing_in_cfg = set(self.id2class.values()) - set(self.classes)\n",
    "        extra_in_cfg = set(self.classes) - set(self.id2class.values())\n",
    "        if missing_in_cfg:\n",
    "            self.logger.warning(f\"[LOAD] Classes in JSON not listed in config: {missing_in_cfg}\")\n",
    "        if extra_in_cfg:\n",
    "            self.logger.info(f\"[LOAD] Classes in config not present in JSON: {extra_in_cfg}\")\n",
    "\n",
    "        # --- Count annotations per class ---\n",
    "        self.class_counts = {cls: 0 for cls in self.classes}\n",
    "        for ann in self.annotations:\n",
    "            cname = self.id2class.get(ann.get(\"category_id\"))\n",
    "            if cname in self.class_counts:\n",
    "                self.class_counts[cname] += 1\n",
    "            else:\n",
    "                self.logger.debug(f\"[LOAD] Unknown category_id={ann.get('category_id')} found in annotations.\")\n",
    "\n",
    "        # --- Integrity checks ---\n",
    "        unique_image_ids = len(valid_ids)\n",
    "        unique_ann_ids = len({a[\"id\"] for a in self.annotations if \"id\" in a})\n",
    "\n",
    "        if unique_ann_ids < len(self.annotations):\n",
    "            self.logger.warning(\"[LOAD] Duplicate annotation IDs detected.\")\n",
    "        if len(valid_ids) < len(self.images):\n",
    "            self.logger.warning(\"[LOAD] Duplicate image IDs detected.\")\n",
    "        if removed > 0:\n",
    "            self.logger.warning(f\"[LOAD] Removed {removed} orphan annotations (invalid image references).\")\n",
    "            \n",
    "        n_imgs = len(self.images)\n",
    "        n_anns = len(self.annotations)\n",
    "        n_cls = len(self.categories)\n",
    "        self.logger.info(f\"[LOAD] Split '{self.split}' loaded successfully -> {n_imgs} images, {n_anns} annotations, {n_cls} classes.\")\n",
    "\n",
    "        return {\n",
    "            \"images\": n_imgs,\n",
    "            \"annotations\": n_anns,\n",
    "            \"classes\": n_cls,\n",
    "            \"unique_images\": unique_image_ids,\n",
    "            \"unique_annotations\": unique_ann_ids,\n",
    "            \"removed_orphans\": removed\n",
    "        }\n",
    "\n",
    "    def analyze_class_distribution(self, plot: bool = True) -> dict | None:\n",
    "        \"\"\"\n",
    "        Compute and optionally visualize annotation counts per class.\n",
    "        Returns a summary dict or None if empty.\n",
    "        \"\"\"\n",
    "        # --- Ensure data is loaded ---\n",
    "        if not self.class_counts:\n",
    "            self.logger.warning(f\"[ANALYZE] Class counts not initialized. Reloading split='{self.split}'.\")\n",
    "            self.load_data()\n",
    "            if not self.class_counts:\n",
    "                return None\n",
    "\n",
    "        total = sum(self.class_counts.values())\n",
    "        if total == 0:\n",
    "            self.logger.warning(f\"[ANALYZE] No annotations for split='{self.split}'.\")\n",
    "            return None\n",
    "\n",
    "        # --- Build arrays ---\n",
    "        classes = list(self.class_counts.keys())\n",
    "        counts = np.array([self.class_counts[c] for c in classes], dtype=int)\n",
    "        percents = np.round(100 * counts / total, 2)\n",
    "\n",
    "        # --- Sort descending for clarity ---\n",
    "        order = np.argsort(-counts)\n",
    "        classes = [classes[i] for i in order]\n",
    "        counts = counts[order]\n",
    "        percents = percents[order]\n",
    "        \n",
    "        # --- Compute stats for consistency with post-rebalance evaluation ---\n",
    "        max_c = int(counts.max())\n",
    "        min_c = int(counts.min())\n",
    "        mean_c = float(np.mean(counts))\n",
    "        ratio = round(max_c / max(min_c, 1), 2)\n",
    "        tol = float(getattr(self, \"tolerance\", 0.1))\n",
    "        deviation_mean = float(np.mean(np.abs((counts - mean_c) / mean_c)))\n",
    "        within_tolerance = deviation_mean <= tol\n",
    "\n",
    "\n",
    "        # --- Logging summary ---\n",
    "        self.logger.info(f\"[ANALYZE] Annotation distribution for split='{self.split}':\")\n",
    "        for c, n, p in zip(classes, counts, percents):\n",
    "            self.logger.info(f\"   - {c:<15}: {n:>6} ({p:5.2f}%)\")\n",
    "        self.logger.info(f\"   Total annotations: {int(total)}\")\n",
    "\n",
    "        # --- Plot visualization ---\n",
    "        if plot:\n",
    "            try:\n",
    "                sns.set(style=\"whitegrid\", font_scale=0.9)\n",
    "                fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "                palette = sns.color_palette(\"crest\", len(classes))\n",
    "\n",
    "                sns.barplot(x=classes, y=counts, ax=ax, palette=palette, legend=False)\n",
    "                ax.set_ylim(0, max(counts) * 1.15)\n",
    "\n",
    "                # Annotate counts and percentages\n",
    "                offset = max(3, max(counts) * 0.02)\n",
    "                for i, c in enumerate(classes):\n",
    "                    ax.text(\n",
    "                        i,\n",
    "                        counts[i] + offset,\n",
    "                        f\"{counts[i]} ({percents[i]:.1f}%)\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        fontsize=9,\n",
    "                        color=\"black\"\n",
    "                    )\n",
    "\n",
    "                ax.set_title(f\"Class Distribution - {self.split.upper()} (mean dev={deviation_mean:.3f}, tol={tol:.2f})\", fontsize=12, fontweight=\"bold\")\n",
    "                ax.set_xlabel(\"Class\")\n",
    "                ax.set_ylabel(\"Number of Annotations\")\n",
    "                plt.xticks(rotation=25)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[ANALYZE] Plotting failed ({type(e).__name__}): {e}\")\n",
    "\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"classes\": classes,\n",
    "            \"counts\": counts.tolist(),\n",
    "            \"percents\": percents.tolist(),\n",
    "            \"total\": int(total),\n",
    "            \"max_count\": max_c,\n",
    "            \"min_count\": min_c,\n",
    "            \"mean_count\": round(mean_c, 2),\n",
    "            \"ratio_max_min\": ratio,\n",
    "            \"deviation_mean\": round(deviation_mean, 4),\n",
    "            \"within_tolerance\": within_tolerance,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    def summarize_balance(self) -> dict:\n",
    "        \"\"\"\n",
    "        Quantify imbalance ratios and identify minority/majority classes.\n",
    "        Returns detailed per-class ratios, deviations, and global imbalance metrics.\n",
    "        \"\"\"\n",
    "        # --- Ensure data ---\n",
    "        if not self.class_counts:\n",
    "            self.load_data()\n",
    "        counts = self.class_counts\n",
    "        if not counts:\n",
    "            self.logger.warning(f\"[BALANCE] No class counts found for split='{self.split}'.\")\n",
    "            return {}\n",
    "\n",
    "        # --- Basic stats ---\n",
    "        values = np.array(list(counts.values()), dtype=float)\n",
    "        max_count = float(np.max(values))\n",
    "        min_count = float(np.min(values))\n",
    "        mean_count = float(np.mean(values))\n",
    "        std_count = float(np.std(values))\n",
    "        tol = float(self.tolerance)\n",
    "\n",
    "        if mean_count == 0:\n",
    "            self.logger.warning(f\"[BALANCE] Empty dataset (mean count = 0) for split='{self.split}'.\")\n",
    "            return {}\n",
    "\n",
    "        total_count = int(np.sum(values))\n",
    "        cv = round(std_count / mean_count, 3)\n",
    "\n",
    "        # --- Ratios and deviations ---\n",
    "        ratios_to_max = {cls: round(cnt / max_count, 3) for cls, cnt in counts.items()}\n",
    "        ratios_to_mean = {cls: round(cnt / mean_count, 3) for cls, cnt in counts.items()}\n",
    "        deviation_signed = {cls: (cnt - mean_count) / mean_count for cls, cnt in counts.items()}\n",
    "        deviation_abs = {cls: abs(v) for cls, v in deviation_signed.items()}\n",
    "\n",
    "        # --- Determine minority and majority classes ---\n",
    "        if getattr(self, \"minority_strategy\", \"auto\") == \"bottom_n\":\n",
    "            if len(counts) <= self.bottom_n:\n",
    "                self.logger.warning(\"[BALANCE] bottom_n strategy ignored (fewer classes than N).\")\n",
    "                minority, majority = [], []\n",
    "            else:\n",
    "                sorted_classes = sorted(counts.items(), key=lambda x: x[1])\n",
    "                minority = [cls for cls, _ in sorted_classes[: self.bottom_n]]\n",
    "                majority = [cls for cls, _ in sorted(counts.items(), key=lambda x: -x[1])[: self.bottom_n]]\n",
    "                self.logger.info(f\"[BALANCE] Using fixed bottom_n strategy ({self.bottom_n}).\")\n",
    "        else:\n",
    "            minority = [cls for cls, dev in deviation_abs.items() if counts[cls] < mean_count and dev > tol]\n",
    "            majority = [cls for cls, dev in deviation_abs.items() if counts[cls] > mean_count and dev > tol]\n",
    "\n",
    "        imbalance_ratio = round(max_count / max(min_count, 1), 2)\n",
    "        balanced_within_tol = not (minority or majority)\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"[BALANCE] Split='{self.split}' | tol={tol:.2f} | ratio={imbalance_ratio:.2f} | CV={cv:.3f}\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"[BALANCE] Max={max_count:.0f} ({max(counts, key=counts.get)}) | \"\n",
    "            f\"Min={min_count:.0f} ({min(counts, key=counts.get)}) | Mean={mean_count:.1f}\"\n",
    "        )\n",
    "\n",
    "        for cls, cnt in sorted(counts.items(), key=lambda x: -x[1]):\n",
    "            rmax = ratios_to_max[cls]\n",
    "            rmean = ratios_to_mean[cls]\n",
    "            devp = deviation_signed[cls] * 100\n",
    "            mark = \"ðŸ”»\" if cls in minority else (\"ðŸ”º\" if cls in majority else \"\")\n",
    "            self.logger.info(\n",
    "                f\"   {cls:<15}: count={cnt:<6} | r_max={rmax:.3f} | r_mean={rmean:.3f} | Î”={devp:6.1f}% {mark}\"\n",
    "            )\n",
    "\n",
    "        if minority:\n",
    "            self.logger.info(f\"[BALANCE] Minority ({len(minority)}): {minority}\")\n",
    "        if majority:\n",
    "            self.logger.info(f\"[BALANCE] Majority ({len(majority)}): {majority}\")\n",
    "        if balanced_within_tol:\n",
    "            self.logger.info(\"[BALANCE] Dataset appears balanced within tolerance range.\")\n",
    "\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"total_annotations\": total_count,\n",
    "            \"max_class\": max(counts, key=counts.get),\n",
    "            \"min_class\": min(counts, key=counts.get),\n",
    "            \"max_count\": int(max_count),\n",
    "            \"min_count\": int(min_count),\n",
    "            \"mean_count\": round(mean_count, 2),\n",
    "            \"std_count\": round(std_count, 2),\n",
    "            \"cv\": cv,\n",
    "            \"ratios_to_max\": ratios_to_max,\n",
    "            \"ratios_to_mean\": ratios_to_mean,\n",
    "            \"deviation_signed\": deviation_signed,\n",
    "            \"minority_classes\": minority,\n",
    "            \"majority_classes\": majority,\n",
    "            \"imbalance_ratio\": imbalance_ratio,\n",
    "            \"balanced_within_tol\": balanced_within_tol,\n",
    "            \"tolerance\": tol\n",
    "        }\n",
    "\n",
    "    def set_seed_everywhere(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fix global random seeds across all supported modules for full reproducibility.\n",
    "\n",
    "        Ensures deterministic behavior for:\n",
    "        - Python's random module\n",
    "        - NumPy random generator\n",
    "        - PyTorch (CPU and CUDA, if available)\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Validate and normalize seed\n",
    "        # ==========================================================\n",
    "        try:\n",
    "            self.seed = int(self.seed)\n",
    "        except Exception:\n",
    "            self.seed = 42\n",
    "            if hasattr(self, \"logger\") and self.logger:\n",
    "                self.logger.warning(\"[SETUP] Invalid seed type detected, fallback to 42.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Set Python and NumPy seeds\n",
    "        # ==========================================================\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        cuda_available = False\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ PyTorch seed handling\n",
    "        # ==========================================================\n",
    "        try:\n",
    "            import torch\n",
    "            torch.manual_seed(self.seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(self.seed)\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "\n",
    "                try:\n",
    "                    torch.use_deterministic_algorithms(True)\n",
    "                except Exception:\n",
    "                    # Compatible fallback for older torch versions\n",
    "                    pass\n",
    "\n",
    "                cuda_available = True\n",
    "        except ModuleNotFoundError:\n",
    "            if hasattr(self, \"logger\") and self.logger:\n",
    "                self.logger.debug(\"[SETUP] PyTorch not installed â€” skipping CUDA seeding.\")\n",
    "        except Exception as e:\n",
    "            if hasattr(self, \"logger\") and self.logger:\n",
    "                self.logger.warning(f\"[SETUP] Error while seeding PyTorch: {e}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Logging and return\n",
    "        # ==========================================================\n",
    "        if hasattr(self, \"logger\") and self.logger:\n",
    "            self.logger.info(f\"[SETUP] Global seed fixed â†’ {self.seed} | CUDA available: {cuda_available}\")\n",
    "\n",
    "        return {\"seed\": self.seed, \"cuda_available\": cuda_available}\n",
    "\n",
    "    def prepare_transforms(self):\n",
    "        \"\"\"\n",
    "        Dynamically build Albumentations pipeline from configuration.\n",
    "        Each transform inside the pipeline is applied independently according to\n",
    "        its individual probability `p`. The whole pipeline runs once per image\n",
    "        (ReplayCompose has p=1.0).\n",
    "        \"\"\"\n",
    "        cfg_aug = self.config.get(\"augmentations\", {})\n",
    "\n",
    "        if not cfg_aug.get(\"enabled\", False):\n",
    "            self.logger.info(\"[AUGMENT] Augmentations disabled in configuration.\")\n",
    "            self.pipeline = None\n",
    "            return None\n",
    "\n",
    "        transforms_dict = cfg_aug.get(\"transforms\", {})\n",
    "        if not transforms_dict:\n",
    "            raise ValueError(\"[AUGMENT] No transforms defined in config['augmentations']['transforms'].\")\n",
    "\n",
    "        aug_list = []\n",
    "        for name, params in transforms_dict.items():\n",
    "            p = float(params.get(\"p\", 0.5))  # individual transform probability\n",
    "\n",
    "            try:\n",
    "                transform_cls = getattr(A, name, None)\n",
    "                if transform_cls is None:\n",
    "                    self.logger.warning(f\"[AUGMENT] Unknown transform '{name}' ignored.\")\n",
    "                    continue\n",
    "\n",
    "                # Default border mode for Rotate\n",
    "                if name == \"Rotate\" and \"border_mode\" not in params:\n",
    "                    params[\"border_mode\"] = cv2.BORDER_REFLECT_101\n",
    "\n",
    "                # Filter out invalid keys\n",
    "                valid_params = {k: v for k, v in params.items() if k != \"p\"}\n",
    "\n",
    "                transform = transform_cls(**valid_params, p=p)\n",
    "                aug_list.append(transform)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[AUGMENT] Error initializing '{name}': {e}\")\n",
    "                continue\n",
    "\n",
    "        if not aug_list:\n",
    "            raise ValueError(\"[AUGMENT] No valid transforms found after parsing configuration.\")\n",
    "\n",
    "        try:\n",
    "            aug_blocks = []\n",
    "            # Always ensure at least one transform is applied\n",
    "            aug_blocks.append(A.OneOf(aug_list, p=1.0))\n",
    "            # Optional: apply a second random combination (if SomeOf is supported)\n",
    "            try:\n",
    "                aug_blocks.append(A.SomeOf(aug_list, n=2, replace=False, p=0.3))\n",
    "            except Exception:\n",
    "                self.logger.warning(\"[AUGMENT] A.SomeOf not supported in this Albumentations version. Skipping secondary block.\")\n",
    "\n",
    "            self.pipeline = A.ReplayCompose(\n",
    "                aug_blocks,\n",
    "                bbox_params=A.BboxParams(\n",
    "                    format=\"coco\",\n",
    "                    label_fields=[\"category_ids\"],\n",
    "                    min_visibility=0.25  # discard boxes mostly out of frame\n",
    "                ),p=1.0  # pipeline always executed, internal transforms decide individually\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"[AUGMENT] Failed to build Albumentations pipeline: {e}\")\n",
    "\n",
    "        used_transforms = [t.__class__.__name__ for t in aug_list]\n",
    "        self.logger.info(f\"[AUGMENT] Albumentations pipeline created successfully.\")\n",
    "        self.logger.info(f\"          Includes {len(aug_list)} transforms: {', '.join(used_transforms)}\")\n",
    "\n",
    "        avg_prob = np.mean([t.p for t in aug_list])\n",
    "        self.logger.info(f\"[AUGMENT] Expected average of {len(aug_list)*avg_prob:.2f} transforms applied per image (stochastically).\")\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "    def apply_global_proportional_augmentation(self) -> dict:\n",
    "        \"\"\"\n",
    "        Apply Albumentations augmentation to a random global subset of clean images.\n",
    "        Each augmented image is saved into `{split}_prop/` and logged in `{split}_prop.json`.\n",
    "\n",
    "        Ensures:\n",
    "        - Consistent naming convention\n",
    "        - Unique IDs offset from the original dataset\n",
    "        - Thread-safe augmentation and JSON export\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Preconditions\n",
    "        # ==========================================================\n",
    "        if not isinstance(self.pipeline, A.ReplayCompose):\n",
    "            raise RuntimeError(\"[PROP] Albumentations pipeline not initialized. Run prepare_transforms() first.\")\n",
    "        if not self.clean_json.exists():\n",
    "            raise FileNotFoundError(f\"[PROP] Clean JSON not found: {self.clean_json}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Load base dataset\n",
    "        # ==========================================================\n",
    "        with open(self.clean_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = {img[\"id\"]: img for img in data.get(\"images\", [])}\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = data.get(\"categories\", [])\n",
    "\n",
    "        if not images or not cats:\n",
    "            raise ValueError(\"[PROP] Empty clean dataset or missing categories.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Setup proportional output directory\n",
    "        # ==========================================================\n",
    "        self.prop_dir.mkdir(parents=True, exist_ok=True)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Sampling configuration\n",
    "        # ==========================================================\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        total_imgs = len(images)\n",
    "        n_to_aug = max(1, int(total_imgs * self.proportion))\n",
    "        sampled_ids = rng.choice(list(images.keys()), size=n_to_aug, replace=False)\n",
    "\n",
    "        self.logger.info(f\"[PROP] Selected {n_to_aug}/{total_imgs} images ({self.proportion * 100:.1f}%) for augmentation.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Safe ID offsets (prevent collisions)\n",
    "        # ==========================================================\n",
    "        max_img_id = max(images.keys(), default=0)\n",
    "        max_ann_id = max((a[\"id\"] for a in anns), default=0)\n",
    "        img_id_counter = itertools.count(start=max_img_id + 1)\n",
    "        ann_id_counter = itertools.count(start=max_ann_id + 1)\n",
    "        id_lock = threading.Lock()\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Category lookup\n",
    "        # ==========================================================\n",
    "        cat_lookup = {c[\"id\"]: c[\"name\"] for c in cats}\n",
    "        img_class_map = {\n",
    "            a[\"image_id\"]: cat_lookup.get(a[\"category_id\"], \"Unknown\")\n",
    "            for a in anns\n",
    "        }\n",
    "\n",
    "        new_images, new_anns = [], []\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Inner augmentation function\n",
    "        # ==========================================================\n",
    "        def _augment_image(img_id, cls_name):\n",
    "            try:\n",
    "                img_entry = images.get(img_id)\n",
    "                if not img_entry:\n",
    "                    return None\n",
    "\n",
    "                fname = Path(img_entry[\"file_name\"]).name\n",
    "                img_path = next(\n",
    "                    (p for p in [self.clean_dir / fname, self.clean_dir / \"images\" / fname] if p.exists()),\n",
    "                    None\n",
    "                )\n",
    "                if img_path is None:\n",
    "                    return None\n",
    "\n",
    "                with Image.open(img_path) as im:\n",
    "                    img = np.array(im.convert(\"RGB\"))\n",
    "\n",
    "                anns_img = [a for a in anns if a[\"image_id\"] == img_id]\n",
    "                if not anns_img:\n",
    "                    return None\n",
    "\n",
    "                bboxes = [a[\"bbox\"] for a in anns_img]\n",
    "                labels = [a[\"category_id\"] for a in anns_img]\n",
    "\n",
    "                transformed = self.pipeline(image=img, bboxes=bboxes, category_ids=labels)\n",
    "                if not transformed or not transformed.get(\"bboxes\"):\n",
    "                    return None\n",
    "\n",
    "                aug_img = np.clip(transformed[\"image\"], 0, 255).astype(np.uint8)\n",
    "                if aug_img.size == 0:\n",
    "                    return None\n",
    "\n",
    "                h, w = aug_img.shape[:2]\n",
    "                clamped_bboxes = []\n",
    "                for bbox in transformed[\"bboxes\"]:\n",
    "                    x, y, bw, bh = bbox\n",
    "                    x = max(0, min(x, w - 1))\n",
    "                    y = max(0, min(y, h - 1))\n",
    "                    bw = max(1.0, min(bw, w - x))\n",
    "                    bh = max(1.0, min(bh, h - y))\n",
    "                    clamped_bboxes.append([x, y, bw, bh])\n",
    "\n",
    "                # --- Assign new image ID and save ---\n",
    "                with id_lock:\n",
    "                    new_img_id = next(img_id_counter)\n",
    "                new_name = f\"{Path(fname).stem}_prop_{uuid4().hex[:8]}.jpg\"\n",
    "                out_path = self.prop_dir / new_name\n",
    "                Image.fromarray(aug_img).save(out_path, format=\"JPEG\", quality=95)\n",
    "\n",
    "                new_images.append({\n",
    "                    \"id\": new_img_id,\n",
    "                    \"file_name\": new_name,\n",
    "                    \"orig_file\": fname,\n",
    "                    \"width\": w,\n",
    "                    \"height\": h,\n",
    "                    \"source_stage\": \"proportional\"\n",
    "                })\n",
    "\n",
    "                # --- Annotations ---\n",
    "                for bbox, cid in zip(clamped_bboxes, transformed[\"category_ids\"]):\n",
    "                    with id_lock:\n",
    "                        ann_id = next(ann_id_counter)\n",
    "                    new_anns.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": new_img_id,\n",
    "                        \"category_id\": cid,\n",
    "                        \"bbox\": [float(x) for x in bbox],\n",
    "                        \"area\": float(bbox[2] * bbox[3]),\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"source_stage\": \"proportional\"\n",
    "                    })\n",
    "\n",
    "                return True\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"[PROP] Error augmenting image {img_id}: {e}\")\n",
    "                return None\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Parallel execution\n",
    "        # ==========================================================\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            futures = [\n",
    "                executor.submit(_augment_image, int(img_id), img_class_map.get(int(img_id), \"Unknown\"))\n",
    "                for img_id in sampled_ids\n",
    "            ]\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Proportional augment {self.split}\"):\n",
    "                pass  # Progress bar only; results appended inside threads\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Save JSON\n",
    "        # ==========================================================\n",
    "        json_out = {\n",
    "            \"images\": new_images,\n",
    "            \"annotations\": new_anns,\n",
    "            \"categories\": cats,\n",
    "            \"_meta\": {\n",
    "                \"split\": self.split,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"total_images\": len(new_images),\n",
    "                \"total_annotations\": len(new_anns)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(self.prop_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_out, f, indent=2)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Logging and return\n",
    "        # ==========================================================\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        self.logger.info(f\"[PROP] Completed proportional augmentation for split='{self.split}'.\")\n",
    "        self.logger.info(f\"       -> {len(new_images)} new images | {len(new_anns)} annotations | {duration:.2f}s\")\n",
    "        self.logger.info(f\"       -> JSON saved to {self.prop_json}\")\n",
    "\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"augmented_images\": len(new_images),\n",
    "            \"augmented_annotations\": len(new_anns),\n",
    "            \"output_dir\": str(self.prop_dir),\n",
    "            \"json_path\": str(self.prop_json),\n",
    "            \"duration_sec\": duration,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "    def define_rebalance_strategy(self) -> dict:\n",
    "        \"\"\"\n",
    "        Define the base corrective strategy for class imbalance.\n",
    "\n",
    "        This stage determines numeric targets and actions per class.\n",
    "        It does NOT yet decide which specific images to augment - that\n",
    "        is handled by plan_rebalance_sampling().\n",
    "        \"\"\"\n",
    "        # 1. Validate mode\n",
    "        if self.mode not in [\"over\", \"under\"]:\n",
    "            self.logger.warning(f\"[REBALANCE] Invalid mode='{self.mode}'. Expected 'over' or 'under'. Skipping stage.\")\n",
    "            self.rebalance_plan = {}\n",
    "            return {}\n",
    "\n",
    "        counts = {cls: 0 for cls in self.classes}\n",
    "\n",
    "        # 2. Load clean dataset counts\n",
    "        if self.clean_json.exists():\n",
    "            with open(self.clean_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                clean_data = json.load(f)\n",
    "            clean_cats = {c[\"id\"]: c[\"name\"] for c in clean_data.get(\"categories\", [])}\n",
    "            for ann in clean_data.get(\"annotations\", []):\n",
    "                cname = clean_cats.get(ann[\"category_id\"])\n",
    "                if cname in counts:\n",
    "                    counts[cname] += 1\n",
    "        else:\n",
    "            self.logger.warning(\"[REBALANCE] Clean JSON not found. Counts may be incomplete.\")\n",
    "\n",
    "        # 3. Add proportional dataset counts if available\n",
    "        if self.prop_json.exists():\n",
    "            with open(self.prop_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                prop_data = json.load(f)\n",
    "            prop_cats = {c[\"id\"]: c[\"name\"] for c in prop_data.get(\"categories\", [])}\n",
    "\n",
    "            # Optional consistency check\n",
    "            if set(clean_cats.values()) != set(prop_cats.values()):\n",
    "                self.logger.warning(\"[REBALANCE] Category mismatch detected between clean and proportional datasets.\")\n",
    "\n",
    "            for ann in prop_data.get(\"annotations\", []):\n",
    "                cname = prop_cats.get(ann[\"category_id\"])\n",
    "                if cname in counts:\n",
    "                    counts[cname] += 1\n",
    "            self.logger.info(\"[REBALANCE] Combined counts from clean + proportional datasets.\")\n",
    "        else:\n",
    "            self.logger.warning(\"[REBALANCE] Proportional JSON not found. Using clean counts only.\")\n",
    "\n",
    "        # 4. Validate counts\n",
    "        self.class_counts = counts\n",
    "        if not counts or all(v == 0 for v in counts.values()):\n",
    "            raise RuntimeError(\"[REBALANCE] No valid counts found in datasets.\")\n",
    "\n",
    "        summary = self.summarize_balance()\n",
    "        max_count = summary.get(\"max_count\", 0)\n",
    "        min_count = summary.get(\"min_count\", 0)\n",
    "        tol = float(self.tolerance)\n",
    "        rebalance_plan = {}\n",
    "\n",
    "        # 5. Define strategy based on mode\n",
    "        if self.mode == \"over\":\n",
    "            target = int(max_count * (1 - tol))\n",
    "            target = max(target, 1)\n",
    "            self.fixed_target = target\n",
    "            self.logger.info(f\"[REBALANCE] Mode='over' -> target â‰ˆ {target:,} per class (tol={tol:.2f})\")\n",
    "\n",
    "            for cls, count in self.class_counts.items():\n",
    "                if count < target:\n",
    "                    deficit = target - count\n",
    "                    rebalance_plan[cls] = {\"action\": \"augment\", \"target\": target, \"deficit\": deficit}\n",
    "                else:\n",
    "                    rebalance_plan[cls] = {\"action\": \"keep\", \"target\": count, \"deficit\": 0}\n",
    "\n",
    "        elif self.mode == \"under\":\n",
    "            target = int(min_count * (1 + tol))\n",
    "            target = max(target, 1)\n",
    "            self.fixed_target = target\n",
    "            self.logger.info(f\"[REBALANCE] Mode='under' -> target â‰ˆ {target:,} per class (tol={tol:.2f})\")\n",
    "\n",
    "            for cls, count in self.class_counts.items():\n",
    "                if count > target:\n",
    "                    excess = count - target\n",
    "                    rebalance_plan[cls] = {\"action\": \"downsample\", \"target\": target, \"excess\": excess}\n",
    "                else:\n",
    "                    rebalance_plan[cls] = {\"action\": \"keep\", \"target\": count, \"excess\": 0}\n",
    "\n",
    "        # 6. Summaries and logging\n",
    "        total_to_augment = sum(v.get(\"deficit\", 0) for v in rebalance_plan.values())\n",
    "        total_to_down = sum(v.get(\"excess\", 0) for v in rebalance_plan.values())\n",
    "\n",
    "        for cls, plan in rebalance_plan.items():\n",
    "            act = plan[\"action\"]\n",
    "            tgt = plan[\"target\"]\n",
    "            if plan.get(\"deficit\", 0) > 0:\n",
    "                self.logger.info(f\"   - {cls:<15} -> {act:>10} to {tgt:<6} (+{plan['deficit']})\")\n",
    "            elif plan.get(\"excess\", 0) > 0:\n",
    "                self.logger.info(f\"   - {cls:<15} -> {act:>10} to {tgt:<6} (-{plan['excess']})\")\n",
    "            else:\n",
    "                self.logger.info(f\"   - {cls:<15} -> {act:>10} (no change)\")\n",
    "\n",
    "        self.rebalance_plan = rebalance_plan\n",
    "        self.rebalance_plan[\"_meta\"] = {\n",
    "            \"target_global\": target,\n",
    "            \"total_deficit\": total_to_augment,\n",
    "            \"total_excess\": total_to_down,\n",
    "            \"mode\": self.mode,\n",
    "            \"tolerance\": tol\n",
    "        }\n",
    "\n",
    "        # 7. Save plan for traceability\n",
    "        try:\n",
    "            plan_path = self.output_dir / f\"rebalance_plan_{self.split}.json\"\n",
    "            with open(plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self.rebalance_plan, f, indent=2)\n",
    "            self.logger.info(f\"[REBALANCE] Plan saved to: {plan_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[REBALANCE] Failed to save plan JSON: {e}\")\n",
    "\n",
    "        # 8. Final summary\n",
    "        self.logger.info(\n",
    "            f\"[REBALANCE] Summary -> augment_total={total_to_augment:,} | downsample_total={total_to_down:,}\"\n",
    "        )\n",
    "        self.logger.info(\"[REBALANCE] Strategy ready - next: run plan_rebalance_sampling() to check feasibility.\")\n",
    "\n",
    "        return rebalance_plan\n",
    "\n",
    "    def plan_rebalance_sampling(self) -> dict:\n",
    "        \"\"\"\n",
    "        Build a realistic, finite sampling plan for rebalance augmentation.\n",
    "        \"\"\"\n",
    "        if not self.rebalance_plan:\n",
    "            raise RuntimeError(\"[PLAN] Rebalance plan not defined. Run define_rebalance_strategy() first.\")\n",
    "        if not self.prop_json.exists():\n",
    "            raise FileNotFoundError(f\"[PLAN] Missing proportional JSON: {self.prop_json}\")\n",
    "\n",
    "        # Load proportional dataset\n",
    "        with open(self.prop_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "        if not anns or not cats:\n",
    "            raise ValueError(\"[PLAN] Invalid proportional dataset: missing annotations or categories.\")\n",
    "\n",
    "        # Identify minority classes\n",
    "        summary = self.summarize_balance()\n",
    "        minority_classes = summary.get(\"minority_classes\", [])\n",
    "\n",
    "        if not minority_classes:\n",
    "            self.logger.warning(\"[PLAN] No minority classes detected; skipping rebalance sampling.\")\n",
    "            return {}\n",
    "\n",
    "        eligible_imgs = self._filter_minority_images(anns, cats, minority_classes)\n",
    "        if not eligible_imgs:\n",
    "            self.logger.warning(\"[PLAN] No eligible minority images found. Skipping rebalance planning.\")\n",
    "            return {}\n",
    "\n",
    "        # Build mapping: class -> image IDs\n",
    "        cls_to_imgs = {cname: set() for cname in cats.values()}\n",
    "        for ann in anns:\n",
    "            cname = cats.get(ann[\"category_id\"])\n",
    "            if cname in cls_to_imgs and ann[\"image_id\"] in eligible_imgs:\n",
    "                cls_to_imgs[cname].add(ann[\"image_id\"])\n",
    "\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        plan = {}\n",
    "\n",
    "        max_repeats = int(self.max_repeats)\n",
    "        min_unique_ratio = float(self.min_unique_ratio)\n",
    "        self.logger.info(f\"[PLAN] Building sampling plan (max_repeats={max_repeats}, min_unique_ratio={min_unique_ratio:.2f})...\")\n",
    "\n",
    "        total_planned = 0\n",
    "        # Class-by-class planning\n",
    "        for cls, rule in self.rebalance_plan.items():\n",
    "            action = rule.get(\"action\")\n",
    "\n",
    "            # Only minority classes marked for augmentation\n",
    "            if action != \"augment\" or cls not in minority_classes:\n",
    "                continue\n",
    "\n",
    "            img_ids = list(cls_to_imgs.get(cls, []))\n",
    "            n_unique = len(img_ids)\n",
    "            needed = int(rule.get(\"deficit\", 0))\n",
    "\n",
    "            if n_unique == 0 or needed == 0:\n",
    "                self.logger.warning(f\"[PLAN] Class '{cls}' has no eligible images or no deficit.\")\n",
    "                continue\n",
    "\n",
    "            # --- Compute constraints ---\n",
    "            max_possible = n_unique * max_repeats\n",
    "            limited = needed > max_possible\n",
    "\n",
    "            # --- Compute diversity threshold ---\n",
    "            min_unique = max(1, int(n_unique * min_unique_ratio))\n",
    "            unique_used = min(n_unique, max(min_unique, math.ceil(needed / max_repeats)))\n",
    "\n",
    "            # --- Select images randomly ---\n",
    "            selected_unique = rng.choice(img_ids, size=unique_used, replace=False)\n",
    "\n",
    "            # --- Compute repeat factor ---\n",
    "            repeat_factor = math.ceil(needed / unique_used)\n",
    "            repeat_factor = min(repeat_factor, max_repeats)\n",
    "            total_samples = min(needed, unique_used * repeat_factor)\n",
    "\n",
    "            # --- Final selected IDs ---\n",
    "            selected_ids = np.tile(selected_unique, repeat_factor)[:needed].tolist()\n",
    "\n",
    "            # --- Save class plan ---\n",
    "            plan[cls] = {\n",
    "                \"available\": n_unique,\n",
    "                \"needed\": needed,\n",
    "                \"unique_used\": unique_used,\n",
    "                \"repeat_factor\": repeat_factor,\n",
    "                \"max_possible\": max_possible,\n",
    "                \"limited\": limited,\n",
    "                \"selected_ids\": selected_ids\n",
    "            }\n",
    "            total_planned += len(selected_ids)\n",
    "\n",
    "            # --- Logging ---\n",
    "            msg = (f\"[PLAN] {cls:<15} -> need={needed:>5} | unique={unique_used:>4} | \"\n",
    "                f\"repeatÃ—={repeat_factor:<2} | total={len(selected_ids):>5}\")\n",
    "            self.logger.info(msg)\n",
    "            if limited:\n",
    "                self.logger.warning(\n",
    "                    f\"[PLAN] Class '{cls}' target unreachable: need {needed}, only {max_possible} possible \"\n",
    "                    f\"with {max_repeats}Ã— repeats.\"\n",
    "                )\n",
    "\n",
    "        self.rebalance_sampling_plan = plan\n",
    "\n",
    "        if not plan:\n",
    "            self.logger.warning(\"[PLAN] No valid sampling plan generated. Nothing to rebalance.\")\n",
    "            return {}\n",
    "\n",
    "        plan[\"_meta\"] = {\n",
    "            \"split\": self.split,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"version\": getattr(self, \"version\", \"unknown\"),\n",
    "            \"total_planned\": total_planned,\n",
    "            \"minority_classes\": minority_classes,\n",
    "            \"max_repeats\": max_repeats,\n",
    "            \"min_unique_ratio\": min_unique_ratio,\n",
    "            \"eligible_images\": len(eligible_imgs)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            plan_path = self.output_dir / f\"rebalance_sampling_{self.split}.json\"\n",
    "            with open(plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(plan, f, indent=2)\n",
    "            self.logger.info(f\"[PLAN] Sampling plan saved to: {plan_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[PLAN] Failed to save sampling plan: {e}\")\n",
    "\n",
    "        self.logger.info(f\"[PLAN] Sampling plan ready -> total {total_planned} samples across {len(plan)} classes.\")\n",
    "        self.logger.info(\"[PLAN] Next: run apply_rebalance_augmentation() to generate new samples.\")\n",
    "\n",
    "        return plan\n",
    "\n",
    "    def evaluate_rebalance_result(self, show_plot: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate post-rebalance class distribution and determine whether\n",
    "        the imbalance across classes is within the configured tolerance.\n",
    "\n",
    "        The method:\n",
    "        - Computes per-class annotation counts\n",
    "        - Calculates mean deviation relative to the mean\n",
    "        - Checks if balance is within self.tolerance\n",
    "        - Optionally visualizes results and saves a JSON summary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_plot : bool\n",
    "            Whether to display the class distribution plot.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with detailed balance metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Preconditions\n",
    "        # ==========================================================\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[EVAL] Missing final JSON: {self.final_json}\")\n",
    "\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "        if not anns or not cats:\n",
    "            raise ValueError(\"[EVAL] Invalid or empty final dataset structure.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Compute per-class annotation counts\n",
    "        # ==========================================================\n",
    "        counts = {cats[k]: 0 for k in cats}\n",
    "        for ann in anns:\n",
    "            cname = cats.get(ann[\"category_id\"])\n",
    "            if cname:\n",
    "                counts[cname] += 1\n",
    "\n",
    "        if not counts or all(v == 0 for v in counts.values()):\n",
    "            raise RuntimeError(\"[EVAL] No valid annotations detected in final dataset.\")\n",
    "\n",
    "        counts_sorted = dict(sorted(counts.items(), key=lambda x: x[0]))\n",
    "        total_anns = sum(counts_sorted.values())\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Statistical analysis\n",
    "        # ==========================================================\n",
    "        vals = list(counts_sorted.values())\n",
    "        max_c = max(vals)\n",
    "        min_c = min(vals)\n",
    "        mean_c = float(np.mean(vals))\n",
    "        ratio = round(max_c / max(min_c, 1), 3)\n",
    "        tol = float(self.tolerance)\n",
    "        deviation_mean = float(np.mean([abs((v - mean_c) / mean_c) for v in vals]))\n",
    "        within_tolerance = deviation_mean <= tol\n",
    "\n",
    "        minority_classes = [k for k, v in counts_sorted.items() if v < mean_c * (1 - tol)]\n",
    "        majority_classes = [k for k, v in counts_sorted.items() if v > mean_c * (1 + tol)]\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Logging summary\n",
    "        # ==========================================================\n",
    "        self.logger.info(f\"[EVAL] Post-rebalance evaluation for split='{self.split}':\")\n",
    "        self.logger.info(f\"       -> Max={max_c} | Min={min_c} | Mean={mean_c:.2f} | Ratio={ratio:.2f}\")\n",
    "        self.logger.info(f\"       -> Mean deviation={deviation_mean:.4f} | Tolerance={tol:.2f}\")\n",
    "        self.logger.info(f\"       -> Minority classes: {minority_classes or 'None'}\")\n",
    "        self.logger.info(f\"       -> Majority classes: {majority_classes or 'None'}\")\n",
    "\n",
    "        for cls, count in counts_sorted.items():\n",
    "            pct = (count / total_anns) * 100\n",
    "            self.logger.info(f\"   - {cls:<15} {count:>6} ({pct:>5.2f}%)\")\n",
    "\n",
    "        if within_tolerance:\n",
    "            self.logger.info(f\"[EVAL] Balance achieved within tolerance (â‰¤ {tol:.2f}).\")\n",
    "        else:\n",
    "            self.logger.warning(f\"[EVAL] Balance not achieved (mean deviation={deviation_mean:.4f} > tol={tol:.2f}).\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Visualization (optional)\n",
    "        # ==========================================================\n",
    "        if show_plot:\n",
    "            sns.set(style=\"whitegrid\")\n",
    "            df = pd.DataFrame(list(counts_sorted.items()), columns=[\"Class\", \"Count\"])\n",
    "            df[\"Percent\"] = 100 * df[\"Count\"] / df[\"Count\"].sum()\n",
    "\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            ax = sns.barplot(x=\"Class\", y=\"Count\", data=df, palette=\"crest\")\n",
    "            plt.title(f\"Post-Rebalance Class Distribution ({self.split})\", fontsize=13)\n",
    "            plt.ylabel(\"Annotations\")\n",
    "            plt.xlabel(\"Class\")\n",
    "            plt.xticks(rotation=25, ha=\"right\")\n",
    "\n",
    "            for i, row in df.iterrows():\n",
    "                ax.text(i, row[\"Count\"], f\"{int(row['Count'])}\\n({row['Percent']:.1f}%)\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9, color=\"black\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Save evaluation summary\n",
    "        # ==========================================================\n",
    "        try:\n",
    "            eval_path = self.output_dir / f\"rebalance_eval_{self.split}.json\"\n",
    "            summary = {\n",
    "                \"split\": self.split,\n",
    "                \"counts\": counts_sorted,\n",
    "                \"max_class\": max_c,\n",
    "                \"min_class\": min_c,\n",
    "                \"mean_count\": round(mean_c, 2),\n",
    "                \"ratio\": ratio,\n",
    "                \"deviation_mean\": round(deviation_mean, 4),\n",
    "                \"within_tolerance\": within_tolerance,\n",
    "                \"minority_classes\": minority_classes,\n",
    "                \"majority_classes\": majority_classes,\n",
    "                \"total_annotations\": total_anns,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            self.logger.info(f\"[EVAL] Evaluation summary saved to {eval_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[EVAL] Failed to save evaluation summary: {e}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Return structured metrics\n",
    "        # ==========================================================\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"counts\": counts_sorted,\n",
    "            \"max_class\": max_c,\n",
    "            \"min_class\": min_c,\n",
    "            \"mean\": round(mean_c, 2),\n",
    "            \"ratio\": ratio,\n",
    "            \"deviation_mean\": round(deviation_mean, 4),\n",
    "            \"within_tolerance\": within_tolerance,\n",
    "            \"minority_classes\": minority_classes,\n",
    "            \"majority_classes\": majority_classes\n",
    "        }\n",
    "\n",
    "    def apply_minority_focus_zoom(self) -> dict:\n",
    "        \"\"\"\n",
    "        Reinforce minority-class representation by applying localized zooms\n",
    "        on a random subset of minority-containing images.\n",
    "        Saves results in `{split}_zoom/` with JSON `{split}_zoom.json`.\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Configuration and prechecks\n",
    "        # ==========================================================\n",
    "        start_time = time.time()\n",
    "        cfg = self.config.get(\"augmentations\", {})\n",
    "        prop = float(cfg.get(\"focus_reinforce_prop\", 0.1))\n",
    "        scale_zoom = float(cfg.get(\"focus_zoom_scale\", 1.5))\n",
    "        offset_ratio = float(cfg.get(\"focus_zoom_offset\", 0.2))\n",
    "\n",
    "        if prop <= 0:\n",
    "            self.logger.info(\"[ZOOM] Reinforcement disabled (focus_reinforce_prop <= 0).\")\n",
    "            return {\n",
    "                \"split\": self.split,\n",
    "                \"focus_zoom_applied\": False,\n",
    "                \"generated\": 0,\n",
    "                \"status\": \"disabled\"\n",
    "            }\n",
    "\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[ZOOM] Missing final JSON: {self.final_json}\")\n",
    "\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Determine minority classes\n",
    "        # ==========================================================\n",
    "        if not getattr(self, \"minority_classes\", []):\n",
    "            summary = self.summarize_balance_from_json(self.final_json)\n",
    "            self.minority_classes = summary.get(\"minority_classes\", [])\n",
    "\n",
    "        minority_classes = getattr(self, \"minority_classes\", [])\n",
    "        if not anns or not cats or not minority_classes:\n",
    "            self.logger.warning(\"[ZOOM] Missing annotations, categories, or minority class info.\")\n",
    "            return {\"split\": self.split, \"focus_zoom_applied\": False, \"generated\": 0, \"status\": \"incomplete_data\"}\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"[ZOOM] Applying minority zoom reinforcement \"\n",
    "            f\"(prop={prop}, scale={scale_zoom}, offset={offset_ratio})...\"\n",
    "        )\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Select eligible images\n",
    "        # ==========================================================\n",
    "        eligible_imgs = {a[\"image_id\"] for a in anns if cats[a[\"category_id\"]] in minority_classes}\n",
    "        if not eligible_imgs:\n",
    "            self.logger.info(\"[ZOOM] No minority images found. Skipping reinforcement.\")\n",
    "            return {\"status\": \"no_minority_images\"}\n",
    "\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        n_select = max(1, int(len(eligible_imgs) * prop))\n",
    "        selected_ids = rng.choice(list(eligible_imgs), size=n_select, replace=False)\n",
    "\n",
    "        self.logger.info(f\"[ZOOM] Selected {n_select}/{len(eligible_imgs)} minority images for zoom reinforcement.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Prepare directories and counters\n",
    "        # ==========================================================\n",
    "        self.zoom_dir.mkdir(parents=True, exist_ok=True)\n",
    "        images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "        new_images, new_anns = [], []\n",
    "\n",
    "        img_id_counter = itertools.count(start=max((i[\"id\"] for i in data[\"images\"]), default=0) + 1)\n",
    "        ann_id_counter = itertools.count(start=max((a[\"id\"] for a in data[\"annotations\"]), default=0) + 1)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Process each selected image\n",
    "        # ==========================================================\n",
    "        for img_id in tqdm(selected_ids, desc=f\"Zoom reinforce {self.split}\"):\n",
    "            img_entry = images.get(int(img_id))\n",
    "            if not img_entry:\n",
    "                continue\n",
    "\n",
    "            fname = Path(img_entry[\"file_name\"]).name\n",
    "            img_path = self.final_dir / fname\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(img_path) as im:\n",
    "                    img = np.array(im.convert(\"RGB\"))\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"[ZOOM] Failed reading {fname}: {e}\")\n",
    "                continue\n",
    "\n",
    "            anns_img = [a for a in anns if a[\"image_id\"] == img_id]\n",
    "            bboxes = [a[\"bbox\"] for a in anns_img]\n",
    "            labels = [a[\"category_id\"] for a in anns_img]\n",
    "            if not bboxes:\n",
    "                continue\n",
    "\n",
    "            # Compute region centered on minority instances\n",
    "            minority_bboxes = [b for b, cid in zip(bboxes, labels) if cats[cid] in minority_classes]\n",
    "            if not minority_bboxes:\n",
    "                continue\n",
    "\n",
    "            x_min = min(b[0] for b in minority_bboxes)\n",
    "            y_min = min(b[1] for b in minority_bboxes)\n",
    "            x_max = max(b[0] + b[2] for b in minority_bboxes)\n",
    "            y_max = max(b[1] + b[3] for b in minority_bboxes)\n",
    "\n",
    "            cx, cy = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "            w, h = (x_max - x_min) * scale_zoom, (y_max - y_min) * scale_zoom\n",
    "\n",
    "            dx = rng.uniform(-offset_ratio, offset_ratio) * w\n",
    "            dy = rng.uniform(-offset_ratio, offset_ratio) * h\n",
    "\n",
    "            x0 = int(max(0, cx - w / 2 + dx))\n",
    "            y0 = int(max(0, cy - h / 2 + dy))\n",
    "            x1 = int(min(img.shape[1], cx + w / 2 + dx))\n",
    "            y1 = int(min(img.shape[0], cy + h / 2 + dy))\n",
    "\n",
    "            cropped = img[y0:y1, x0:x1]\n",
    "            if cropped.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Adjust bounding boxes for cropped region\n",
    "            shifted_bboxes, shifted_labels = [], []\n",
    "            for bbox, cid in zip(bboxes, labels):\n",
    "                x, y, bw, bh = bbox\n",
    "                nx, ny = x - x0, y - y0\n",
    "                if nx + bw <= 0 or ny + bh <= 0 or nx >= (x1 - x0) or ny >= (y1 - y0):\n",
    "                    continue\n",
    "                nx = np.clip(nx, 0, x1 - x0 - 1)\n",
    "                ny = np.clip(ny, 0, y1 - y0 - 1)\n",
    "                shifted_bboxes.append([float(nx), float(ny), float(bw), float(bh)])\n",
    "                shifted_labels.append(cid)\n",
    "\n",
    "            if not shifted_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Save cropped image\n",
    "            h2, w2 = cropped.shape[:2]\n",
    "            new_img_id = next(img_id_counter)\n",
    "            new_fname = f\"{Path(fname).stem}_zoom_{uuid4().hex[:8]}.jpg\"\n",
    "            out_path = self.zoom_dir / new_fname\n",
    "\n",
    "            try:\n",
    "                Image.fromarray(cropped).save(out_path, format=\"JPEG\", quality=95)\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"[ZOOM] Failed saving {new_fname}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Append new entries\n",
    "            new_images.append({\n",
    "                \"id\": new_img_id,\n",
    "                \"file_name\": new_fname,\n",
    "                \"width\": w2,\n",
    "                \"height\": h2,\n",
    "                \"source_stage\": \"zoom_reinforce\"\n",
    "            })\n",
    "\n",
    "            for bbox, cid in zip(shifted_bboxes, shifted_labels):\n",
    "                new_anns.append({\n",
    "                    \"id\": next(ann_id_counter),\n",
    "                    \"image_id\": new_img_id,\n",
    "                    \"category_id\": cid,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"area\": float(bbox[2] * bbox[3]),\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"source_stage\": \"zoom_reinforce\"\n",
    "                })\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Save JSON\n",
    "        # ==========================================================\n",
    "        json_out = {\n",
    "            \"images\": new_images,\n",
    "            \"annotations\": new_anns,\n",
    "            \"categories\": data[\"categories\"],\n",
    "            \"_meta\": {\n",
    "            \"split\": self.split,\n",
    "            \"version\": getattr(self, \"version\", \"unknown\"),\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_generated\": len(new_images),\n",
    "            \"total_annotations\": len(new_anns)\n",
    "        }\n",
    "\n",
    "        }\n",
    "\n",
    "        with open(self.zoom_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_out, f, indent=2)\n",
    "\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        self.logger.info(f\"[ZOOM] Completed minority zoom reinforcement â†’ {len(new_images)} images in {duration:.2f}s.\")\n",
    "        self.logger.info(f\"[ZOOM] JSON saved to {self.zoom_json}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Return summary\n",
    "        # ==========================================================\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"focus_zoom_applied\": True,\n",
    "            \"generated\": len(new_images),\n",
    "            \"output_dir\": str(self.zoom_dir),\n",
    "            \"json_path\": str(self.zoom_json),\n",
    "            \"duration_sec\": duration,\n",
    "            \"status\": \"success\",\n",
    "            \"total_annotations\": len(new_anns)\n",
    "\n",
    "        }\n",
    "\n",
    "    def apply_rebalance_augmentation(self, source: str = \"prop\", iteration: int = 1) -> dict:\n",
    "        \"\"\"\n",
    "        Apply corrective (rebalance) augmentation according to the current sampling plan.\n",
    "        Saves only new augmented images and annotations for this iteration in:\n",
    "            mirror_clean/{split}_rebalance_{iteration}/\n",
    "            â†’ {split}_rebalance_{iteration}.json\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Preconditions\n",
    "        # ==========================================================\n",
    "        if self.mode not in [\"over\", \"under\"]:\n",
    "            self.logger.info(\"[REBALANCE] Skipping corrective augmentation (mode='none').\")\n",
    "            return {\"split\": self.split, \"rebalance_applied\": False}\n",
    "\n",
    "        if not hasattr(self, \"rebalance_sampling_plan\") or not self.rebalance_sampling_plan:\n",
    "            raise RuntimeError(\"[REBALANCE] Sampling plan missing. Run plan_rebalance_sampling() first.\")\n",
    "\n",
    "        if not isinstance(self.pipeline, A.ReplayCompose):\n",
    "            raise RuntimeError(\"[REBALANCE] Albumentations pipeline not initialized. Run prepare_transforms() first.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Source dataset (prop or final)\n",
    "        # ==========================================================\n",
    "        if source == \"final\":\n",
    "            src_json, src_dir = self.final_json, self.final_dir\n",
    "        else:\n",
    "            src_json, src_dir = self.prop_json, self.prop_dir\n",
    "\n",
    "        if not src_json.exists():\n",
    "            raise FileNotFoundError(f\"[REBALANCE] Missing source JSON: {src_json}\")\n",
    "\n",
    "        with open(src_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = {img[\"id\"]: img for img in data.get(\"images\", [])}\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = data.get(\"categories\", [])\n",
    "\n",
    "        if not images or not anns or not cats:\n",
    "            raise ValueError(\"[REBALANCE] Source dataset invalid or empty.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Prepare iteration-specific output\n",
    "        # ==========================================================\n",
    "        self.rebalance_dir = self.mirror_dir / f\"{self.split}_rebalance_{iteration}\"\n",
    "        self.rebalance_dir.mkdir(parents=True, exist_ok=True)\n",
    "        rebalance_json_path = self.rebalance_dir / f\"{self.split}_rebalance_{iteration}.json\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Unique ID offset setup\n",
    "        # ==========================================================\n",
    "        max_img_id = max(images.keys(), default=0)\n",
    "        max_ann_id = max((a[\"id\"] for a in anns), default=0)\n",
    "        img_id_counter = itertools.count(start=max_img_id + 1)\n",
    "        ann_id_counter = itertools.count(start=max_ann_id + 1)\n",
    "        id_lock = threading.Lock()\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        max_boxes = getattr(self, \"max_boxes_per_image\", 9999)\n",
    "\n",
    "        new_images, new_anns = [], []\n",
    "        total_failures = 0\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Inner augmentation function\n",
    "        # ==========================================================\n",
    "        def _augment_image(img_id, cls_name=\"mixed\"):\n",
    "            try:\n",
    "                img_entry = images.get(img_id)\n",
    "                if not img_entry:\n",
    "                    return None\n",
    "\n",
    "                fname = Path(img_entry[\"file_name\"]).name\n",
    "                candidates = [\n",
    "                    src_dir / fname,\n",
    "                    src_dir / \"images\" / fname,\n",
    "                    src_dir / fname.lower(),\n",
    "                    src_dir / fname.upper(),\n",
    "                ]\n",
    "                img_path = next((p for p in candidates if p.exists()), None)\n",
    "                if img_path is None:\n",
    "                    return None\n",
    "\n",
    "                with Image.open(img_path) as im:\n",
    "                    img = np.array(im.convert(\"RGB\"))\n",
    "\n",
    "                anns_img = [a for a in anns if a[\"image_id\"] == img_id]\n",
    "                if not anns_img or len(anns_img) > max_boxes:\n",
    "                    return None\n",
    "\n",
    "                bboxes = [a[\"bbox\"] for a in anns_img]\n",
    "                labels = [a[\"category_id\"] for a in anns_img]\n",
    "\n",
    "                transformed = self.pipeline(image=img, bboxes=bboxes, category_ids=labels)\n",
    "                if not transformed or not transformed.get(\"bboxes\"):\n",
    "                    return None\n",
    "\n",
    "                aug_img = np.clip(transformed[\"image\"], 0, 255).astype(np.uint8)\n",
    "                if aug_img.size == 0:\n",
    "                    return None\n",
    "\n",
    "                h, w = aug_img.shape[:2]\n",
    "                clamped_bboxes = []\n",
    "                for bbox in transformed[\"bboxes\"]:\n",
    "                    x, y, bw, bh = bbox\n",
    "                    x = max(0, min(x, w - 1))\n",
    "                    y = max(0, min(y, h - 1))\n",
    "                    bw = max(1.0, min(bw, w - x))\n",
    "                    bh = max(1.0, min(bh, h - y))\n",
    "                    clamped_bboxes.append([x, y, bw, bh])\n",
    "\n",
    "                # --- Assign IDs and save ---\n",
    "                with id_lock:\n",
    "                    new_img_id = next(img_id_counter)\n",
    "                new_name = f\"{Path(fname).stem}_rebalance_{uuid4().hex[:8]}.jpg\"\n",
    "                out_path = self.rebalance_dir / new_name\n",
    "                Image.fromarray(aug_img).save(out_path, format=\"JPEG\", quality=95)\n",
    "\n",
    "                new_images.append({\n",
    "                    \"id\": new_img_id,\n",
    "                    \"file_name\": new_name,\n",
    "                    \"orig_file\": fname,\n",
    "                    \"width\": w,\n",
    "                    \"height\": h,\n",
    "                    \"source_stage\": f\"rebalance_{iteration}\"\n",
    "                })\n",
    "\n",
    "                for bbox, cid in zip(clamped_bboxes, transformed[\"category_ids\"]):\n",
    "                    with id_lock:\n",
    "                        ann_id = next(ann_id_counter)\n",
    "                    new_anns.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": new_img_id,\n",
    "                        \"category_id\": cid,\n",
    "                        \"bbox\": [float(x) for x in bbox],\n",
    "                        \"area\": float(bbox[2] * bbox[3]),\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"source_stage\": f\"rebalance_{iteration}\"\n",
    "                    })\n",
    "                return True\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"[REBALANCE] Error augmenting {img_id}: {e}\")\n",
    "                return None\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Parallel execution\n",
    "        # ==========================================================\n",
    "        start_time = time.time()\n",
    "        futures = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            plan = self.rebalance_sampling_plan\n",
    "            if \"selected_ids\" in plan:\n",
    "                selected_ids = plan[\"selected_ids\"]\n",
    "                for img_id in selected_ids:\n",
    "                    futures.append(executor.submit(_augment_image, int(img_id)))\n",
    "            else:\n",
    "                for cls_name, entry in plan.items():\n",
    "                    for img_id in entry.get(\"selected_ids\", []):\n",
    "                        futures.append(executor.submit(_augment_image, int(img_id), cls_name))\n",
    "\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Rebalance iter {iteration}\"):\n",
    "                pass  # progress tracking only\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Save JSON for this iteration\n",
    "        # ==========================================================\n",
    "        valid_img_ids = {img[\"id\"] for img in new_images}\n",
    "        new_anns = [a for a in new_anns if a[\"image_id\"] in valid_img_ids]\n",
    "\n",
    "        json_out = {\n",
    "            \"images\": new_images,\n",
    "            \"annotations\": new_anns,\n",
    "            \"categories\": cats,\n",
    "            \"_meta\": {\n",
    "                \"split\": self.split,\n",
    "                \"iteration\": iteration,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"version\": getattr(self, \"version\", \"unknown\"),\n",
    "                \"source_used\": source,\n",
    "                \"total_generated\": len(new_images),\n",
    "                \"total_annotations\": len(new_anns),\n",
    "                \"categories_used\": len(cats)\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(rebalance_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_out, f, indent=2)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Summary logging\n",
    "        # ==========================================================\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        self.logger.info(f\"[REBALANCE] Iteration {iteration} completed â†’ {len(new_images)} images | {len(new_anns)} annots | {duration:.2f}s\")\n",
    "        self.logger.info(f\"[REBALANCE] JSON saved to {rebalance_json_path}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Return structured result\n",
    "        # ==========================================================\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"iteration\": iteration,\n",
    "            \"rebalance_applied\": True,\n",
    "            \"augmented_images\": len(new_images),\n",
    "            \"augmented_annotations\": len(new_anns),\n",
    "            \"output_dir\": str(self.rebalance_dir),\n",
    "            \"json_path\": str(rebalance_json_path),\n",
    "            \"duration_sec\": duration\n",
    "        }\n",
    "\n",
    "    def apply_adaptive_rebalance_images(self) -> dict:\n",
    "        \"\"\"\n",
    "        Iteratively apply class-aware rebalance augmentation until balance tolerance is reached.\n",
    "        Each iteration:\n",
    "          - Selects minority images probabilistically\n",
    "          - Generates new augmented samples in its own folder (rebalance_i/)\n",
    "          - Merges outputs into the final dataset\n",
    "        After all iterations, a minority-focus zoom reinforcement is applied (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ CONFIGURATION\n",
    "        # ==========================================================\n",
    "        cfg = self.config.get(\"augmentations\", {})\n",
    "        max_iter = int(cfg.get(\"adaptive_rebalance_iter\", 5))\n",
    "        proportion = float(cfg.get(\"adaptive_rebalance_prop\", 0.1))\n",
    "        dominance_thr = float(cfg.get(\"adaptive_minority_dominance\", 0.7))\n",
    "        focus_prop = float(cfg.get(\"focus_reinforce_prop\", 0.1))\n",
    "        focus_scale = float(cfg.get(\"focus_zoom_scale\", 1.5))\n",
    "        focus_offset = float(cfg.get(\"focus_zoom_offset\", 0.2))\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"[ADAPT] Starting adaptive rebalance for split='{self.split}' \"\n",
    "            f\"(max_iter={max_iter}, prop={proportion}, dominance_thr={dominance_thr})\"\n",
    "        )\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Ensure base dataset exists\n",
    "        # ==========================================================\n",
    "        if not self.final_json.exists():\n",
    "            self.logger.warning(\"[ADAPT] Final JSON not found. Attempting to build it via merge...\")\n",
    "            try:\n",
    "                self.merge_augmented_splits(reindex=True)\n",
    "            except Exception as e:\n",
    "                raise FileNotFoundError(f\"[ADAPT] Could not initialize final dataset: {e}\")\n",
    "\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[ADAPT] Missing final JSON: {self.final_json}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Initial setup\n",
    "        # ==========================================================\n",
    "        achieved = False\n",
    "        total_new_images = 0\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        total_start = time.time()\n",
    "        previous_gen = 0\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Iterative rebalance loop\n",
    "        # ==========================================================\n",
    "        for iteration in range(1, max_iter + 1):\n",
    "            self.logger.info(f\"[ADAPT] === Iteration {iteration}/{max_iter} ===\")\n",
    "\n",
    "            # Step 1: Analyze current balance\n",
    "            summary = self.summarize_balance_from_json(self.final_json)\n",
    "            minority_classes = summary.get(\"minority_classes\", [])\n",
    "            if not minority_classes:\n",
    "                self.logger.info(\"[ADAPT] Dataset is balanced. No minority classes remain.\")\n",
    "                achieved = True\n",
    "                break\n",
    "\n",
    "            # Step 2: Load data and compute class dominance\n",
    "            with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            anns = data[\"annotations\"]\n",
    "            cats = {c[\"id\"]: c[\"name\"] for c in data[\"categories\"]}\n",
    "\n",
    "            img_class_map, img_class_counts = {}, {}\n",
    "            for a in anns:\n",
    "                cname = cats[a[\"category_id\"]]\n",
    "                img_class_map.setdefault(a[\"image_id\"], set()).add(cname)\n",
    "                img_class_counts.setdefault(a[\"image_id\"], {}).setdefault(cname, 0)\n",
    "                img_class_counts[a[\"image_id\"]][cname] += 1\n",
    "\n",
    "            minority_set = set(minority_classes)\n",
    "            eligible_primary, eligible_mixed = [], []\n",
    "\n",
    "            for img_id, cls_set in img_class_map.items():\n",
    "                if not (cls_set & minority_set):\n",
    "                    continue\n",
    "                counts = img_class_counts[img_id]\n",
    "                minority_total = sum(v for k, v in counts.items() if k in minority_set)\n",
    "                majority_total = sum(v for k, v in counts.items() if k not in minority_set)\n",
    "                minority_ratio = minority_total / max(minority_total + majority_total, 1)\n",
    "                if minority_ratio >= dominance_thr:\n",
    "                    eligible_primary.append(img_id)\n",
    "                else:\n",
    "                    eligible_mixed.append(img_id)\n",
    "\n",
    "            # Step 3: Sampling for current iteration\n",
    "            n_total = len(eligible_primary) + len(eligible_mixed)\n",
    "            if n_total == 0:\n",
    "                self.logger.warning(\"[ADAPT] No eligible images found for this iteration.\")\n",
    "                break\n",
    "\n",
    "            n_select = max(1, int(np.ceil(n_total * proportion)))\n",
    "            n_primary = int(n_select * 0.8)\n",
    "            n_mixed = n_select - n_primary\n",
    "\n",
    "            selected_ids = []\n",
    "            if eligible_primary:\n",
    "                selected_ids.extend(\n",
    "                    rng.choice(eligible_primary, size=min(n_primary, len(eligible_primary)), replace=False)\n",
    "                )\n",
    "            if eligible_mixed:\n",
    "                selected_ids.extend(\n",
    "                    rng.choice(eligible_mixed, size=min(n_mixed, len(eligible_mixed)), replace=False)\n",
    "                )\n",
    "\n",
    "            self.rebalance_sampling_plan = {\"selected_ids\": [int(x) for x in selected_ids]}\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"[ADAPT] Selected {len(selected_ids)} images \"\n",
    "                f\"(primary={len(eligible_primary)}, mixed={len(eligible_mixed)}) \"\n",
    "                f\"for rebalance iteration {iteration}.\"\n",
    "            )\n",
    "\n",
    "            # Step 4: Apply augmentation for this iteration\n",
    "            self.logger.info(f\"[ADAPT] Applying corrective augmentation (iteration {iteration})...\")\n",
    "            reb_report = self.apply_rebalance_augmentation(source=\"final\", iteration=iteration)\n",
    "\n",
    "            new_imgs = reb_report.get(\"augmented_images\", 0)\n",
    "            new_anns = reb_report.get(\"augmented_annotations\", 0)\n",
    "            total_new_images += new_imgs\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"[REBALANCE] Iter {iteration} â†’ {new_imgs} images | {new_anns} annotations\"\n",
    "            )\n",
    "\n",
    "            # Step 5: Merge results into final dataset\n",
    "            self.logger.info(\"[ADAPT] Merging iteration output into final dataset...\")\n",
    "            try:\n",
    "                self.merge_augmented_splits(reindex=True, clean_previous=True)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[ADAPT] Merge failed at iteration {iteration}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Step 6: Evaluate new balance\n",
    "            try:\n",
    "                eval_report = self.evaluate_rebalance_result(show_plot=False)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[ADAPT] Evaluation failed: {e}\")\n",
    "                eval_report = {}\n",
    "\n",
    "            if eval_report.get(\"within_tolerance\", False):\n",
    "                self.logger.info(f\"[ADAPT] Balance achieved within tolerance at iteration {iteration}.\")\n",
    "                achieved = True\n",
    "                break\n",
    "\n",
    "            # Step 7: Detect stagnation (no progress)\n",
    "            if new_imgs == 0 or new_imgs == previous_gen:\n",
    "                self.logger.warning(f\"[ADAPT] No progress detected at iteration {iteration}. Stopping early.\")\n",
    "                break\n",
    "            previous_gen = new_imgs\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Minority focus reinforcement (optional zoom)\n",
    "        # ==========================================================\n",
    "        if focus_prop > 0:\n",
    "            try:\n",
    "                self.logger.info(\n",
    "                    f\"[FOCUS] Applying minority zoom reinforcement \"\n",
    "                    f\"(prop={focus_prop}, scale={focus_scale}, offset={focus_offset})...\"\n",
    "                )\n",
    "                reinforce_report = self.apply_minority_focus_zoom()\n",
    "                total_new_images += reinforce_report.get(\"generated\", 0)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[FOCUS] Zoom reinforcement failed or skipped: {e}\")\n",
    "        else:\n",
    "            self.logger.info(\"[FOCUS] Zoom reinforcement disabled in configuration.\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Final summary\n",
    "        # ==========================================================\n",
    "        duration = round(time.time() - total_start, 2)\n",
    "        status = \"success\" if achieved else \"max_iter_reached\"\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"[ADAPT] Finished after {iteration} iteration(s) | \"\n",
    "            f\"Total new images={total_new_images} | Status={status.upper()} | Duration={duration:.2f}s\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"adaptive_applied\": True,\n",
    "            \"achieved_tolerance\": achieved,\n",
    "            \"iterations\": iteration,\n",
    "            \"iterations_run\": iteration,\n",
    "            \"status\": status,\n",
    "            \"total_new_images\": total_new_images,\n",
    "            \"duration_sec\": duration\n",
    "        }\n",
    "\n",
    "    def merge_augmented_splits(self, reindex: bool = True, clean_previous: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Merge all dataset stages (clean, proportional, rebalance iterations, and zoom)\n",
    "        into a unified final split. Automatically detects and merges all available\n",
    "        augmentation folders in logical order.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reindex : bool\n",
    "            Whether to reassign sequential IDs to images and annotations after merging.\n",
    "        clean_previous : bool\n",
    "            If True, deletes all previous content inside `final_dir` before merging.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Summary of merge results, including number of images, annotations,\n",
    "            and duration in seconds.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Optional cleanup\n",
    "        # ==========================================================\n",
    "        if clean_previous:\n",
    "            self.logger.info(f\"[MERGE] Cleaning previous contents from {self.final_dir}...\")\n",
    "            for p in self.final_dir.rglob(\"*\"):\n",
    "                try:\n",
    "                    if p.is_file():\n",
    "                        p.unlink()\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"[MERGE] Could not remove {p}: {e}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Collect datasets in correct merge order\n",
    "        # ==========================================================\n",
    "        stages = []\n",
    "\n",
    "        # Clean base\n",
    "        if getattr(self, \"clean_json\", None) and Path(self.clean_json).exists():\n",
    "            stages.append((\"clean\", self.clean_json, self.clean_dir))\n",
    "\n",
    "        # Proportional augmentation\n",
    "        if getattr(self, \"prop_json\", None) and Path(self.prop_json).exists():\n",
    "            stages.append((\"prop\", self.prop_json, self.prop_dir))\n",
    "\n",
    "        # Rebalance iterations (dynamic detection)\n",
    "        rebalance_dirs = sorted(self.mirror_dir.glob(f\"{self.split}_rebalance_*\"))\n",
    "        for d in rebalance_dirs:\n",
    "            jpath = d / f\"{self.split}_rebalance_{d.name.split('_')[-1]}.json\"\n",
    "            if jpath.exists():\n",
    "                stages.append((f\"rebalance_{d.name.split('_')[-1]}\", jpath, d))\n",
    "\n",
    "        # Zoom reinforcement\n",
    "        if hasattr(self, \"zoom_json\") and Path(self.zoom_json).exists():\n",
    "            stages.append((\"zoom\", self.zoom_json, self.zoom_dir))\n",
    "\n",
    "        if not stages:\n",
    "            raise RuntimeError(f\"[MERGE] No valid dataset stages found for split='{self.split}'.\")\n",
    "\n",
    "        self.logger.info(f\"[MERGE] Found {len(stages)} dataset stages: {[s[0] for s in stages]}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Initialize accumulators\n",
    "        # ==========================================================\n",
    "        merged_imgs, merged_anns = [], []\n",
    "        img_counter = itertools.count(1)\n",
    "        ann_counter = itertools.count(1)\n",
    "        id_map = {}\n",
    "        seen_files = set()\n",
    "        valid_exts = {\".jpg\", \".jpeg\", \".png\"}\n",
    "        cats = None\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Merge datasets sequentially\n",
    "        # ==========================================================\n",
    "        for stage, json_path, img_dir in stages:\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    d = json.load(f)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[MERGE] Failed to load {stage} JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "            imgs = d.get(\"images\", [])\n",
    "            anns = d.get(\"annotations\", [])\n",
    "            cats = d.get(\"categories\", cats)\n",
    "\n",
    "            if not imgs or not cats:\n",
    "                self.logger.warning(f\"[MERGE] Skipping {stage} (empty or invalid).\")\n",
    "                continue\n",
    "\n",
    "            self.logger.info(f\"[MERGE] Adding {len(imgs)} images from stage '{stage}'...\")\n",
    "\n",
    "            # --- Add images\n",
    "            for img in imgs:\n",
    "                fname = Path(img[\"file_name\"]).name\n",
    "                if fname in seen_files:\n",
    "                    continue\n",
    "                seen_files.add(fname)\n",
    "\n",
    "                new_id = next(img_counter)\n",
    "                id_map[img[\"id\"]] = new_id\n",
    "\n",
    "                merged_imgs.append({\n",
    "                    \"id\": new_id,\n",
    "                    \"file_name\": fname,\n",
    "                    \"orig_file\": img.get(\"orig_file\", fname),\n",
    "                    \"width\": img.get(\"width\", 0),\n",
    "                    \"height\": img.get(\"height\", 0),\n",
    "                    \"source_stage\": img.get(\"source_stage\", stage)\n",
    "                })\n",
    "\n",
    "                # Copy image file safely\n",
    "                src_path = Path(img_dir) / fname\n",
    "                if not src_path.exists():\n",
    "                    # Try alternate case-insensitive matches\n",
    "                    base_name = Path(fname).stem\n",
    "                    for ext in valid_exts:\n",
    "                        alt = list(Path(img_dir).rglob(f\"{base_name}{ext}\"))\n",
    "                        if alt:\n",
    "                            src_path = alt[0]\n",
    "                            break\n",
    "                if src_path and src_path.exists():\n",
    "                    dst_path = self.final_dir / fname\n",
    "                    try:\n",
    "                        if not dst_path.exists():\n",
    "                            shutil.copy2(src_path, dst_path)\n",
    "                    except Exception as e:\n",
    "                        self.logger.debug(f\"[MERGE] Copy failed for {fname}: {e}\")\n",
    "                else:\n",
    "                    self.logger.debug(f\"[MERGE] Missing file for {fname} ({stage})\")\n",
    "\n",
    "            # --- Add annotations\n",
    "            for ann in anns:\n",
    "                old_img = ann.get(\"image_id\")\n",
    "                new_img = id_map.get(old_img)\n",
    "                if new_img is None:\n",
    "                    continue\n",
    "                new_anns = ann.copy()\n",
    "                new_anns[\"id\"] = next(ann_counter)\n",
    "                new_anns[\"image_id\"] = new_img\n",
    "                new_anns[\"source_stage\"] = ann.get(\"source_stage\", stage)\n",
    "                merged_anns.append(new_anns)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Reindex IDs (optional)\n",
    "        # ==========================================================\n",
    "        if reindex:\n",
    "            self.logger.info(\"[MERGE] Reindexing all IDs for consistency...\")\n",
    "            img_id_map = {img[\"id\"]: idx + 1 for idx, img in enumerate(merged_imgs)}\n",
    "            ann_id_map = itertools.count(1)\n",
    "\n",
    "            for img in merged_imgs:\n",
    "                img[\"id\"] = img_id_map[img[\"id\"]]\n",
    "\n",
    "            for ann in merged_anns:\n",
    "                ann[\"id\"] = next(ann_id_map)\n",
    "                ann[\"image_id\"] = img_id_map.get(ann[\"image_id\"], ann[\"image_id\"])\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Save final merged JSON\n",
    "        # ==========================================================\n",
    "        json_out = {\n",
    "            \"images\": merged_imgs,\n",
    "            \"annotations\": merged_anns,\n",
    "            \"categories\": cats,\n",
    "            \"_meta\": {\n",
    "                \"split\": self.split,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"stages_merged\": [s[0] for s in stages],\n",
    "                \"total_images\": len(merged_imgs),\n",
    "                \"total_annotations\": len(merged_anns),\n",
    "                \"reindexed\": reindex,\n",
    "                \"clean_previous\": clean_previous\n",
    "            }\n",
    "        }\n",
    "\n",
    "        tmp_path = self.final_json.with_suffix(\".tmp\")\n",
    "        try:\n",
    "            with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(json_out, f, indent=2)\n",
    "            tmp_path.replace(self.final_json)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[MERGE] Failed to save final JSON: {e}\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Summary\n",
    "        # ==========================================================\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        self.logger.info(f\"[MERGE] Final dataset assembled successfully for split='{self.split}'.\")\n",
    "        self.logger.info(f\"         â†’ {len(merged_imgs)} images | {len(merged_anns)} annotations | {duration:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"images\": len(merged_imgs),\n",
    "            \"annotations\": len(merged_anns),\n",
    "            \"stages_merged\": [s[0] for s in stages],\n",
    "            \"output_dir\": str(self.final_dir),\n",
    "            \"json_path\": str(self.final_json),\n",
    "            \"duration_sec\": duration\n",
    "        }\n",
    "\n",
    "    def verify_integrity(self) -> dict:\n",
    "        \"\"\"\n",
    "        Verify consistency between JSON annotations and image files.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[VERIFY] Missing final JSON: {self.final_json}\")\n",
    "\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        imgs = pd.DataFrame(data.get(\"images\", []))\n",
    "        anns = pd.DataFrame(data.get(\"annotations\", []))\n",
    "        cats = pd.DataFrame(data.get(\"categories\", []))\n",
    "        img_dir = Path(self.final_dir)\n",
    "\n",
    "        if imgs.empty:\n",
    "            raise ValueError(\"[VERIFY] No image entries found in final JSON.\")\n",
    "\n",
    "        # 1. Gather valid image filenames safely\n",
    "        valid_exts = {\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"}\n",
    "        file_names = set()\n",
    "        for p in img_dir.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix in valid_exts:\n",
    "                file_names.add(p.name)\n",
    "\n",
    "        img_names = set(imgs[\"file_name\"].unique())\n",
    "\n",
    "        # 2. Detect inconsistencies\n",
    "        missing_files = sorted([n for n in img_names if n not in file_names])\n",
    "        orphan_files = sorted([f for f in file_names if f not in img_names])\n",
    "\n",
    "        dup_img_ids = imgs[\"id\"][imgs[\"id\"].duplicated()].tolist() if \"id\" in imgs else []\n",
    "        dup_ann_ids = anns[\"id\"][anns[\"id\"].duplicated()].tolist() if \"id\" in anns else []\n",
    "\n",
    "        # --- Orphan annotations ---\n",
    "        orphan_ann_count = 0\n",
    "        valid_ann_count = 0\n",
    "        if not anns.empty and \"image_id\" in anns and \"id\" in imgs:\n",
    "            valid_img_ids = set(imgs[\"id\"])\n",
    "            orphan_mask = ~anns[\"image_id\"].isin(valid_img_ids)\n",
    "            orphan_ann_count = int(orphan_mask.sum())\n",
    "            anns = anns[~orphan_mask].copy()  # remove orphans before next checks\n",
    "            valid_ann_count = len(anns)\n",
    "\n",
    "        # --- Empty categories ---\n",
    "        empty_cats = []\n",
    "        if not cats.empty:\n",
    "            cat_ids = set(cats[\"id\"])\n",
    "            used_cats = set(anns[\"category_id\"].unique()) if not anns.empty else set()\n",
    "            unused_ids = sorted([c for c in cat_ids if c not in used_cats])\n",
    "            if unused_ids:\n",
    "                cat_map = dict(zip(cats[\"id\"], cats[\"name\"]))\n",
    "                empty_cats = [cat_map[i] for i in unused_ids if i in cat_map]\n",
    "\n",
    "        # --- Zero-area annotations ---\n",
    "        zero_area_anns = 0\n",
    "        if not anns.empty and \"area\" in anns:\n",
    "            zero_mask = anns[\"area\"] <= 0\n",
    "            zero_area_anns = int(zero_mask.sum())\n",
    "            anns = anns[~zero_mask].copy()  # filter them out for cleaner stats\n",
    "\n",
    "        # 3. Compute integrity metrics\n",
    "        total_imgs = len(imgs)\n",
    "        total_anns = len(anns) + orphan_ann_count + zero_area_anns\n",
    "        total_files = len(file_names)\n",
    "\n",
    "        integrity_img = 100 * (1 - len(missing_files) / max(total_imgs, 1))\n",
    "        integrity_ann = 100 * (1 - orphan_ann_count / max(total_anns, 1))\n",
    "        integrity_global = round((integrity_img + integrity_ann) / 2, 2)\n",
    "\n",
    "        # 4. Build report\n",
    "        report = {\n",
    "            \"split\": self.split,\n",
    "            \"images_total\": total_imgs,\n",
    "            \"annotations_total\": total_anns,\n",
    "            \"missing_images\": missing_files,\n",
    "            \"orphan_images\": orphan_files,\n",
    "            \"duplicate_image_ids\": dup_img_ids,\n",
    "            \"duplicate_annotation_ids\": dup_ann_ids,\n",
    "            \"orphan_annotations\": orphan_ann_count,\n",
    "            \"valid_annotations\": valid_ann_count,\n",
    "            \"empty_categories\": empty_cats,\n",
    "            \"zero_area_annotations\": zero_area_anns,\n",
    "            \"integrity_image_pct\": round(integrity_img, 2),\n",
    "            \"integrity_annotation_pct\": round(integrity_ann, 2),\n",
    "            \"integrity_global_pct\": integrity_global,\n",
    "            \"duration_sec\": round(time.time() - start_time, 2),\n",
    "            \"missing_files_list\": missing_files[:10],\n",
    "            \"duplicate_files_list\": dup_img_ids[:10]\n",
    "\n",
    "        }\n",
    "\n",
    "        # 5. Logging summary\n",
    "        self.logger.info(f\"[VERIFY] Integrity check for split='{self.split}':\")\n",
    "        self.logger.info(f\"       -> Missing image files: {len(missing_files)}\")\n",
    "        self.logger.info(f\"       -> Orphan image files: {len(orphan_files)}\")\n",
    "        self.logger.info(f\"       -> Orphan annotations: {orphan_ann_count}\")\n",
    "        self.logger.info(f\"       -> Duplicated IDs: images={len(dup_img_ids)} | annotations={len(dup_ann_ids)}\")\n",
    "        self.logger.info(f\"       -> Empty categories: {len(empty_cats)} | Zero-area annots: {zero_area_anns}\")\n",
    "        self.logger.info(f\"       -> Integrity (images={integrity_img:.2f}% | annotations={integrity_ann:.2f}%)\")\n",
    "        self.logger.info(f\"       -> Overall integrity score: {integrity_global:.2f}%\")\n",
    "\n",
    "        if integrity_global < 99.0:\n",
    "            self.logger.warning(\"[VERIFY] Integrity issues detected â€” review details in report.\")\n",
    "        else:\n",
    "            self.logger.info(\"[VERIFY] All integrity checks passed successfully.\")\n",
    "\n",
    "        # 6. Save report\n",
    "        try:\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            report_path = self.output_dir / f\"integrity_report_{self.split}.json\"\n",
    "            with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "            self.logger.info(f\"[VERIFY] Integrity report saved â†’ {report_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[VERIFY] Failed to save integrity report: {e}\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def plot_balance_progression(self) -> str | None:\n",
    "        \"\"\"\n",
    "        Compare class distributions across stages. Generates and saves\n",
    "        \"\"\"\n",
    "        out_dir = getattr(self, \"output_dir\", self.root_dir / self.split)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        stages = {\n",
    "            \"clean\": self.clean_json,\n",
    "            \"prop\": getattr(self, \"prop_json\", None),\n",
    "            \"final\": getattr(self, \"final_json\", None)\n",
    "        }\n",
    "        available = {k: v for k, v in stages.items() if v and Path(v).exists()}\n",
    "\n",
    "        if len(available) < 2:\n",
    "            self.logger.warning(\"[REPORT] Not enough valid datasets to compare balance progression.\")\n",
    "            return None\n",
    "        def _count_classes(json_path, stage):\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                anns = pd.DataFrame(data.get(\"annotations\", []))\n",
    "                cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[REPORT] Failed to read {json_path}: {e}\")\n",
    "                return pd.DataFrame(columns=[\"class\", \"count\", \"stage\"])\n",
    "\n",
    "            if anns.empty or not cats:\n",
    "                return pd.DataFrame(columns=[\"class\", \"count\", \"stage\"])\n",
    "\n",
    "            anns[\"class\"] = anns[\"category_id\"].map(cats)\n",
    "            df = anns[\"class\"].value_counts().reset_index()\n",
    "            df.columns = [\"class\", \"count\"]\n",
    "            df[\"stage\"] = stage\n",
    "            return df\n",
    "        df_all = pd.concat(\n",
    "            [_count_classes(path, stage) for stage, path in available.items()],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        if df_all.empty:\n",
    "            self.logger.warning(\"[REPORT] No valid data for plotting balance progression.\")\n",
    "            return None\n",
    "\n",
    "        df_all[\"class\"] = df_all[\"class\"].astype(str)\n",
    "\n",
    "        all_classes = sorted(df_all[\"class\"].unique())\n",
    "        all_stages = sorted(df_all[\"stage\"].unique())\n",
    "\n",
    "        df_full = (\n",
    "            df_all.pivot(index=\"class\", columns=\"stage\", values=\"count\")\n",
    "            .reindex(index=all_classes, columns=all_stages, fill_value=0)\n",
    "            .reset_index()\n",
    "            .melt(id_vars=\"class\", var_name=\"stage\", value_name=\"count\")\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        palette = [\"#4C72B0\", \"#55A868\", \"#C44E52\"]\n",
    "        ax = sns.barplot(\n",
    "            data=df_full,\n",
    "            x=\"class\",\n",
    "            y=\"count\",\n",
    "            hue=\"stage\",\n",
    "            palette=palette,\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt=\"%d\", label_type=\"edge\", fontsize=8, padding=2)\n",
    "\n",
    "        plt.title(f\"Class balance progression - {self.split}\", fontsize=13, fontweight=\"bold\")\n",
    "        plt.xlabel(\"Class\", fontsize=11)\n",
    "        plt.ylabel(\"Annotation count\", fontsize=11)\n",
    "        plt.xticks(rotation=25, ha=\"right\")\n",
    "        plt.legend(title=\"Stage\", loc=\"upper right\", frameon=True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save plot and table\n",
    "        plot_path = out_dir / f\"{self.split}_balance_progression.png\"\n",
    "        csv_path = out_dir / f\"balance_progression_{self.split}.csv\"\n",
    "\n",
    "        try:\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            df_full.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "            self.logger.info(f\"[REPORT] Balance progression plot saved -> {plot_path}\")\n",
    "            self.logger.info(f\"[REPORT] Data table saved -> {csv_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[REPORT] Failed to save plot or CSV: {e}\")\n",
    "            return None\n",
    "\n",
    "        df_wide = df_full.pivot(index=\"class\", columns=\"stage\", values=\"count\").fillna(0)\n",
    "        if \"clean\" in df_wide.columns and \"final\" in df_wide.columns:\n",
    "            df_wide[\"Î”_final_vs_clean_%\"] = (\n",
    "                (df_wide[\"final\"] - df_wide[\"clean\"]) / df_wide[\"clean\"].replace(0, np.nan) * 100\n",
    "            ).round(1)\n",
    "            for cls, row in df_wide.iterrows():\n",
    "                delta = row[\"Î”_final_vs_clean_%\"]\n",
    "                sign = \"â†‘\" if delta > 0 else (\"â†“\" if delta < 0 else \"->\")\n",
    "                self.logger.info(f\"[REPORT] {cls:<15}: clean={int(row['clean']):>5}, final={int(row['final']):>5}, Î”={delta:>6.1f}% {sign}\")\n",
    "\n",
    "        return str(plot_path)\n",
    "\n",
    "    def generate_report(self) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a unified JSON summary of all augmentation stages.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        out_dir = getattr(self, \"output_dir\", self.root_dir / self.split)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger.info(f\"[REPORT] Generating unified augmentation summary for split='{self.split}'...\")\n",
    "\n",
    "        summary = {\n",
    "            \"split\": self.split,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"version\": getattr(self, \"version\", \"unknown\"),\n",
    "            \"configuration\": {\n",
    "                \"mode\": getattr(self, \"mode\", None),\n",
    "                \"proportion\": getattr(self, \"proportion\", None),\n",
    "                \"tolerance\": getattr(self, \"tolerance\", None),\n",
    "                \"num_workers\": getattr(self, \"num_workers\", None),\n",
    "                \"seed\": getattr(self, \"seed\", None),\n",
    "                \"classes\": getattr(self, \"classes\", []),\n",
    "            },\n",
    "            \"stages\": {},\n",
    "            \"files\": {},\n",
    "            \"metrics\": {},\n",
    "        }\n",
    "\n",
    "        def _extract_counts(path):\n",
    "            if not path or not Path(path).exists():\n",
    "                return {\"images\": 0, \"annotations\": 0}\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                return {\n",
    "                    \"images\": len(data.get(\"images\", [])),\n",
    "                    \"annotations\": len(data.get(\"annotations\", []))\n",
    "                }\n",
    "            except Exception:\n",
    "                return {\"images\": 0, \"annotations\": 0}\n",
    "\n",
    "        # Stage-wise statistics\n",
    "        summary[\"stages\"][\"clean\"] = _extract_counts(self.clean_json)\n",
    "        summary[\"stages\"][\"proportional\"] = _extract_counts(getattr(self, \"prop_json\", None))\n",
    "        summary[\"stages\"][\"rebalance\"] = _extract_counts(getattr(self, \"rebalance_json\", None))\n",
    "        summary[\"stages\"][\"final\"] = _extract_counts(getattr(self, \"final_json\", None))\n",
    "\n",
    "        # File references\n",
    "        plot_path = out_dir / f\"{self.split}_balance_progression.png\"\n",
    "        summary[\"files\"] = {\n",
    "            \"clean_json\": str(self.clean_json),\n",
    "            \"prop_json\": str(getattr(self, \"prop_json\", \"\")),\n",
    "            \"rebalance_json\": str(getattr(self, \"rebalance_json\", \"\")),\n",
    "            \"final_json\": str(getattr(self, \"final_json\", \"\")),\n",
    "            \"balance_plot\": str(plot_path),\n",
    "        }\n",
    "\n",
    "        # Compute high-level metrics\n",
    "        clean_anns = summary[\"stages\"][\"clean\"][\"annotations\"]\n",
    "        prop_anns = summary[\"stages\"][\"proportional\"][\"annotations\"]\n",
    "        reb_anns = summary[\"stages\"][\"rebalance\"][\"annotations\"]\n",
    "        final_anns = summary[\"stages\"][\"final\"][\"annotations\"]\n",
    "\n",
    "        delta_prop = prop_anns - clean_anns\n",
    "        delta_reb = reb_anns - prop_anns\n",
    "        delta_final = final_anns - clean_anns\n",
    "\n",
    "        pct_prop = (delta_prop / clean_anns * 100) if clean_anns else 0\n",
    "        pct_reb = (delta_reb / prop_anns * 100) if prop_anns else 0\n",
    "        pct_final = (delta_final / clean_anns * 100) if clean_anns else 0\n",
    "\n",
    "        durations = {\n",
    "            \"proportional_sec\": getattr(self, \"prop_duration\", None),\n",
    "            \"rebalance_sec\": getattr(self, \"rebalance_duration\", None),\n",
    "            \"total_sec\": round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "        summary[\"metrics\"] = {\n",
    "            \"delta_annotations\": {\n",
    "                \"proportional_vs_clean\": int(delta_prop),\n",
    "                \"rebalance_vs_prop\": int(delta_reb),\n",
    "                \"final_vs_clean\": int(delta_final),\n",
    "            },\n",
    "            \"percent_change_annotations\": {\n",
    "                \"proportional_vs_clean\": round(pct_prop, 2),\n",
    "                \"rebalance_vs_prop\": round(pct_reb, 2),\n",
    "                \"final_vs_clean\": round(pct_final, 2),\n",
    "            },\n",
    "            \"durations\": durations,\n",
    "        }\n",
    "\n",
    "        # Save JSON summary\n",
    "        summary_path = out_dir / f\"{self.split}_augmentation_summary.json\"\n",
    "        tmp_path = summary_path.with_suffix(\".tmp\")\n",
    "        try:\n",
    "            with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            tmp_path.replace(summary_path)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[REPORT] Failed to write summary: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        # Save CSV version for quick inspection\n",
    "        try:\n",
    "            df_stages = pd.DataFrame(summary[\"stages\"]).T\n",
    "            df_stages[\"stage\"] = df_stages.index\n",
    "            df_stages.to_csv(out_dir / f\"augmentation_summary_{self.split}.csv\", index=False, encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[REPORT] Failed to export CSV summary: {e}\")\n",
    "\n",
    "        # Logging overview\n",
    "        self.logger.info(f\"[REPORT]  Augmentation summary saved -> {summary_path}\")\n",
    "        self.logger.info(f\"       -> Î” prop vs clean: {delta_prop:+} ({pct_prop:.2f}%)\")\n",
    "        self.logger.info(f\"       -> Î” rebalance vs prop: {delta_reb:+} ({pct_reb:.2f}%)\")\n",
    "        self.logger.info(f\"       -> Î” final vs clean: {delta_final:+} ({pct_final:.2f}%)\")\n",
    "        self.logger.info(f\"         Total report generation time: {durations['total_sec']:.2f}s\")\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def plot_final_annotation_distribution(self, show: bool = True) -> str | None:\n",
    "        \"\"\"\n",
    "        Plot the final annotation distribution per class after all augmentations.\n",
    "        \"\"\"\n",
    "        if not self.final_json.exists():\n",
    "            self.logger.warning(\"[PLOT] Final JSON not found. Skipping final distribution plot.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[PLOT] Failed to read final JSON: {e}\")\n",
    "            return None\n",
    "\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = {c[\"id\"]: c[\"name\"] for c in data.get(\"categories\", [])}\n",
    "        if not anns or not cats:\n",
    "            self.logger.warning(\"[PLOT] No annotations or categories found in final JSON.\")\n",
    "            return None\n",
    "\n",
    "        # Count annotations per class\n",
    "        df = pd.DataFrame(anns)\n",
    "        df[\"class\"] = df[\"category_id\"].map(cats)\n",
    "        class_counts = df[\"class\"].value_counts().reset_index()\n",
    "        class_counts.columns = [\"class\", \"count\"]\n",
    "        class_counts = class_counts.sort_values(\"class\")\n",
    "        total = class_counts[\"count\"].sum()\n",
    "        class_counts[\"percent\"] = (class_counts[\"count\"] / total * 100).round(1)\n",
    "\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        ax = sns.barplot(\n",
    "            data=class_counts,\n",
    "            x=\"class\",\n",
    "            y=\"count\",\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "\n",
    "        for i, row in enumerate(class_counts.itertuples()):\n",
    "            ax.text(\n",
    "                i,\n",
    "                row.count + max(class_counts[\"count\"]) * 0.02,\n",
    "                f\"{row.count} ({row.percent:.1f}%)\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "                color=\"black\"\n",
    "            )\n",
    "\n",
    "        plt.title(f\"DistribuciÃ³n final PostAugmentation - {self.split.upper()}\", fontsize=13, weight=\"bold\")\n",
    "        plt.xlabel(\"Clase\", fontsize=11)\n",
    "        plt.ylabel(\"Anotaciones\", fontsize=11)\n",
    "        plt.xticks(rotation=25, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save to reports folder\n",
    "        out_path = self.output_dir / f\"{self.split}_final_distribution.png\"\n",
    "        try:\n",
    "            plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "            self.logger.info(f\"[PLOT] Final annotation distribution saved -> {out_path}\")\n",
    "            return str(out_path)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[PLOT] Failed to save final distribution plot: {e}\")\n",
    "            return None\n",
    "\n",
    "    def verify_final_split(self, show: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Perform integrity and quality verification on the final merged split.\n",
    "\n",
    "        Checks performed:\n",
    "        - JSON structure and file consistency\n",
    "        - Missing or duplicate image files\n",
    "        - Bounding box validity and area distribution\n",
    "        - Per-class annotation counts\n",
    "        - Mean boxes per image (only when >0)\n",
    "        - Optional plots of distributions and class balance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show : bool\n",
    "            If True, displays visual plots for class distribution and bbox stats.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Summary of verification metrics and detected issues.\n",
    "        \"\"\"\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.logger.info(f\"[VERIFY] Starting verification of final split='{self.split}'...\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Load and validate JSON\n",
    "        # ==========================================================\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[VERIFY] Missing final JSON: {self.final_json}\")\n",
    "\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = data.get(\"images\", [])\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = data.get(\"categories\", [])\n",
    "        if not images or not anns or not cats:\n",
    "            raise ValueError(\"[VERIFY] Final JSON is incomplete or empty.\")\n",
    "\n",
    "        cat_map = {c[\"id\"]: c[\"name\"] for c in cats}\n",
    "        img_map = {img[\"id\"]: img for img in images}\n",
    "        img_dir = self.final_dir\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ File existence and duplicates\n",
    "        # ==========================================================\n",
    "        missing_files, duplicates = [], []\n",
    "        seen_names = set()\n",
    "\n",
    "        for img in images:\n",
    "            fname = Path(img[\"file_name\"]).name\n",
    "            if fname in seen_names:\n",
    "                duplicates.append(fname)\n",
    "            seen_names.add(fname)\n",
    "\n",
    "            fpath = img_dir / fname\n",
    "            if not fpath.exists():\n",
    "                missing_files.append(fname)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Bounding box validation\n",
    "        # ==========================================================\n",
    "        valid_anns, invalid_anns = [], []\n",
    "        areas = []\n",
    "        for a in anns:\n",
    "            bbox = a.get(\"bbox\", [])\n",
    "            if len(bbox) != 4:\n",
    "                invalid_anns.append(a)\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            if w <= 0 or h <= 0:\n",
    "                invalid_anns.append(a)\n",
    "                continue\n",
    "            areas.append(w * h)\n",
    "            valid_anns.append(a)\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Class distribution and per-image stats\n",
    "        # ==========================================================\n",
    "        df = pd.DataFrame(valid_anns)\n",
    "        df[\"class\"] = df[\"category_id\"].map(cat_map)\n",
    "        class_counts = df[\"class\"].value_counts().sort_index()\n",
    "\n",
    "        # Count annotations per image\n",
    "        ann_per_img = df.groupby(\"image_id\").size()\n",
    "        mean_boxes = ann_per_img.mean()\n",
    "        std_boxes = ann_per_img.std()\n",
    "        max_boxes = ann_per_img.max()\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Summary of results\n",
    "        # ==========================================================\n",
    "        total_images = len(images)\n",
    "        total_annotations = len(anns)\n",
    "        total_valid = len(valid_anns)\n",
    "        total_invalid = len(invalid_anns)\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "\n",
    "        report = {\n",
    "            \"split\": self.split,\n",
    "            \"images_total\": total_images,\n",
    "            \"annotations_total\": total_annotations,\n",
    "            \"annotations_valid\": total_valid,\n",
    "            \"annotations_invalid\": total_invalid,\n",
    "            \"missing_files\": len(missing_files),\n",
    "            \"duplicate_files\": len(duplicates),\n",
    "            \"mean_boxes_per_image\": round(mean_boxes, 2),\n",
    "            \"std_boxes_per_image\": round(std_boxes, 2),\n",
    "            \"max_boxes_per_image\": int(max_boxes),\n",
    "            \"class_distribution\": class_counts.to_dict(),\n",
    "            \"duration_sec\": duration,\n",
    "        }\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Visualizations (optional)\n",
    "        # ==========================================================\n",
    "        if show:\n",
    "            sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "            # Class distribution\n",
    "            counts = class_counts\n",
    "            total = counts.sum()\n",
    "            labels = [f\"{cls}\\n({(n / total * 100):.1f}%)\" for cls, n in counts.items()]\n",
    "            sns.barplot(x=list(counts.index), y=list(counts.values), ax=axes[0], palette=\"viridis\")\n",
    "            axes[0].set_title(\"Class Distribution (Final Split)\")\n",
    "            axes[0].set_xlabel(\"Class\")\n",
    "            axes[0].set_ylabel(\"Count\")\n",
    "            for i, v in enumerate(counts.values):\n",
    "                axes[0].text(i, v, f\"{v} ({(v/total*100):.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "            # BBox area distribution\n",
    "            sns.histplot(areas, kde=True, ax=axes[1], color=\"steelblue\")\n",
    "            axes[1].set_title(\"Bounding Box Area Distribution\")\n",
    "            axes[1].set_xlabel(\"Area (pxÂ²)\")\n",
    "            axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Mean boxes per image histogram\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.histplot(ann_per_img, kde=False, color=\"darkorange\")\n",
    "            plt.title(f\"Mean boxes per image = {mean_boxes:.2f} (Ïƒ={std_boxes:.2f})\")\n",
    "            plt.xlabel(\"Annotations per image\")\n",
    "            plt.ylabel(\"Image count\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # ==========================================================\n",
    "        # ðŸ”¹ Logging summary\n",
    "        # ==========================================================\n",
    "        self.logger.info(f\"[VERIFY] Verified {total_images} images | {total_annotations} annotations\")\n",
    "        self.logger.info(f\"[VERIFY] Invalid boxes: {total_invalid}, Missing files: {len(missing_files)}, Duplicates: {len(duplicates)}\")\n",
    "        self.logger.info(f\"[VERIFY] Mean boxes/image = {mean_boxes:.2f} | Std = {std_boxes:.2f} | Max = {max_boxes:.0f}\")\n",
    "        self.logger.info(f\"[VERIFY] Duration = {duration:.2f}s\")\n",
    "\n",
    "        if missing_files:\n",
    "            self.logger.warning(f\"[VERIFY] Missing {len(missing_files)} files (see report).\")\n",
    "        if duplicates:\n",
    "            self.logger.warning(f\"[VERIFY] Found {len(duplicates)} duplicate file names.\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def run(self) -> dict:\n",
    "        \"\"\"\n",
    "        Execute the complete modular augmentation pipeline for the given split.\n",
    "        Each stage generates its own dataset folder and JSON file before\n",
    "        final consolidation and verification.\n",
    "\n",
    "        Workflow:\n",
    "            1. Data load & initial analysis\n",
    "            2. Transform setup\n",
    "            3. Proportional augmentation (prop/)\n",
    "            4. Rebalance stage (adaptive or single-pass)\n",
    "            5. Minority zoom reinforcement (focus_zoom/)\n",
    "            6. Merge all iterations into final dataset\n",
    "            7. Verify final split integrity and class balance\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.set_seed_everywhere()\n",
    "        self.logger.info(f\"[RUN] Starting augmentation pipeline for split='{self.split}'...\")\n",
    "\n",
    "        try:\n",
    "            phase_times = {}\n",
    "\n",
    "            # ==========================================================\n",
    "            # 1ï¸âƒ£ Load data & analyze distribution\n",
    "            # ==========================================================\n",
    "            t0 = time.time()\n",
    "            self.load_data()\n",
    "            self.analyze_class_distribution(plot=False)\n",
    "            phase_times[\"data_analysis_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "            # ==========================================================\n",
    "            # 2ï¸âƒ£ Prepare Albumentations transforms\n",
    "            # ==========================================================\n",
    "            t0 = time.time()\n",
    "            self.prepare_transforms()\n",
    "            phase_times[\"transform_setup_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "            # ==========================================================\n",
    "            # 3ï¸âƒ£ Global proportional augmentation\n",
    "            # ==========================================================\n",
    "            t0 = time.time()\n",
    "            if self.prop_json.exists():\n",
    "                self.logger.info(\"[RUN] Found existing proportional JSON. Skipping regeneration.\")\n",
    "                with open(self.prop_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                prop_report = {\n",
    "                    \"split\": self.split,\n",
    "                    \"augmented_images\": len(data.get(\"images\", [])),\n",
    "                    \"augmented_annotations\": len(data.get(\"annotations\", [])),\n",
    "                    \"output_dir\": str(self.prop_dir),\n",
    "                    \"json_path\": str(self.prop_json),\n",
    "                    \"duration_sec\": 0.0,\n",
    "                    \"skipped\": True,\n",
    "                }\n",
    "            else:\n",
    "                prop_report = self.apply_global_proportional_augmentation()\n",
    "            phase_times[\"proportional_aug_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "            # ==========================================================\n",
    "            # 4ï¸âƒ£ Rebalance (adaptive or single-pass)\n",
    "            # ==========================================================\n",
    "            reb_report = {\"rebalance_applied\": False}\n",
    "            if self.mode in [\"over\", \"under\"]:\n",
    "                t0 = time.time()\n",
    "                self.define_rebalance_strategy()\n",
    "                aug_cfg = self.config.get(\"augmentations\", {})\n",
    "                adaptive_enabled = bool(aug_cfg.get(\"adaptive_rebalance\", False))\n",
    "\n",
    "                if adaptive_enabled and self.mode == \"over\":\n",
    "                    self.logger.info(\"[RUN] Adaptive image-level rebalance enabled.\")\n",
    "                    reb_report = self.apply_adaptive_rebalance_images()\n",
    "                else:\n",
    "                    self.logger.info(\"[RUN] Single-pass rebalance mode activated.\")\n",
    "                    self.plan_rebalance_sampling()\n",
    "                    reb_report = self.apply_rebalance_augmentation()\n",
    "\n",
    "                    try:\n",
    "                        eval_report = self.evaluate_rebalance_result(show_plot=False)\n",
    "                        reb_report[\"evaluation\"] = eval_report\n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"[RUN] Evaluation skipped: {e}\")\n",
    "\n",
    "                phase_times[\"rebalance_aug_sec\"] = getattr(self, \"rebalance_duration\", round(time.time() - t0, 2))\n",
    "            else:\n",
    "                self.logger.info(f\"[RUN] Rebalance skipped (mode='{self.mode}').\")\n",
    "                phase_times[\"rebalance_aug_sec\"] = 0.0\n",
    "\n",
    "            # ==========================================================\n",
    "            # 5ï¸âƒ£ Minority focus zoom (optional)\n",
    "            # ==========================================================\n",
    "            t0 = time.time()\n",
    "            zoom_report = {\"focus_zoom_applied\": False, \"generated\": 0}\n",
    "\n",
    "            try:\n",
    "                zoom_report = self.apply_minority_focus_zoom()\n",
    "                gen = zoom_report.get(\"generated\", 0)\n",
    "                self.logger.info(f\"[RUN] Minority zoom completed â†’ {gen} new images.\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[RUN] Focus zoom skipped: {e}\")\n",
    "                zoom_report = {\"focus_zoom_applied\": False, \"generated\": 0, \"error\": str(e)}\n",
    "\n",
    "            phase_times[\"focus_zoom_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "            # ==========================================================\n",
    "            # 6ï¸âƒ£ Merge + final verification\n",
    "            # ==========================================================\n",
    "            t0 = time.time()\n",
    "            merge_report = self.merge_augmented_splits(reindex=True, clean_previous=True)\n",
    "\n",
    "            try:\n",
    "                verify_report = self.verify_final_split(show=False)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[RUN] Verification failed: {e}\")\n",
    "                verify_report = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "            try:\n",
    "                eval_report = self.evaluate_rebalance_result(show_plot=False)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[RUN] Evaluation failed: {e}\")\n",
    "                eval_report = {}\n",
    "\n",
    "            phase_times[\"merge_verify_sec\"] = round(time.time() - t0, 2)\n",
    "\n",
    "            # ==========================================================\n",
    "            # 7ï¸âƒ£ Summary & reporting\n",
    "            # ==========================================================\n",
    "            total_time = round(time.time() - start_time, 2)\n",
    "            self.logger.info(f\"[RUN] Pipeline completed successfully in {total_time:.2f}s for split='{self.split}'.\")\n",
    "\n",
    "            summary = {\n",
    "                \"split\": self.split,\n",
    "                \"status\": \"success\",\n",
    "                \"duration_total_sec\": total_time,\n",
    "                \"phase_durations\": phase_times,\n",
    "                \"proportional\": prop_report,\n",
    "                \"rebalance\": reb_report,\n",
    "                \"focus_zoom\": zoom_report,\n",
    "                \"merge\": merge_report,\n",
    "                \"verify\": verify_report,\n",
    "                \"evaluation\": eval_report\n",
    "            }\n",
    "\n",
    "            # Save structured report\n",
    "            try:\n",
    "                report_path = self.output_dir / f\"pipeline_summary_{self.split}.json\"\n",
    "                with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(summary, f, indent=2)\n",
    "                self.logger.info(f\"[RUN] Full pipeline summary saved to: {report_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"[RUN] Failed to save summary JSON: {e}\")\n",
    "\n",
    "            return summary\n",
    "\n",
    "        # ==========================================================\n",
    "        # 8ï¸âƒ£ Error handling\n",
    "        # ==========================================================\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"[RUN] Pipeline failed for split='{self.split}': {e}\", exc_info=True)\n",
    "            return {\n",
    "                \"split\": self.split,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"duration_total_sec\": round(time.time() - start_time, 2)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc054200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:53:00] INFO: Starting AugmentationPipeline...\n",
      "[17:53:00] INFO: Adaptive rebalance=ON\n",
      "[17:53:00] INFO: [INIT] Directory structure validated and configuration is consistent.\n",
      "[17:53:00] INFO: [INIT] AugmentationPipeline (v2.4.0) initialized for split='train_joined'.\n",
      "[17:53:00] INFO: Mode=over | Proportion=0.1 | Tolerance=0.2 | max_repeats=15 | min_unique_ratio=0.1 | max_boxes_per_image=100\n",
      "[17:53:00] INFO: [LOAD] Loaded JSON (4293.2 KB) from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined\\train_joined.json\n",
      "[17:53:00] WARNING: [LOAD] Duplicate annotation IDs detected.\n",
      "[17:53:00] INFO: [LOAD] Split 'train_joined' loaded successfully -> 2652 images, 14420 annotations, 6 classes.\n",
      "[17:53:00] INFO: [SETUP] Global seed fixed â†’ 42 | CUDA available: True\n",
      "[17:53:00] INFO: [RUN] Starting augmentation pipeline for split='train_joined'...\n",
      "[17:53:00] INFO: [LOAD] Loaded JSON (4293.2 KB) from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined\\train_joined.json\n",
      "[17:53:00] WARNING: [LOAD] Duplicate annotation IDs detected.\n",
      "[17:53:00] INFO: [LOAD] Split 'train_joined' loaded successfully -> 2652 images, 14420 annotations, 6 classes.\n",
      "[17:53:00] INFO: [ANALYZE] Annotation distribution for split='train_joined':\n",
      "[17:53:00] INFO:    - Elephant       :   4453 (30.88%)\n",
      "[17:53:00] INFO:    - Kob            :   3477 (24.11%)\n",
      "[17:53:00] INFO:    - Alcelaphinae   :   3379 (23.43%)\n",
      "[17:53:00] INFO:    - Buffalo        :   2137 (14.82%)\n",
      "[17:53:00] INFO:    - Warthog        :    639 ( 4.43%)\n",
      "[17:53:00] INFO:    - Waterbuck      :    335 ( 2.32%)\n",
      "[17:53:00] INFO:    Total annotations: 14420\n",
      "[17:53:00] INFO: [AUGMENT] Albumentations pipeline created successfully.\n",
      "[17:53:00] INFO:           Includes 12 transforms: Affine, HorizontalFlip, Perspective, RandomBrightnessContrast, HueSaturationValue, RandomGamma, RandomShadow, RandomSunFlare, RandomFog, GaussianBlur, RGBShift, CoarseDropout\n",
      "[17:53:00] INFO: [AUGMENT] Expected average of 0.74 transforms applied per image (stochastically).\n",
      "[17:53:00] INFO: [PROP] Selected 265/2652 images (10.0%) for augmentation.\n",
      "Proportional augment train_joined: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265/265 [00:59<00:00,  4.44it/s]\n",
      "[17:54:00] INFO: [PROP] Completed proportional augmentation for split='train_joined'.\n",
      "[17:54:00] INFO:        -> 261 new images | 1747 annotations | 59.77s\n",
      "[17:54:00] INFO:        -> JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_prop\\train_joined_prop.json\n",
      "[17:54:00] INFO: [REBALANCE] Combined counts from clean + proportional datasets.\n",
      "[17:54:00] INFO: [BALANCE] Using fixed bottom_n strategy (2).\n",
      "[17:54:00] INFO: [BALANCE] Split='train_joined' | tol=0.20 | ratio=14.16 | CV=0.635\n",
      "[17:54:00] INFO: [BALANCE] Max=4912 (Elephant) | Min=347 (Waterbuck) | Mean=2694.5\n",
      "[17:54:00] INFO:    Elephant       : count=4912   | r_max=1.000 | r_mean=1.823 | Î”=  82.3% ðŸ”º\n",
      "[17:54:00] INFO:    Alcelaphinae   : count=4014   | r_max=0.817 | r_mean=1.490 | Î”=  49.0% ðŸ”º\n",
      "[17:54:00] INFO:    Kob            : count=3857   | r_max=0.785 | r_mean=1.431 | Î”=  43.1% \n",
      "[17:54:00] INFO:    Buffalo        : count=2323   | r_max=0.473 | r_mean=0.862 | Î”= -13.8% \n",
      "[17:54:00] INFO:    Warthog        : count=714    | r_max=0.145 | r_mean=0.265 | Î”= -73.5% ðŸ”»\n",
      "[17:54:00] INFO:    Waterbuck      : count=347    | r_max=0.071 | r_mean=0.129 | Î”= -87.1% ðŸ”»\n",
      "[17:54:00] INFO: [BALANCE] Minority (2): ['Waterbuck', 'Warthog']\n",
      "[17:54:00] INFO: [BALANCE] Majority (2): ['Elephant', 'Alcelaphinae']\n",
      "[17:54:00] INFO: [REBALANCE] Mode='over' -> target â‰ˆ 3,929 per class (tol=0.20)\n",
      "[17:54:00] INFO:    - Buffalo         ->    augment to 3929   (+1606)\n",
      "[17:54:00] INFO:    - Elephant        ->       keep (no change)\n",
      "[17:54:00] INFO:    - Kob             ->    augment to 3929   (+72)\n",
      "[17:54:00] INFO:    - Alcelaphinae    ->       keep (no change)\n",
      "[17:54:00] INFO:    - Warthog         ->    augment to 3929   (+3215)\n",
      "[17:54:00] INFO:    - Waterbuck       ->    augment to 3929   (+3582)\n",
      "[17:54:00] INFO: [REBALANCE] Plan saved to: C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\rebalance_plan_train_joined.json\n",
      "[17:54:00] INFO: [REBALANCE] Summary -> augment_total=8,475 | downsample_total=0\n",
      "[17:54:00] INFO: [REBALANCE] Strategy ready - next: run plan_rebalance_sampling() to check feasibility.\n",
      "[17:54:00] INFO: [RUN] Adaptive image-level rebalance enabled.\n",
      "[17:54:00] INFO: [ADAPT] Starting adaptive rebalance for split='train_joined' (max_iter=3, prop=0.8, dominance_thr=0.8)\n",
      "[17:54:00] WARNING: [ADAPT] Final JSON not found. Attempting to build it via merge...\n",
      "[17:54:00] INFO: [MERGE] Cleaning previous contents from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final...\n",
      "[17:54:00] INFO: [MERGE] Found 2 dataset stages: ['clean', 'prop']\n",
      "[17:54:00] INFO: [MERGE] Adding 2652 images from stage 'clean'...\n",
      "[17:54:39] INFO: [MERGE] Adding 261 images from stage 'prop'...\n",
      "[17:54:40] INFO: [MERGE] Reindexing all IDs for consistency...\n",
      "[17:54:40] INFO: [MERGE] Final dataset assembled successfully for split='train_joined'.\n",
      "[17:54:40] INFO:          â†’ 2913 images | 16167 annotations | 40.23s\n",
      "[17:54:40] INFO: [ADAPT] === Iteration 1/3 ===\n",
      "[17:54:41] INFO: [SUMMARY] Class balance summary for train_joined_final.json:\n",
      "[17:54:41] INFO:        -> Max=4912 | Min=347 | Mean=2694.5 | Ratio=14.16\n",
      "[17:54:41] INFO:        -> Std=1711.8 | CV=0.635 | Within tol=False\n",
      "[17:54:41] INFO:    - Elephant          4912 Î”=  0.0% OK\n",
      "[17:54:41] INFO:    - Alcelaphinae      4014 Î”= 18.3% OK\n",
      "[17:54:41] INFO:    - Kob               3857 Î”= 21.5% OK\n",
      "[17:54:41] INFO:    - Buffalo           2323 Î”= 52.7% OK\n",
      "[17:54:41] INFO:    - Warthog            714 Î”= 85.5% WARNING\n",
      "[17:54:41] INFO:    - Waterbuck          347 Î”= 92.9% WARNING\n",
      "[17:54:41] INFO: [ADAPT] Selected 304 images (primary=343, mixed=52) for rebalance iteration 1.\n",
      "[17:54:41] INFO: [ADAPT] Applying corrective augmentation (iteration 1)...\n",
      "Rebalance iter 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [01:28<00:00,  3.42it/s]\n",
      "[17:56:09] INFO: [REBALANCE] Iteration 1 completed â†’ 299 images | 1242 annots | 88.82s\n",
      "[17:56:09] INFO: [REBALANCE] JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_rebalance_1\\train_joined_rebalance_1.json\n",
      "[17:56:09] INFO: [REBALANCE] Iter 1 â†’ 299 images | 1242 annotations\n",
      "[17:56:09] INFO: [ADAPT] Merging iteration output into final dataset...\n",
      "[17:56:09] INFO: [MERGE] Cleaning previous contents from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final...\n",
      "[17:56:10] INFO: [MERGE] Found 3 dataset stages: ['clean', 'prop', 'rebalance_1']\n",
      "[17:56:11] INFO: [MERGE] Adding 2652 images from stage 'clean'...\n",
      "[17:56:53] INFO: [MERGE] Adding 261 images from stage 'prop'...\n",
      "[17:56:56] INFO: [MERGE] Adding 299 images from stage 'rebalance_1'...\n",
      "[17:56:59] INFO: [MERGE] Reindexing all IDs for consistency...\n",
      "[17:56:59] INFO: [MERGE] Final dataset assembled successfully for split='train_joined'.\n",
      "[17:56:59] INFO:          â†’ 3212 images | 17409 annotations | 49.67s\n",
      "[17:56:59] INFO: [EVAL] Post-rebalance evaluation for split='train_joined':\n",
      "[17:56:59] INFO:        -> Max=4912 | Min=584 | Mean=2901.50 | Ratio=8.41\n",
      "[17:56:59] INFO:        -> Mean deviation=0.5114 | Tolerance=0.20\n",
      "[17:56:59] INFO:        -> Minority classes: ['Warthog', 'Waterbuck']\n",
      "[17:56:59] INFO:        -> Majority classes: ['Alcelaphinae', 'Elephant', 'Kob']\n",
      "[17:56:59] INFO:    - Alcelaphinae      4151 (23.84%)\n",
      "[17:56:59] INFO:    - Buffalo           2427 (13.94%)\n",
      "[17:56:59] INFO:    - Elephant          4912 (28.22%)\n",
      "[17:56:59] INFO:    - Kob               4093 (23.51%)\n",
      "[17:56:59] INFO:    - Warthog           1242 ( 7.13%)\n",
      "[17:56:59] INFO:    - Waterbuck          584 ( 3.35%)\n",
      "[17:56:59] WARNING: [EVAL] Balance not achieved (mean deviation=0.5114 > tol=0.20).\n",
      "[17:56:59] INFO: [EVAL] Evaluation summary saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\rebalance_eval_train_joined.json\n",
      "[17:56:59] INFO: [ADAPT] === Iteration 2/3 ===\n",
      "[17:56:59] INFO: [SUMMARY] Class balance summary for train_joined_final.json:\n",
      "[17:56:59] INFO:        -> Max=4912 | Min=584 | Mean=2901.5 | Ratio=8.41\n",
      "[17:56:59] INFO:        -> Std=1600.7 | CV=0.552 | Within tol=False\n",
      "[17:56:59] INFO:    - Elephant          4912 Î”=  0.0% OK\n",
      "[17:56:59] INFO:    - Alcelaphinae      4151 Î”= 15.5% OK\n",
      "[17:56:59] INFO:    - Kob               4093 Î”= 16.7% OK\n",
      "[17:56:59] INFO:    - Buffalo           2427 Î”= 50.6% OK\n",
      "[17:56:59] INFO:    - Warthog           1242 Î”= 74.7% WARNING\n",
      "[17:56:59] INFO:    - Waterbuck          584 Î”= 88.1% WARNING\n",
      "[17:56:59] INFO: [ADAPT] Selected 547 images (primary=591, mixed=103) for rebalance iteration 2.\n",
      "[17:56:59] INFO: [ADAPT] Applying corrective augmentation (iteration 2)...\n",
      "Rebalance iter 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 547/547 [02:06<00:00,  4.32it/s]\n",
      "[17:59:06] INFO: [REBALANCE] Iteration 2 completed â†’ 541 images | 2368 annots | 126.77s\n",
      "[17:59:06] INFO: [REBALANCE] JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_rebalance_2\\train_joined_rebalance_2.json\n",
      "[17:59:06] INFO: [REBALANCE] Iter 2 â†’ 541 images | 2368 annotations\n",
      "[17:59:06] INFO: [ADAPT] Merging iteration output into final dataset...\n",
      "[17:59:06] INFO: [MERGE] Cleaning previous contents from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final...\n",
      "[17:59:07] INFO: [MERGE] Found 4 dataset stages: ['clean', 'prop', 'rebalance_1', 'rebalance_2']\n",
      "[17:59:07] INFO: [MERGE] Adding 2652 images from stage 'clean'...\n",
      "[17:59:48] INFO: [MERGE] Adding 261 images from stage 'prop'...\n",
      "[17:59:51] INFO: [MERGE] Adding 299 images from stage 'rebalance_1'...\n",
      "[17:59:57] INFO: [MERGE] Adding 541 images from stage 'rebalance_2'...\n",
      "[18:00:02] INFO: [MERGE] Reindexing all IDs for consistency...\n",
      "[18:00:02] INFO: [MERGE] Final dataset assembled successfully for split='train_joined'.\n",
      "[18:00:02] INFO:          â†’ 3753 images | 19777 annotations | 55.96s\n",
      "[18:00:02] INFO: [EVAL] Post-rebalance evaluation for split='train_joined':\n",
      "[18:00:02] INFO:        -> Max=4912 | Min=1064 | Mean=3296.17 | Ratio=4.62\n",
      "[18:00:02] INFO:        -> Mean deviation=0.4053 | Tolerance=0.20\n",
      "[18:00:02] INFO:        -> Minority classes: ['Buffalo', 'Warthog', 'Waterbuck']\n",
      "[18:00:02] INFO:        -> Majority classes: ['Alcelaphinae', 'Elephant', 'Kob']\n",
      "[18:00:02] INFO:    - Alcelaphinae      4425 (22.37%)\n",
      "[18:00:02] INFO:    - Buffalo           2635 (13.32%)\n",
      "[18:00:02] INFO:    - Elephant          4912 (24.84%)\n",
      "[18:00:02] INFO:    - Kob               4559 (23.05%)\n",
      "[18:00:02] INFO:    - Warthog           2182 (11.03%)\n",
      "[18:00:02] INFO:    - Waterbuck         1064 ( 5.38%)\n",
      "[18:00:02] WARNING: [EVAL] Balance not achieved (mean deviation=0.4053 > tol=0.20).\n",
      "[18:00:02] INFO: [EVAL] Evaluation summary saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\rebalance_eval_train_joined.json\n",
      "[18:00:02] INFO: [ADAPT] === Iteration 3/3 ===\n",
      "[18:00:02] INFO: [SUMMARY] Class balance summary for train_joined_final.json:\n",
      "[18:00:02] INFO:        -> Max=4912 | Min=1064 | Mean=3296.2 | Ratio=4.62\n",
      "[18:00:02] INFO:        -> Std=1422.5 | CV=0.432 | Within tol=False\n",
      "[18:00:02] INFO:    - Elephant          4912 Î”=  0.0% OK\n",
      "[18:00:02] INFO:    - Kob               4559 Î”=  7.2% OK\n",
      "[18:00:02] INFO:    - Alcelaphinae      4425 Î”=  9.9% OK\n",
      "[18:00:02] INFO:    - Buffalo           2635 Î”= 46.4% WARNING\n",
      "[18:00:02] INFO:    - Warthog           2182 Î”= 55.6% WARNING\n",
      "[18:00:02] INFO:    - Waterbuck         1064 Î”= 78.3% WARNING\n",
      "[18:00:02] INFO: [ADAPT] Selected 1102 images (primary=1273, mixed=175) for rebalance iteration 3.\n",
      "[18:00:02] INFO: [ADAPT] Applying corrective augmentation (iteration 3)...\n",
      "Rebalance iter 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1102/1102 [03:51<00:00,  4.76it/s]\n",
      "[18:03:54] INFO: [REBALANCE] Iteration 3 completed â†’ 1098 images | 5763 annots | 231.86s\n",
      "[18:03:54] INFO: [REBALANCE] JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_rebalance_3\\train_joined_rebalance_3.json\n",
      "[18:03:54] INFO: [REBALANCE] Iter 3 â†’ 1098 images | 5763 annotations\n",
      "[18:03:54] INFO: [ADAPT] Merging iteration output into final dataset...\n",
      "[18:03:54] INFO: [MERGE] Cleaning previous contents from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final...\n",
      "[18:03:56] INFO: [MERGE] Found 5 dataset stages: ['clean', 'prop', 'rebalance_1', 'rebalance_2', 'rebalance_3']\n",
      "[18:03:56] INFO: [MERGE] Adding 2652 images from stage 'clean'...\n",
      "[18:04:37] INFO: [MERGE] Adding 261 images from stage 'prop'...\n",
      "[18:04:41] INFO: [MERGE] Adding 299 images from stage 'rebalance_1'...\n",
      "[18:04:47] INFO: [MERGE] Adding 541 images from stage 'rebalance_2'...\n",
      "[18:04:57] INFO: [MERGE] Adding 1098 images from stage 'rebalance_3'...\n",
      "[18:05:07] INFO: [MERGE] Reindexing all IDs for consistency...\n",
      "[18:05:07] INFO: [MERGE] Final dataset assembled successfully for split='train_joined'.\n",
      "[18:05:07] INFO:          â†’ 4851 images | 25540 annotations | 72.67s\n",
      "[18:05:07] INFO: [EVAL] Post-rebalance evaluation for split='train_joined':\n",
      "[18:05:07] INFO:        -> Max=5488 | Min=1899 | Mean=4256.67 | Ratio=2.89\n",
      "[18:05:07] INFO:        -> Mean deviation=0.2223 | Tolerance=0.20\n",
      "[18:05:07] INFO:        -> Minority classes: ['Waterbuck']\n",
      "[18:05:07] INFO:        -> Majority classes: ['Kob']\n",
      "[18:05:07] INFO:    - Alcelaphinae      4990 (19.54%)\n",
      "[18:05:07] INFO:    - Buffalo           4475 (17.52%)\n",
      "[18:05:07] INFO:    - Elephant          4912 (19.23%)\n",
      "[18:05:07] INFO:    - Kob               5488 (21.49%)\n",
      "[18:05:07] INFO:    - Warthog           3776 (14.78%)\n",
      "[18:05:07] INFO:    - Waterbuck         1899 ( 7.44%)\n",
      "[18:05:07] WARNING: [EVAL] Balance not achieved (mean deviation=0.2223 > tol=0.20).\n",
      "[18:05:07] INFO: [EVAL] Evaluation summary saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\rebalance_eval_train_joined.json\n",
      "[18:05:07] INFO: [FOCUS] Applying minority zoom reinforcement (prop=0.08, scale=1.5, offset=0.2)...\n",
      "[18:05:07] INFO: [SUMMARY] Class balance summary for train_joined_final.json:\n",
      "[18:05:07] INFO:        -> Max=5488 | Min=1899 | Mean=4256.7 | Ratio=2.89\n",
      "[18:05:07] INFO:        -> Std=1177.6 | CV=0.277 | Within tol=False\n",
      "[18:05:07] INFO:    - Kob               5488 Î”=  0.0% OK\n",
      "[18:05:07] INFO:    - Alcelaphinae      4990 Î”=  9.1% OK\n",
      "[18:05:07] INFO:    - Elephant          4912 Î”= 10.5% OK\n",
      "[18:05:07] INFO:    - Buffalo           4475 Î”= 18.5% OK\n",
      "[18:05:07] INFO:    - Warthog           3776 Î”= 31.2% OK\n",
      "[18:05:07] INFO:    - Waterbuck         1899 Î”= 65.4% WARNING\n",
      "[18:05:07] INFO: [ZOOM] Applying minority zoom reinforcement (prop=0.08, scale=1.5, offset=0.2)...\n",
      "[18:05:07] INFO: [ZOOM] Selected 50/636 minority images for zoom reinforcement.\n",
      "Zoom reinforce train_joined: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:07<00:00,  6.49it/s]\n",
      "[18:05:15] INFO: [ZOOM] Completed minority zoom reinforcement â†’ 50 images in 7.89s.\n",
      "[18:05:15] INFO: [ZOOM] JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_zoom\\train_joined_zoom.json\n",
      "[18:05:15] INFO: [ADAPT] Finished after 3 iteration(s) | Total new images=1988 | Status=MAX_ITER_REACHED | Duration=634.52s\n",
      "[18:05:15] INFO: [ZOOM] Applying minority zoom reinforcement (prop=0.08, scale=1.5, offset=0.2)...\n",
      "[18:05:15] INFO: [ZOOM] Selected 50/636 minority images for zoom reinforcement.\n",
      "Zoom reinforce train_joined: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00,  8.84it/s]\n",
      "[18:05:21] INFO: [ZOOM] Completed minority zoom reinforcement â†’ 50 images in 5.75s.\n",
      "[18:05:21] INFO: [ZOOM] JSON saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_zoom\\train_joined_zoom.json\n",
      "[18:05:21] INFO: [RUN] Minority zoom completed â†’ 50 new images.\n",
      "[18:05:21] INFO: [MERGE] Cleaning previous contents from C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final...\n",
      "[18:05:23] INFO: [MERGE] Found 6 dataset stages: ['clean', 'prop', 'rebalance_1', 'rebalance_2', 'rebalance_3', 'zoom']\n",
      "[18:05:23] INFO: [MERGE] Adding 2652 images from stage 'clean'...\n",
      "[18:06:00] INFO: [MERGE] Adding 261 images from stage 'prop'...\n",
      "[18:06:05] INFO: [MERGE] Adding 299 images from stage 'rebalance_1'...\n",
      "[18:06:09] INFO: [MERGE] Adding 541 images from stage 'rebalance_2'...\n",
      "[18:06:20] INFO: [MERGE] Adding 1098 images from stage 'rebalance_3'...\n",
      "[18:06:38] INFO: [MERGE] Adding 50 images from stage 'zoom'...\n",
      "[18:06:38] INFO: [MERGE] Reindexing all IDs for consistency...\n",
      "[18:06:39] INFO: [MERGE] Final dataset assembled successfully for split='train_joined'.\n",
      "[18:06:39] INFO:          â†’ 4901 images | 25706 annotations | 78.03s\n",
      "[18:06:39] INFO: [VERIFY] Starting verification of final split='train_joined'...\n",
      "[18:06:39] INFO: [VERIFY] Verified 4901 images | 25706 annotations\n",
      "[18:06:39] INFO: [VERIFY] Invalid boxes: 0, Missing files: 0, Duplicates: 0\n",
      "[18:06:39] INFO: [VERIFY] Mean boxes/image = 5.25 | Std = 8.50 | Max = 118\n",
      "[18:06:39] INFO: [VERIFY] Duration = 0.59s\n",
      "[18:06:39] INFO: [EVAL] Post-rebalance evaluation for split='train_joined':\n",
      "[18:06:39] INFO:        -> Max=5488 | Min=2059 | Mean=4284.33 | Ratio=2.67\n",
      "[18:06:40] INFO:        -> Mean deviation=0.2122 | Tolerance=0.20\n",
      "[18:06:40] INFO:        -> Minority classes: ['Waterbuck']\n",
      "[18:06:40] INFO:        -> Majority classes: ['Kob']\n",
      "[18:06:40] INFO:    - Alcelaphinae      4990 (19.41%)\n",
      "[18:06:40] INFO:    - Buffalo           4475 (17.41%)\n",
      "[18:06:40] INFO:    - Elephant          4912 (19.11%)\n",
      "[18:06:40] INFO:    - Kob               5488 (21.35%)\n",
      "[18:06:40] INFO:    - Warthog           3782 (14.71%)\n",
      "[18:06:40] INFO:    - Waterbuck         2059 ( 8.01%)\n",
      "[18:06:40] WARNING: [EVAL] Balance not achieved (mean deviation=0.2122 > tol=0.20).\n",
      "[18:06:40] INFO: [EVAL] Evaluation summary saved to C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\rebalance_eval_train_joined.json\n",
      "[18:06:40] INFO: [RUN] Pipeline completed successfully in 819.36s for split='train_joined'.\n",
      "[18:06:40] INFO: [RUN] Full pipeline summary saved to: C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports\\pipeline_summary_train_joined.json\n",
      "[18:06:40] INFO: [MAIN] Pipeline execution finished successfully for split='train_joined'.\n",
      "[18:06:40] INFO: [MAIN] Pipeline summary for split='train_joined':\n",
      "[18:06:40] INFO: {'split': 'train_joined', 'status': 'success', 'duration_total_sec': 819.36, 'phase_durations': {'data_analysis_sec': 0.05, 'transform_setup_sec': 0.01, 'proportional_aug_sec': 59.81, 'rebalance_aug_sec': 674.83, 'focus_zoom_sec': 5.76, 'merge_verify_sec': 78.76}, 'proportional': {'split': 'train_joined', 'augmented_images': 261, 'augmented_annotations': 1747, 'output_dir': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_prop', 'json_path': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_prop\\\\train_joined_prop.json', 'duration_sec': 59.77, 'timestamp': '2025-10-23 17:54:00'}, 'rebalance': {'split': 'train_joined', 'adaptive_applied': True, 'achieved_tolerance': False, 'iterations': 3, 'iterations_run': 3, 'status': 'max_iter_reached', 'total_new_images': 1988, 'duration_sec': 634.52}, 'focus_zoom': {'split': 'train_joined', 'focus_zoom_applied': True, 'generated': 50, 'output_dir': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_zoom', 'json_path': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_zoom\\\\train_joined_zoom.json', 'duration_sec': 5.75, 'status': 'success', 'total_annotations': 166}, 'merge': {'split': 'train_joined', 'images': 4901, 'annotations': 25706, 'stages_merged': ['clean', 'prop', 'rebalance_1', 'rebalance_2', 'rebalance_3', 'zoom'], 'output_dir': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_final', 'json_path': 'C:\\\\Users\\\\Jaime\\\\Documents\\\\Projects\\\\animal-count\\\\data\\\\outputs\\\\mirror_clean\\\\train_joined_final\\\\train_joined_final.json', 'duration_sec': 78.03}, 'verify': {'split': 'train_joined', 'images_total': 4901, 'annotations_total': 25706, 'annotations_valid': 25706, 'annotations_invalid': 0, 'missing_files': 0, 'duplicate_files': 0, 'mean_boxes_per_image': np.float64(5.25), 'std_boxes_per_image': np.float64(8.5), 'max_boxes_per_image': 118, 'class_distribution': {'Alcelaphinae': 4990, 'Buffalo': 4475, 'Elephant': 4912, 'Kob': 5488, 'Warthog': 3782, 'Waterbuck': 2059}, 'duration_sec': 0.59}, 'evaluation': {'split': 'train_joined', 'counts': {'Alcelaphinae': 4990, 'Buffalo': 4475, 'Elephant': 4912, 'Kob': 5488, 'Warthog': 3782, 'Waterbuck': 2059}, 'max_class': 5488, 'min_class': 2059, 'mean': 4284.33, 'ratio': 2.665, 'deviation_mean': 0.2122, 'within_tolerance': False, 'minority_classes': ['Waterbuck'], 'majority_classes': ['Kob']}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURATION & LOGGER SETUP\n",
    "# ==========================================================\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "CONFIG_PATH = Path(\"augmentation_config.yaml\")\n",
    "\n",
    "# Define which dataset split to process (e.g., \"train\", \"train_joined\", \"val\")\n",
    "SPLIT = \"train_joined\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load YAML configuration file safely\n",
    "# ----------------------------------------------------------\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Initialize logger for this pipeline instance\n",
    "# ----------------------------------------------------------\n",
    "logger = logging.getLogger(f\"AugmentationPipeline.{SPLIT}\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Add handler only if none exists (avoid duplicate logs)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s] %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# Prevent log propagation to the root logger\n",
    "logger.propagate = False\n",
    "\n",
    "# Log pipeline initialization\n",
    "logger.info(\"Starting AugmentationPipeline...\")\n",
    "\n",
    "# ==========================================================\n",
    "# PIPELINE INITIALIZATION\n",
    "# ==========================================================\n",
    "\n",
    "# Instantiate the augmentation pipeline with loaded configuration\n",
    "pipeline = AugmentationPipeline(config=config, split=SPLIT, logger=logger)\n",
    "\n",
    "# Load and analyze the input dataset\n",
    "pipeline.load_data()\n",
    "\n",
    "# ==========================================================\n",
    "# FULL PIPELINE EXECUTION\n",
    "# ==========================================================\n",
    "try:\n",
    "    # ------------------------------------------------------\n",
    "    # Run the complete augmentation pipeline\n",
    "    # Includes proportional, rebalance, and verification stages\n",
    "    # ------------------------------------------------------\n",
    "    results = pipeline.run()\n",
    "\n",
    "    # Log successful completion for this split\n",
    "    logger.info(\n",
    "        f\"[MAIN] Pipeline execution finished successfully for split='{SPLIT}'.\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"[MAIN] Pipeline execution failed: {e}\", exc_info=True)\n",
    "    results = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# ==========================================================\n",
    "# FINAL OUTPUT SUMMARY\n",
    "# ==========================================================\n",
    "\n",
    "# Log the summary of results for visibility and debugging\n",
    "logger.info(f\"[MAIN] Pipeline summary for split='{SPLIT}':\")\n",
    "logger.info(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf546939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class PostAugmentationQuality:\n",
    "    \"\"\"\n",
    "    Control de calidad posterior a la augmentaciÃ³n.\n",
    "    - Detecta imÃ¡genes invÃ¡lidas o corruptas.\n",
    "    - Verifica que las cajas no superen los lÃ­mites de la imagen.\n",
    "    - Corrige coordenadas fuera de rango.\n",
    "    - Limpia anotaciones invÃ¡lidas y huÃ©rfanas.\n",
    "    - Genera reportes grÃ¡ficos y CSV de resumen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, yaml_path: str, num_workers: int = 8, verbose: bool = True):\n",
    "        \"\"\"Inicializa el controlador de calidad desde el archivo YAML.\"\"\"\n",
    "        self.yaml_path = Path(yaml_path)\n",
    "        if not self.yaml_path.exists():\n",
    "            raise FileNotFoundError(f\"No se encontrÃ³ el archivo YAML: {self.yaml_path}\")\n",
    "\n",
    "        with open(self.yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "        # Directorios principales\n",
    "        self.output_dir = Path(self.config.get(\"output_dir\", \"data/outputs\")).resolve()\n",
    "        self.mirror_dir = self.output_dir / \"mirror_clean\"\n",
    "        self.final_dir = self.mirror_dir / \"train_joined_final\"\n",
    "        self.final_json = self.final_dir / \"train_joined_final.json\"\n",
    "\n",
    "        if not self.final_dir.exists():\n",
    "            raise FileNotFoundError(f\"[INIT] No se encontrÃ³ el directorio final: {self.final_dir}\")\n",
    "        if not self.final_json.exists():\n",
    "            raise FileNotFoundError(f\"[INIT] No se encontrÃ³ el JSON final: {self.final_json}\")\n",
    "\n",
    "        # Directorios de reportes\n",
    "        self.report_dir = self.output_dir / \"reports_quality\" / self.final_dir.name\n",
    "        self.report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.invalid_dir = self.final_dir / \"invalid\"\n",
    "        self.invalid_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # ConfiguraciÃ³n del logger\n",
    "        log_file = self.report_dir / \"post_quality.log\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file, mode=\"w\"),\n",
    "                logging.StreamHandler() if verbose else logging.NullHandler(),\n",
    "            ],\n",
    "        )\n",
    "        self.logger = logging.getLogger(\"PostAugmentationQuality\")\n",
    "        self.num_workers = num_workers\n",
    "        self.logger.info(f\"[INIT] Control de calidad inicializado correctamente.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # VALIDACIÃ“N DE IMÃGENES Y CORRECCIÃ“N/ELIMINACIÃ“N DE CAJAS\n",
    "    # ==========================================================\n",
    "    def validate_images(self) -> dict:\n",
    "        \"\"\"Valida imÃ¡genes, corrige cajas fuera de rango y elimina las completamente fuera.\"\"\"\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = data.get(\"images\", [])\n",
    "        anns = pd.DataFrame(data.get(\"annotations\", []))\n",
    "        invalid_images = []\n",
    "        bbox_corrected = 0\n",
    "        bbox_removed = 0\n",
    "        corrections_by_image = {}\n",
    "        image_sizes = {}\n",
    "\n",
    "        def check_image(img):\n",
    "            \"\"\"Valida la imagen y obtiene su tamaÃ±o.\"\"\"\n",
    "            fname = img[\"file_name\"]\n",
    "            path = self.final_dir / fname\n",
    "            if not path.exists():\n",
    "                return (\"missing\", fname, None)\n",
    "            img_cv = cv2.imread(str(path))\n",
    "            if img_cv is None or img_cv.size == 0:\n",
    "                return (\"corrupt\", fname, None)\n",
    "            h, w, _ = img_cv.shape\n",
    "            if h < 5 or w < 5:\n",
    "                return (\"tiny\", fname, None)\n",
    "            return (\"valid\", fname, (w, h))\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # Validar imÃ¡genes\n",
    "        # ----------------------------------------------------------\n",
    "        self.logger.info(f\"[VALIDATE] Validando {len(images)} imÃ¡genes...\")\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as ex:\n",
    "            futures = {ex.submit(check_image, img): img for img in images}\n",
    "            for f in tqdm(as_completed(futures), total=len(futures), desc=\"[VALIDATE]\"):\n",
    "                status, fname, size = f.result()\n",
    "                if status != \"valid\":\n",
    "                    invalid_images.append(fname)\n",
    "                else:\n",
    "                    image_sizes[futures[f][\"id\"]] = size\n",
    "\n",
    "        # Mover imÃ¡genes invÃ¡lidas\n",
    "        if invalid_images:\n",
    "            self.logger.warning(f\"[VALIDATE] {len(invalid_images)} imÃ¡genes invÃ¡lidas detectadas.\")\n",
    "            for fname in invalid_images:\n",
    "                src = self.final_dir / fname\n",
    "                if src.exists():\n",
    "                    dst = self.invalid_dir / fname\n",
    "                    shutil.move(src, dst)\n",
    "                    self.logger.info(f\"[MOVE] Imagen invÃ¡lida movida -> {dst}\")\n",
    "        else:\n",
    "            self.logger.info(\"[VALIDATE] No se detectaron imÃ¡genes invÃ¡lidas.\")\n",
    "\n",
    "        # Filtrar imÃ¡genes vÃ¡lidas\n",
    "        data[\"images\"] = [img for img in images if img[\"file_name\"] not in invalid_images]\n",
    "        valid_ids = {img[\"id\"] for img in data[\"images\"]}\n",
    "        anns = anns[anns[\"image_id\"].isin(valid_ids)]\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # Corregir o eliminar cajas fuera de los lÃ­mites\n",
    "        # ----------------------------------------------------------\n",
    "        if not anns.empty:\n",
    "            EPS = 1e-3\n",
    "            to_remove = []\n",
    "\n",
    "            for idx, row in anns.iterrows():\n",
    "                img_id = row[\"image_id\"]\n",
    "                if img_id not in image_sizes or not isinstance(row[\"bbox\"], list) or len(row[\"bbox\"]) != 4:\n",
    "                    continue\n",
    "\n",
    "                x, y, w, h = row[\"bbox\"]\n",
    "                img_w, img_h = image_sizes[img_id]\n",
    "                x2, y2 = x + w, y + h\n",
    "\n",
    "                # --- Caso 1: caja completamente fuera ---\n",
    "                if x2 <= 0 or y2 <= 0 or x >= img_w or y >= img_h:\n",
    "                    to_remove.append(idx)\n",
    "                    bbox_removed += 1\n",
    "                    continue\n",
    "\n",
    "                # --- Caso 2: parcialmente fuera (se recorta) ---\n",
    "                x_new = max(0, min(x, img_w - EPS))\n",
    "                y_new = max(0, min(y, img_h - EPS))\n",
    "                w_new = max(1, min(w, img_w - x_new - EPS))\n",
    "                h_new = max(1, min(h, img_h - y_new - EPS))\n",
    "\n",
    "                if (x, y, w, h) != (x_new, y_new, w_new, h_new):\n",
    "                    anns.at[idx, \"bbox\"] = [x_new, y_new, w_new, h_new]\n",
    "                    bbox_corrected += 1\n",
    "                    img_name = next((i[\"file_name\"] for i in data[\"images\"] if i[\"id\"] == img_id), None)\n",
    "                    corrections_by_image[img_name] = corrections_by_image.get(img_name, 0) + 1\n",
    "\n",
    "            # Eliminar las cajas completamente fuera\n",
    "            if to_remove:\n",
    "                anns = anns.drop(index=to_remove)\n",
    "                self.logger.warning(f\"[REMOVE] Se eliminaron {len(to_remove)} cajas completamente fuera de la imagen.\")\n",
    "\n",
    "            # Guardar resumen de correcciones\n",
    "            if corrections_by_image:\n",
    "                df_corr = pd.DataFrame(\n",
    "                    [{\"image\": k, \"bbox_corrected\": v} for k, v in corrections_by_image.items()]\n",
    "                ).sort_values(\"bbox_corrected\", ascending=False)\n",
    "                csv_path = self.report_dir / \"bbox_corrections_summary.csv\"\n",
    "                df_corr.to_csv(csv_path, index=False)\n",
    "                self.logger.info(f\"[REPORT] CSV de correcciones generado -> {csv_path}\")\n",
    "\n",
    "            self.logger.info(f\"[COHERENCE] {bbox_corrected} cajas corregidas, {bbox_removed} eliminadas completamente fuera.\")\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # Guardar y sobrescribir JSON\n",
    "        # ----------------------------------------------------------\n",
    "        data[\"annotations\"] = anns.to_dict(orient=\"records\")\n",
    "\n",
    "        validated_backup = self.report_dir / f\"{self.final_dir.name}_validated.json\"\n",
    "        with open(validated_backup, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        self.logger.info(f\"[SAVE] Copia validada guardada -> {validated_backup}\")\n",
    "\n",
    "        with open(self.final_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        self.logger.info(f\"[OVERWRITE] JSON original sobrescrito con correcciones -> {self.final_json}\")\n",
    "\n",
    "        return {\n",
    "            \"invalid\": len(invalid_images),\n",
    "            \"bbox_corrected\": bbox_corrected,\n",
    "            \"bbox_removed\": bbox_removed,\n",
    "            \"json_path\": str(self.final_json)\n",
    "        }\n",
    "\n",
    "    # ==========================================================\n",
    "    # LIMPIEZA DE ANOTACIONES INVÃLIDAS\n",
    "    # ==========================================================\n",
    "    def clean_annotations(self) -> dict:\n",
    "        \"\"\"Elimina anotaciones de Ã¡rea cero o huÃ©rfanas.\"\"\"\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        anns = pd.DataFrame(data.get(\"annotations\", []))\n",
    "        imgs = pd.DataFrame(data.get(\"images\", []))\n",
    "        if anns.empty:\n",
    "            return {\"removed\": 0}\n",
    "\n",
    "        anns[\"bbox_area\"] = anns[\"bbox\"].apply(lambda b: b[2] * b[3] if b and len(b) == 4 else 0)\n",
    "        zero_area = anns[anns[\"bbox_area\"] <= 0]\n",
    "        anns = anns[anns[\"bbox_area\"] > 0]\n",
    "\n",
    "        valid_ids = set(imgs[\"id\"])\n",
    "        orphan_anns = anns[~anns[\"image_id\"].isin(valid_ids)]\n",
    "        anns = anns[anns[\"image_id\"].isin(valid_ids)]\n",
    "\n",
    "        removed = len(zero_area) + len(orphan_anns)\n",
    "        self.logger.info(f\"[CLEAN] Se eliminaron {removed} anotaciones invÃ¡lidas.\")\n",
    "\n",
    "        data[\"annotations\"] = anns.drop(columns=[\"bbox_area\"]).to_dict(orient=\"records\")\n",
    "        cleaned_path = self.report_dir / f\"{self.final_dir.name}_cleaned.json\"\n",
    "        with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        self.final_json = cleaned_path\n",
    "        return {\"removed\": removed, \"json_path\": str(cleaned_path)}\n",
    "\n",
    "    # ==========================================================\n",
    "    # VERIFICACIÃ“N FINAL Y REPORTES\n",
    "    # ==========================================================\n",
    "    def verify_final_split(self, show: bool = True) -> dict:\n",
    "        \"\"\"Genera reportes visuales del dataset final.\"\"\"\n",
    "        with open(self.final_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        anns = pd.DataFrame(data.get(\"annotations\", []))\n",
    "        imgs = pd.DataFrame(data.get(\"images\", []))\n",
    "        cats = data.get(\"categories\", [])\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        if not anns.empty and cats:\n",
    "            id2cls = {c[\"id\"]: c[\"name\"] for c in cats}\n",
    "            anns[\"class\"] = anns[\"category_id\"].map(id2cls)\n",
    "            counts = anns[\"class\"].value_counts()\n",
    "\n",
    "            plt.figure(figsize=(9, 5))\n",
    "            sns.barplot(x=counts.index, y=counts.values, edgecolor=\"black\")\n",
    "            for i, (cls, cnt) in enumerate(counts.items()):\n",
    "                pct = 100 * cnt / counts.sum()\n",
    "                plt.text(i, cnt, f\"{cnt} ({pct:.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "            plt.title(\"DistribuciÃ³n por clase\")\n",
    "            plt.xticks(rotation=25, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            path = self.report_dir / f\"class_distribution_{timestamp}.png\"\n",
    "            plt.savefig(path, dpi=300)\n",
    "            if show:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        if not anns.empty:\n",
    "            counts = anns.groupby(\"image_id\").size()\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            sns.histplot(counts, bins=30, kde=True)\n",
    "            plt.title(\"Anotaciones por imagen\")\n",
    "            plt.tight_layout()\n",
    "            path2 = self.report_dir / f\"ann_per_image_{timestamp}.png\"\n",
    "            plt.savefig(path2, dpi=300)\n",
    "            if show:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        return {\"images_total\": len(imgs), \"annotations_total\": len(anns), \"categories\": len(cats)}\n",
    "\n",
    "    # ==========================================================\n",
    "    # EJECUCIÃ“N COMPLETA\n",
    "    # ==========================================================\n",
    "    def run(self, show: bool = True):\n",
    "        \"\"\"Ejecuta todo el pipeline de verificaciÃ³n y limpieza.\"\"\"\n",
    "        self.logger.info(\"[RUN] Iniciando control de calidad del dataset...\")\n",
    "        try:\n",
    "            invalid_report = self.validate_images()\n",
    "            clean_report = self.clean_annotations()\n",
    "            stats = self.verify_final_split(show=show)\n",
    "            self.logger.info(f\"[RUN] Finalizado. Invalid={invalid_report['invalid']}, \"\n",
    "                             f\"Corrigidas={invalid_report['bbox_corrected']}, \"\n",
    "                             f\"Eliminadas={clean_report['removed']}\")\n",
    "            return {\"status\": \"success\", \"invalid\": invalid_report,\n",
    "                    \"annotations\": clean_report, \"summary\": stats}\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"[RUN] FallÃ³: {e}\", exc_info=True)\n",
    "            return {\"status\": \"failed\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cddd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:29:48] INFO: Iniciando verificaciÃ³n de coherencia posterior a la augmentaciÃ³n...\n",
      "[2025-10-23 18:29:48,501] INFO: [INIT] Control de calidad inicializado correctamente.\n",
      "[2025-10-23 18:29:48,502] INFO: [RUN] Iniciando control de calidad del dataset...\n",
      "[2025-10-23 18:29:48,656] INFO: [VALIDATE] Validando 4901 imÃ¡genes...\n",
      "[VALIDATE]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4901/4901 [00:57<00:00, 85.83it/s] \n",
      "[2025-10-23 18:30:46,033] INFO: [VALIDATE] No se detectaron imÃ¡genes invÃ¡lidas.\n",
      "[2025-10-23 18:30:46,856] WARNING: [REMOVE] Se eliminaron 57 cajas completamente fuera de la imagen.\n",
      "[2025-10-23 18:30:46,860] INFO: [REPORT] CSV de correcciones generado -> C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports_quality\\train_joined_final\\bbox_corrections_summary.csv\n",
      "[2025-10-23 18:30:46,860] INFO: [COHERENCE] 312 cajas corregidas, 57 eliminadas completamente fuera.\n",
      "[2025-10-23 18:30:47,366] INFO: [SAVE] Copia validada guardada -> C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports_quality\\train_joined_final\\train_joined_final_validated.json\n",
      "[2025-10-23 18:30:47,791] INFO: [OVERWRITE] JSON original sobrescrito con correcciones -> C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\mirror_clean\\train_joined_final\\train_joined_final.json\n",
      "[2025-10-23 18:30:47,967] INFO: [CLEAN] Se eliminaron 0 anotaciones invÃ¡lidas.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-23 18:30:49,137] INFO: [RUN] Finalizado. Invalid=0, Corrigidas=312, Eliminadas=0\n",
      "[18:30:49] INFO: [MAIN] VerificaciÃ³n de calidad completada correctamente.\n",
      "[18:30:49] INFO: [MAIN] Reportes guardados en: C:\\Users\\Jaime\\Documents\\Projects\\animal-count\\data\\outputs\\reports_quality\\train_joined_final\n",
      "[18:30:49] INFO: [MAIN] Resumen general del control de calidad:\n",
      "[18:30:49] INFO:   - Estado: success\n",
      "[18:30:49] INFO:   - ImÃ¡genes invÃ¡lidas: 0\n",
      "[18:30:49] INFO:   - Cajas corregidas: 312\n",
      "[18:30:49] INFO:   - Anotaciones eliminadas: 0\n",
      "[18:30:49] INFO:   - Total imÃ¡genes: 4901\n",
      "[18:30:49] INFO:   - Total anotaciones: 25649\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURACIÃ“N Y LOGGER\n",
    "# ==========================================================\n",
    "CONFIG_PATH = Path(\"augmentation_config.yaml\")\n",
    "\n",
    "# --- Cargar configuraciÃ³n YAML ---\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Inicializar logger principal ---\n",
    "logger = logging.getLogger(\"PostAugmentationQualityRunner\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s] %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "logger.propagate = False\n",
    "logger.info(\"Iniciando verificaciÃ³n de coherencia posterior a la augmentaciÃ³n...\")\n",
    "\n",
    "# ==========================================================\n",
    "# EJECUCIÃ“N DEL CONTROL DE CALIDAD\n",
    "# ==========================================================\n",
    "try:\n",
    "    # Instanciar el verificador de calidad\n",
    "    checker = PostAugmentationQuality(\n",
    "        yaml_path=str(CONFIG_PATH),\n",
    "        num_workers=8,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Ejecutar la auditorÃ­a completa (con grÃ¡ficos)\n",
    "    results = checker.run(show=True)\n",
    "\n",
    "    logger.info(\"[MAIN] VerificaciÃ³n de calidad completada correctamente.\")\n",
    "    logger.info(f\"[MAIN] Reportes guardados en: {checker.report_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"[MAIN] FallÃ³ la verificaciÃ³n de calidad: {e}\", exc_info=True)\n",
    "    results = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# ==========================================================\n",
    "# RESUMEN FINAL\n",
    "# ==========================================================\n",
    "logger.info(\"[MAIN] Resumen general del control de calidad:\")\n",
    "logger.info(f\"  - Estado: {results.get('status', 'unknown')}\")\n",
    "if results.get(\"status\") == \"success\":\n",
    "    logger.info(f\"  - ImÃ¡genes invÃ¡lidas: {results['invalid']['invalid']}\")\n",
    "    logger.info(f\"  - Cajas corregidas: {results['invalid']['bbox_corrected']}\")\n",
    "    logger.info(f\"  - Anotaciones eliminadas: {results['annotations']['removed']}\")\n",
    "    logger.info(f\"  - Total imÃ¡genes: {results['summary']['images_total']}\")\n",
    "    logger.info(f\"  - Total anotaciones: {results['summary']['annotations_total']}\")\n",
    "else:\n",
    "    logger.warning(f\"  - Error: {results.get('error', 'Desconocido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5f392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
