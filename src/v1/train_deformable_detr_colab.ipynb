{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Entrenamiento Deformable DETR en Google Colab\n",
    "\n",
    "Este notebook entrena un modelo Deformable DETR con MMDetection en Google Colab.\n",
    "\n",
    "## üìã Caracter√≠sticas\n",
    "- **Modelo**: Deformable DETR con backbone Swin-T o ResNet-50\n",
    "- **Framework**: MMDetection (OpenMMLab)\n",
    "- **Detecci√≥n autom√°tica de GPU**\n",
    "- **Configuraci√≥n autom√°tica de datasets**\n",
    "- **Visualizaci√≥n de resultados**\n",
    "- **Exportaci√≥n de modelos**\n",
    "\n",
    "## üéØ Clases de Animales\n",
    "- Buffalo\n",
    "- Elephant  \n",
    "- Kob\n",
    "- Alcelaphinae\n",
    "- Warthog\n",
    "- Waterbuck\n",
    "\n",
    "## ‚öôÔ∏è Configuraci√≥n por Defecto\n",
    "- **Backbone**: Swin-T (recomendado) o ResNet-50\n",
    "- **Tama√±o de imagen**: 896x896\n",
    "- **√âpocas**: 50\n",
    "- **Batch size**: 4 (train), 2 (val/test)\n",
    "- **Optimizador**: AdamW con weight decay\n",
    "\n",
    "## üî¨ Ventajas de Deformable DETR\n",
    "- **End-to-end**: Sin necesidad de NMS post-procesamiento\n",
    "- **Eficiente**: Menos par√°metros que R-CNN basados en regiones\n",
    "- **Preciso**: Mejor rendimiento en objetos peque√±os\n",
    "- **Flexible**: Arquitectura transformer moderna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Instalaci√≥n de Dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias principales\n",
    "%pip install -q openmim\n",
    "!mim install -q mmcv-full\n",
    "!mim install -q mmdet\n",
    "%pip install -q pyyaml opencv-python pillow tqdm matplotlib seaborn pandas\n",
    "\n",
    "# Verificar instalaci√≥n\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Importar Librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MMDetection imports\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "import mmdet\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "\n",
    "# Google Colab\n",
    "from google.colab import files, drive\n",
    "\n",
    "# Configurar matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"MMDetection version: {mmdet.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Configuraci√≥n de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas y par√°metros\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Par√°metros del modelo\n",
    "        self.backbone = \"swin_t\"  # \"swin_t\" o \"resnet50\"\n",
    "        self.imgsz = 896\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 4\n",
    "        self.val_batch_size = 2\n",
    "        \n",
    "        # Rutas (ajustar seg√∫n tu estructura de datos)\n",
    "        self.data_root = \"/content/aerial-wildlife-count\"\n",
    "        self.work_dir = \"/content/work_dirs/deformable_detr\"\n",
    "        \n",
    "        # Clases del dataset\n",
    "        self.classes = [\n",
    "            \"Buffalo\", \"Elephant\", \"Kob\", \n",
    "            \"Alcelaphinae\", \"Warthog\", \"Waterbuck\"\n",
    "        ]\n",
    "        \n",
    "        # Configuraci√≥n de GPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def print_config(self):\n",
    "        print(\"üîß Configuraci√≥n:\")\n",
    "        print(f\"  Backbone: {self.backbone}\")\n",
    "        print(f\"  Tama√±o de imagen: {self.imgsz}\")\n",
    "        print(f\"  √âpocas: {self.epochs}\")\n",
    "        print(f\"  Batch size: {self.batch_size}\")\n",
    "        print(f\"  Dispositivo: {self.device}\")\n",
    "        print(f\"  Directorio de trabajo: {self.work_dir}\")\n",
    "\n",
    "# Crear instancia de configuraci√≥n\n",
    "cfg = Config()\n",
    "cfg.print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE RUTAS PRINCIPALES (Adaptado para pipelines)\n",
    "# ============================================================\n",
    "\n",
    "# Definir la ruta base principal (ajustar seg√∫n el entorno: Drive o local)\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/aerial-wildlife-count\")\n",
    "\n",
    "# ============================================================\n",
    "# RUTAS DE DATOS PROCESADOS (Outputs de pipelines)\n",
    "# ============================================================\n",
    "\n",
    "# Rutas de los archivos de anotaciones finales (despu√©s de quality + augmentation)\n",
    "TRAIN_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"train_final\" / \"train_final.json\"\n",
    "VAL_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"val_final\" / \"val_final.json\"\n",
    "TEST_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"test_final\" / \"test_final.json\"\n",
    "\n",
    "# Rutas alternativas si no existe la estructura final\n",
    "TRAIN_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"train_validated.json\"\n",
    "VAL_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"val_validated.json\"\n",
    "TEST_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"test_validated.json\"\n",
    "\n",
    "# Rutas de las carpetas de im√°genes correspondientes\n",
    "TRAIN_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"train_final\" / \"images\"\n",
    "VAL_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"val_final\" / \"images\"\n",
    "TEST_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"test_final\" / \"images\"\n",
    "\n",
    "# Rutas alternativas para im√°genes\n",
    "TRAIN_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"train\" / \"images\"\n",
    "VAL_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"val\" / \"images\"\n",
    "TEST_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"test\" / \"images\"\n",
    "\n",
    "# ============================================================\n",
    "# FUNCI√ìN PARA CONFIGURAR DATOS (Adaptada para pipelines)\n",
    "# ============================================================\n",
    "\n",
    "def setup_data():\n",
    "    \"\"\"Configurar datos desde Google Drive con detecci√≥n autom√°tica de pipelines\"\"\"\n",
    "    \n",
    "    # Montar Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive montado\")\n",
    "    \n",
    "    # Verificar si existe la estructura de pipelines\n",
    "    if TRAIN_ANN_FILE.exists() and TRAIN_IMG_DIR.exists():\n",
    "        print(\"‚úÖ Datos de pipelines encontrados (estructura final)\")\n",
    "        cfg.data_root = str(BASE_DIR)\n",
    "        return True, \"final\"\n",
    "    elif TRAIN_ANN_FILE_ALT.exists() and TRAIN_IMG_DIR_ALT.exists():\n",
    "        print(\"‚úÖ Datos de pipelines encontrados (estructura verificada)\")\n",
    "        cfg.data_root = str(BASE_DIR)\n",
    "        return True, \"verified\"\n",
    "    else:\n",
    "        # Buscar en ubicaciones alternativas\n",
    "        drive_path = \"/content/drive/MyDrive\"\n",
    "        possible_paths = [\n",
    "            f\"{drive_path}/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/datasets/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/Colab Notebooks/aerial-wildlife-count\"\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                cfg.data_root = path\n",
    "                print(f\"‚úÖ Dataset encontrado en ubicaci√≥n alternativa: {path}\")\n",
    "                return True, \"legacy\"\n",
    "        \n",
    "        print(\"‚ùå No se encontr√≥ el dataset en Google Drive\")\n",
    "        return False, None\n",
    "\n",
    "# Configurar datos\n",
    "success, data_type = setup_data()\n",
    "if success:\n",
    "    print(f\"üìÅ Ruta de datos configurada: {cfg.data_root}\")\n",
    "    print(f\"üìä Tipo de datos: {data_type}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Configura los datos manualmente antes de continuar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para encontrar y validar datasets (Adaptada para pipelines)\n",
    "def find_datasets():\n",
    "    \"\"\"Encontrar datasets disponibles seg√∫n el tipo de datos\"\"\"\n",
    "    \n",
    "    datasets_found = []\n",
    "    \n",
    "    if data_type == \"final\":\n",
    "        # Estructura final de pipelines (augmentation + quality)\n",
    "        datasets_found = [TRAIN_ANN_FILE, VAL_ANN_FILE, TEST_ANN_FILE]\n",
    "        print(\"üìä Usando datasets finales de pipelines:\")\n",
    "        print(f\"  Train: {TRAIN_ANN_FILE}\")\n",
    "        print(f\"  Val: {VAL_ANN_FILE}\")\n",
    "        print(f\"  Test: {TEST_ANN_FILE}\")\n",
    "        \n",
    "    elif data_type == \"verified\":\n",
    "        # Estructura verificada de quality pipeline\n",
    "        datasets_found = [TRAIN_ANN_FILE_ALT, VAL_ANN_FILE_ALT, TEST_ANN_FILE_ALT]\n",
    "        print(\"üìä Usando datasets verificados de quality pipeline:\")\n",
    "        print(f\"  Train: {TRAIN_ANN_FILE_ALT}\")\n",
    "        print(f\"  Val: {VAL_ANN_FILE_ALT}\")\n",
    "        print(f\"  Test: {TEST_ANN_FILE_ALT}\")\n",
    "        \n",
    "    else:\n",
    "        # Estructura legacy - buscar archivos JSON\n",
    "        data_root = Path(cfg.data_root)\n",
    "        json_patterns = [\n",
    "            \"**/train_*.json\",\n",
    "            \"**/val_*.json\", \n",
    "            \"**/test_*.json\",\n",
    "            \"**/*_big_size_*.json\",\n",
    "            \"**/*_subframes_*.json\",\n",
    "            \"**/*_final.json\",\n",
    "            \"**/*_validated.json\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in json_patterns:\n",
    "            for json_file in data_root.glob(pattern):\n",
    "                if json_file.is_file():\n",
    "                    datasets_found.append(json_file)\n",
    "        \n",
    "        print(f\"üìä Datasets encontrados en estructura legacy ({len(datasets_found)}):\")\n",
    "        for i, dataset in enumerate(datasets_found):\n",
    "            print(f\"  {i+1}. {dataset}\")\n",
    "    \n",
    "    return datasets_found\n",
    "\n",
    "# Funci√≥n para analizar un dataset COCO\n",
    "def analyze_dataset(json_path):\n",
    "    \"\"\"Analizar estad√≠sticas de un dataset COCO\"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìà An√°lisis de {json_path.name}:\")\n",
    "    print(f\"  Im√°genes: {len(data['images'])}\")\n",
    "    print(f\"  Anotaciones: {len(data['annotations'])}\")\n",
    "    print(f\"  Categor√≠as: {len(data['categories'])}\")\n",
    "    \n",
    "    # Estad√≠sticas por categor√≠a\n",
    "    cat_counts = {}\n",
    "    for ann in data['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        cat_counts[cat_id] = cat_counts.get(cat_id, 0) + 1\n",
    "    \n",
    "    print(\"\\n  Distribuci√≥n por categor√≠a:\")\n",
    "    for cat in data['categories']:\n",
    "        count = cat_counts.get(cat['id'], 0)\n",
    "        print(f\"    {cat['name']}: {count}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Buscar y analizar datasets\n",
    "datasets = find_datasets()\n",
    "\n",
    "if datasets:\n",
    "    # Analizar el primer dataset encontrado como ejemplo\n",
    "    sample_data = analyze_dataset(datasets[0])\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron datasets. Verifica la estructura de directorios.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar datos desde Google Drive o subir archivos\n",
    "def setup_data():\n",
    "    \"\"\"Configurar datos desde Google Drive o archivos subidos\"\"\"\n",
    "    \n",
    "    # Opci√≥n 1: Montar Google Drive\n",
    "    print(\"üìÅ Opciones para cargar datos:\")\n",
    "    print(\"1. Montar Google Drive\")\n",
    "    print(\"2. Subir archivos manualmente\")\n",
    "    \n",
    "    choice = input(\"Selecciona opci√≥n (1 o 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        # Montar Google Drive\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Google Drive montado\")\n",
    "        \n",
    "        # Buscar el dataset en Drive\n",
    "        drive_path = \"/content/drive/MyDrive\"\n",
    "        possible_paths = [\n",
    "            f\"{drive_path}/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/datasets/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/Colab Notebooks/aerial-wildlife-count\"\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                cfg.data_root = path\n",
    "                print(f\"‚úÖ Dataset encontrado en: {path}\")\n",
    "                return True\n",
    "        \n",
    "        print(\"‚ùå No se encontr√≥ el dataset en Google Drive\")\n",
    "        return False\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        print(\"üì§ Sube los archivos del dataset...\")\n",
    "        uploaded = files.upload()\n",
    "        print(\"‚úÖ Archivos subidos\")\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Opci√≥n inv√°lida\")\n",
    "        return False\n",
    "\n",
    "# Configurar datos\n",
    "if not setup_data():\n",
    "    print(\"‚ö†Ô∏è  Configura los datos manualmente antes de continuar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para construir configuraci√≥n de Deformable DETR\n",
    "def build_deformable_detr_config(backbone, imgsz, epochs, train_json, val_json, test_json, workdir):\n",
    "    \"\"\"Construir configuraci√≥n de Deformable DETR para MMDetection\"\"\"\n",
    "    \n",
    "    # Configuraci√≥n base seg√∫n backbone\n",
    "    if backbone.lower().startswith(\"swin\"):\n",
    "        base_model = \"mmdet::_base_/models/deformable_detr_swin-t.py\"\n",
    "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/deformable_detr/deformable_detr_swin_t_16x2_50e_coco/deformable_detr_swin_t_16x2_50e_coco_20210419_220030-a12b9512.pth\"\n",
    "    else:\n",
    "        base_model = \"mmdet::_base_/models/deformable_detr_r50.py\"\n",
    "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/deformable_detr/deformable_detr_r50_16x2_50e_coco/deformable_detr_r50_16x2_50e_coco_20210419_220030-a12b9512.pth\"\n",
    "    \n",
    "    # Determinar directorios de im√°genes seg√∫n el tipo de datos\n",
    "    if data_type == \"final\":\n",
    "        train_img_dir = TRAIN_IMG_DIR\n",
    "        val_img_dir = VAL_IMG_DIR\n",
    "        test_img_dir = TEST_IMG_DIR\n",
    "    elif data_type == \"verified\":\n",
    "        train_img_dir = TRAIN_IMG_DIR_ALT\n",
    "        val_img_dir = VAL_IMG_DIR_ALT\n",
    "        test_img_dir = TEST_IMG_DIR_ALT\n",
    "    else:\n",
    "        # Estructura legacy\n",
    "        train_img_dir = train_json.parent / \"images\" if (train_json.parent / \"images\").exists() else train_json.parent.parent / \"train\"\n",
    "        val_img_dir = val_json.parent.parent / \"val\"\n",
    "        test_img_dir = test_json.parent.parent / \"test\"\n",
    "    \n",
    "    config = {\n",
    "        \"_base_\": [\n",
    "            base_model,\n",
    "            \"mmdet::_base_/datasets/coco_instance.py\",\n",
    "            \"mmdet::_base_/schedules/schedule_1x.py\",\n",
    "            \"mmdet::_base_/default_runtime.py\",\n",
    "        ],\n",
    "        \"default_scope\": \"mmdet\",\n",
    "        \n",
    "        # Dataset configuration\n",
    "        \"dataset_type\": \"CocoDataset\",\n",
    "        \"data_root\": str(train_json.parent.parent),\n",
    "        \n",
    "        # Train dataloader\n",
    "        \"train_dataloader\": {\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "            \"num_workers\": 4,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": True},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(train_json),\n",
    "                \"data_prefix\": {\"img\": str(train_img_dir)},\n",
    "                \"filter_cfg\": {\"filter_empty\": True, \"min_size\": 1},\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Validation dataloader\n",
    "        \"val_dataloader\": {\n",
    "            \"batch_size\": cfg.val_batch_size,\n",
    "            \"num_workers\": 2,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(val_json),\n",
    "                \"data_prefix\": {\"img\": str(val_img_dir)},\n",
    "                \"test_mode\": True\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Test dataloader\n",
    "        \"test_dataloader\": {\n",
    "            \"batch_size\": cfg.val_batch_size,\n",
    "            \"num_workers\": 2,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(test_json),\n",
    "                \"data_prefix\": {\"img\": str(test_img_dir)},\n",
    "                \"test_mode\": True\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Data augmentation pipelines (espec√≠fico para DETR)\n",
    "        \"train_pipeline\": [\n",
    "            {\"type\": \"LoadImageFromFile\"},\n",
    "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
    "            {\"type\": \"RandomResize\", \"scale\": [(imgsz, imgsz)], \"keep_ratio\": True},\n",
    "            {\"type\": \"RandomFlip\", \"prob\": 0.5},\n",
    "            {\"type\": \"PackDetInputs\"},\n",
    "        ],\n",
    "        \"test_pipeline\": [\n",
    "            {\"type\": \"LoadImageFromFile\"},\n",
    "            {\"type\": \"Resize\", \"scale\": (imgsz, imgsz), \"keep_ratio\": True},\n",
    "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
    "            {\"type\": \"PackDetInputs\"},\n",
    "        ],\n",
    "        \n",
    "        # Optimizer configuration (AdamW para DETR)\n",
    "        \"optim_wrapper\": {\n",
    "            \"type\": \"OptimWrapper\",\n",
    "            \"optimizer\": {\n",
    "                \"type\": \"AdamW\", \n",
    "                \"lr\": 2e-4, \n",
    "                \"betas\": (0.9, 0.999), \n",
    "                \"weight_decay\": 0.05\n",
    "            },\n",
    "            \"clip_grad\": {\"max_norm\": 1.0, \"norm_type\": 2},\n",
    "        },\n",
    "        \n",
    "        # Learning rate scheduler (CosineAnnealing para DETR)\n",
    "        \"param_scheduler\": [\n",
    "            {\n",
    "                \"type\": \"LinearLR\", \n",
    "                \"start_factor\": 0.001, \n",
    "                \"by_epoch\": False, \n",
    "                \"begin\": 0, \n",
    "                \"end\": 500\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"CosineAnnealingLR\", \n",
    "                \"T_max\": epochs, \n",
    "                \"by_epoch\": True\n",
    "            },\n",
    "        ],\n",
    "        \n",
    "        # Training configuration\n",
    "        \"train_cfg\": {\"max_epochs\": epochs, \"val_interval\": 1},\n",
    "        \"val_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(val_json), \"metric\": \"bbox\"},\n",
    "        \"test_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(test_json), \"metric\": \"bbox\"},\n",
    "        \n",
    "        # Work directory and model loading\n",
    "        \"work_dir\": str(workdir),\n",
    "        \"load_from\": pretrained,\n",
    "        \"resume\": False,\n",
    "        \n",
    "        # Visualization and logging\n",
    "        \"visualizer\": {\"type\": \"DetLocalVisualizer\"},\n",
    "        \"default_hooks\": {\n",
    "            \"checkpoint\": {\n",
    "                \"type\": \"CheckpointHook\", \n",
    "                \"interval\": 1, \n",
    "                \"max_keep_ckpts\": 3, \n",
    "                \"save_best\": \"coco/bbox_mAP\"\n",
    "            },\n",
    "            \"logger\": {\"type\": \"LoggerHook\", \"interval\": 50},\n",
    "            \"param_scheduler\": {\"type\": \"ParamSchedulerHook\"},\n",
    "            \"timer\": {\"type\": \"IterTimerHook\"},\n",
    "            \"sampler_seed\": {\"type\": \"DistSamplerSeedHook\"},\n",
    "        },\n",
    "        \n",
    "        # Environment configuration\n",
    "        \"env_cfg\": {\"cudnn_benchmark\": True},\n",
    "        \"randomness\": {\"seed\": 42, \"deterministic\": False}\n",
    "    }\n",
    "    \n",
    "    return Config(config)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de configuraci√≥n de Deformable DETR creada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para entrenamiento (Adaptada para pipelines)\n",
    "def prepare_training_data():\n",
    "    \"\"\"Preparar y validar datos para entrenamiento seg√∫n el tipo de datos\"\"\"\n",
    "    \n",
    "    if data_type == \"final\":\n",
    "        # Usar rutas finales de pipelines\n",
    "        train_json = TRAIN_ANN_FILE\n",
    "        val_json = VAL_ANN_FILE\n",
    "        test_json = TEST_ANN_FILE\n",
    "        \n",
    "        # Verificar que existan\n",
    "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
    "            print(\"‚ùå Faltan archivos de datos finales de pipelines\")\n",
    "            return None, None, None\n",
    "            \n",
    "    elif data_type == \"verified\":\n",
    "        # Usar rutas verificadas de quality pipeline\n",
    "        train_json = TRAIN_ANN_FILE_ALT\n",
    "        val_json = VAL_ANN_FILE_ALT\n",
    "        test_json = TEST_ANN_FILE_ALT\n",
    "        \n",
    "        # Verificar que existan\n",
    "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
    "            print(\"‚ùå Faltan archivos de datos verificados de quality pipeline\")\n",
    "            return None, None, None\n",
    "            \n",
    "    else:\n",
    "        # Estructura legacy - buscar en datasets encontrados\n",
    "        train_files = [d for d in datasets if 'train' in d.name.lower()]\n",
    "        val_files = [d for d in datasets if 'val' in d.name.lower()]\n",
    "        test_files = [d for d in datasets if 'test' in d.name.lower()]\n",
    "        \n",
    "        print(\"üìä Archivos de datos encontrados:\")\n",
    "        print(f\"  Train: {len(train_files)} archivos\")\n",
    "        print(f\"  Val: {len(val_files)} archivos\") \n",
    "        print(f\"  Test: {len(test_files)} archivos\")\n",
    "        \n",
    "        # Seleccionar archivos (usar los primeros encontrados)\n",
    "        train_json = train_files[0] if train_files else None\n",
    "        val_json = val_files[0] if val_files else None\n",
    "        test_json = test_files[0] if test_files else None\n",
    "        \n",
    "        if not all([train_json, val_json, test_json]):\n",
    "            print(\"‚ùå Faltan archivos de datos. Necesitas train, val y test JSON files.\")\n",
    "            return None, None, None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Archivos seleccionados:\")\n",
    "    print(f\"  Train: {train_json}\")\n",
    "    print(f\"  Val: {val_json}\")\n",
    "    print(f\"  Test: {test_json}\")\n",
    "    \n",
    "    return train_json, val_json, test_json\n",
    "\n",
    "# Preparar datos\n",
    "train_json, val_json, test_json = prepare_training_data()\n",
    "\n",
    "if train_json and val_json and test_json:\n",
    "    print(\"‚úÖ Datos preparados correctamente\")\n",
    "else:\n",
    "    print(\"‚ùå Error preparando datos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cargar y Preparar Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para encontrar y validar datasets\n",
    "def find_datasets():\n",
    "    \"\"\"Encontrar datasets disponibles en el directorio\"\"\"\n",
    "    \n",
    "    data_root = Path(cfg.data_root)\n",
    "    datasets_found = []\n",
    "    \n",
    "    # Buscar archivos JSON de anotaciones\n",
    "    json_patterns = [\n",
    "        \"**/train_*.json\",\n",
    "        \"**/val_*.json\", \n",
    "        \"**/test_*.json\",\n",
    "        \"**/*_big_size_*.json\",\n",
    "        \"**/*_subframes_*.json\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        for json_file in data_root.glob(pattern):\n",
    "            if json_file.is_file():\n",
    "                datasets_found.append(json_file)\n",
    "    \n",
    "    print(f\"üìä Datasets encontrados ({len(datasets_found)}):\")\n",
    "    for i, dataset in enumerate(datasets_found):\n",
    "        print(f\"  {i+1}. {dataset}\")\n",
    "    \n",
    "    return datasets_found\n",
    "\n",
    "# Funci√≥n para analizar un dataset COCO\n",
    "def analyze_dataset(json_path):\n",
    "    \"\"\"Analizar estad√≠sticas de un dataset COCO\"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìà An√°lisis de {json_path.name}:\")\n",
    "    print(f\"  Im√°genes: {len(data['images'])}\")\n",
    "    print(f\"  Anotaciones: {len(data['annotations'])}\")\n",
    "    print(f\"  Categor√≠as: {len(data['categories'])}\")\n",
    "    \n",
    "    # Estad√≠sticas por categor√≠a\n",
    "    cat_counts = {}\n",
    "    for ann in data['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        cat_counts[cat_id] = cat_counts.get(cat_id, 0) + 1\n",
    "    \n",
    "    print(\"\\n  Distribuci√≥n por categor√≠a:\")\n",
    "    for cat in data['categories']:\n",
    "        count = cat_counts.get(cat['id'], 0)\n",
    "        print(f\"    {cat['name']}: {count}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Buscar y analizar datasets\n",
    "datasets = find_datasets()\n",
    "\n",
    "if datasets:\n",
    "    # Analizar el primer dataset encontrado como ejemplo\n",
    "    sample_data = analyze_dataset(datasets[0])\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron datasets. Verifica la estructura de directorios.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entrenamiento del Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos y entrenar modelo\n",
    "def prepare_and_train():\n",
    "    \"\"\"Preparar datos y entrenar el modelo Deformable DETR\"\"\"\n",
    "    \n",
    "    # Buscar archivos de datos\n",
    "    train_files = [d for d in datasets if 'train' in d.name.lower()]\n",
    "    val_files = [d for d in datasets if 'val' in d.name.lower()]\n",
    "    test_files = [d for d in datasets if 'test' in d.name.lower()]\n",
    "    \n",
    "    train_json = train_files[0] if train_files else None\n",
    "    val_json = val_files[0] if val_files else None\n",
    "    test_json = test_files[0] if test_files else None\n",
    "    \n",
    "    if not all([train_json, val_json, test_json]):\n",
    "        print(\"‚ùå Faltan archivos de datos. Necesitas train, val y test JSON files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úÖ Archivos seleccionados:\")\n",
    "    print(f\"  Train: {train_json}\")\n",
    "    print(f\"  Val: {val_json}\")\n",
    "    print(f\"  Test: {test_json}\")\n",
    "    \n",
    "    # Crear directorio de trabajo\n",
    "    workdir = Path(cfg.work_dir)\n",
    "    workdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üèóÔ∏è Construyendo configuraci√≥n del modelo...\")\n",
    "    \n",
    "    # Construir configuraci√≥n\n",
    "    mmdet_config = build_deformable_detr_config(\n",
    "        backbone=cfg.backbone,\n",
    "        imgsz=cfg.imgsz, \n",
    "        epochs=cfg.epochs,\n",
    "        train_json=train_json,\n",
    "        val_json=val_json,\n",
    "        test_json=test_json,\n",
    "        workdir=workdir\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Configuraci√≥n creada\")\n",
    "    \n",
    "    # Guardar configuraci√≥n\n",
    "    config_path = workdir / \"config.py\"\n",
    "    mmdet_config.dump(str(config_path))\n",
    "    print(f\"üíæ Configuraci√≥n guardada en: {config_path}\")\n",
    "    \n",
    "    # Iniciar entrenamiento\n",
    "    print(\"üöÄ Iniciando entrenamiento de Deformable DETR...\")\n",
    "    print(f\"‚è±Ô∏è  Tiempo estimado: {cfg.epochs * 3} minutos (aproximado)\")\n",
    "    \n",
    "    try:\n",
    "        # Crear runner\n",
    "        runner = Runner.from_cfg(mmdet_config)\n",
    "        \n",
    "        # Iniciar entrenamiento\n",
    "        runner.train()\n",
    "        \n",
    "        print(\"‚úÖ Entrenamiento completado exitosamente!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante el entrenamiento: {e}\")\n",
    "        print(\"üí° Verifica que los datos est√©n correctamente configurados\")\n",
    "\n",
    "# Ejecutar entrenamiento\n",
    "prepare_and_train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ ¬°Entrenamiento Completado!\n",
    "\n",
    "### üìã Resumen del Entrenamiento\n",
    "- **Modelo**: Deformable DETR con backbone {cfg.backbone}\n",
    "- **Tama√±o de imagen**: {cfg.imgsz}x{cfg.imgsz}\n",
    "- **√âpocas**: {cfg.epochs}\n",
    "- **Clases detectadas**: {len(cfg.classes)} especies de animales\n",
    "\n",
    "### üìä Pr√≥ximos Pasos\n",
    "1. **Evaluar m√©tricas**: Revisar mAP, precision, recall\n",
    "2. **Ajustar hiperpar√°metros**: Si es necesario mejorar el rendimiento\n",
    "3. **Exportar modelo**: Convertir a ONNX o TorchScript para deployment\n",
    "4. **Probar en nuevas im√°genes**: Validar en datos no vistos\n",
    "\n",
    "### üîß Configuraci√≥n Personalizada\n",
    "Para modificar par√°metros, edita la clase `Config` en la celda de configuraci√≥n:\n",
    "- Cambiar backbone: `\"swin_t\"` o `\"resnet50\"`\n",
    "- Ajustar √©pocas: `epochs = 100`\n",
    "- Modificar tama√±o de imagen: `imgsz = 1024`\n",
    "- Cambiar batch size: `batch_size = 8`\n",
    "\n",
    "### üìö Recursos Adicionales\n",
    "- [Documentaci√≥n MMDetection](https://mmdetection.readthedocs.io/)\n",
    "- [Deformable DETR Paper](https://arxiv.org/abs/2010.04159)\n",
    "- [DETR Paper](https://arxiv.org/abs/2005.12872)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
