# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IaYXe4liCCv05nS7wDKfsZyTWfwxGwS4
"""

# Instalar dependencias
# %pip install -q ultralytics pyyaml opencv-python pillow tqdm matplotlib seaborn pandas

# Verificar instalaciÃ³n
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# ============================================================
# VARIABLES GLOBALES PARA BACKUP AUTOMÃTICO
# ============================================================

# Variables globales para el sistema de backup
backup_thread_running = False
backup_thread = None

print("âœ… Variables globales de backup inicializadas")

"""## ğŸ“¦ Importar LibrerÃ­as"""
import os
import sys
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from PIL import Image
import shutil
import json
from tqdm import tqdm

from ultralytics import YOLO
from google.colab import files, drive
from IPython.display import Image as IPImage, display

# Configurar matplotlib
plt.style.use('default')
sns.set_palette("husl")

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")
    print(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

"""## ğŸ“ ConfiguraciÃ³n de Datos"""

# ============================================================
# CONFIGURACIÃ“N DE RUTAS PRINCIPALES (Unificado con HerdNet)
# ============================================================

# Definir la ruta base principal (ajustar segÃºn el entorno: Drive o local)
BASE_DIR = Path("/content/drive/MyDrive/aerial-wildlife-count")

# ============================================================
# RUTAS DE IMÃGENES Y ANOTACIONES (COCO JSON) - Igual que HerdNet
# ============================================================

# Rutas de los archivos de anotaciones en formato COCO
TRAIN_ANN_FILE = BASE_DIR / "data" / "coco" / "train" / "train_annotations.json"
VAL_ANN_FILE = BASE_DIR / "data" / "coco" / "val" / "val_annotations.json"
TEST_ANN_FILE = BASE_DIR / "data" / "coco" / "test" / "test_annotations.json"

# Rutas de las carpetas de imÃ¡genes correspondientes a cada conjunto
TRAIN_IMG_DIR = BASE_DIR / "data" / "images" / "train"
VAL_IMG_DIR = BASE_DIR / "data" / "images" / "val"
TEST_IMG_DIR = BASE_DIR / "data" / "images" / "test"

# ============================================================
# RUTAS ALTERNATIVAS (Fallback para compatibilidad)
# ============================================================

# Rutas alternativas si no existe la estructura estÃ¡ndar
TRAIN_ANN_FILE_ALT = BASE_DIR / "data" / "groundtruth" / "json" / "big_size" / "train_big_size_A_B_E_K_WH_WB.json"
VAL_ANN_FILE_ALT = BASE_DIR / "data" / "groundtruth" / "json" / "big_size" / "val_big_size_A_B_E_K_WH_WB.json"
TEST_ANN_FILE_ALT = BASE_DIR / "data" / "groundtruth" / "json" / "big_size" / "test_big_size_A_B_E_K_WH_WB.json"

# Rutas alternativas para imÃ¡genes
TRAIN_IMG_DIR_ALT = BASE_DIR / "data" / "train"
VAL_IMG_DIR_ALT = BASE_DIR / "data" / "val"
TEST_IMG_DIR_ALT = BASE_DIR / "data" / "test"

# ============================================================
# FUNCIÃ“N PARA CONFIGURAR DATOS (Unificada con HerdNet)
# ============================================================

def setup_data():
    """Configurar datos desde Google Drive con detecciÃ³n automÃ¡tica de estructura"""

    # Montar Google Drive
    drive.mount('/content/drive')
    print("âœ… Google Drive montado")

    # Verificar si existe la estructura estÃ¡ndar (igual que HerdNet)
    if TRAIN_ANN_FILE.exists() and TRAIN_IMG_DIR.exists():
        print("âœ… Datos encontrados (estructura estÃ¡ndar COCO)")
        return str(BASE_DIR), "standard"
    elif TRAIN_ANN_FILE_ALT.exists() and TRAIN_IMG_DIR_ALT.exists():
        print("âœ… Datos encontrados (estructura alternativa groundtruth)")
        return str(BASE_DIR), "groundtruth"
    else:
        # Buscar en ubicaciones alternativas
        drive_path = "/content/drive/MyDrive"
        possible_paths = [
            f"{drive_path}/aerial-wildlife-count",
            f"{drive_path}/datasets/aerial-wildlife-count",
            f"{drive_path}/Colab Notebooks/aerial-wildlife-count"
        ]

        for path in possible_paths:
            if os.path.exists(path):
                print(f"âœ… Dataset encontrado en ubicaciÃ³n alternativa: {path}")
                return path, "legacy"

        print("âŒ No se encontrÃ³ el dataset en Google Drive")
        return None, None

# Configurar datos
data_path, data_type = setup_data()
if data_path:
    print(f"ğŸ“ Ruta de datos configurada: {data_path}")
    print(f"ğŸ“Š Tipo de datos: {data_type}")
else:
    print("âš ï¸  Configura los datos manualmente antes de continuar")

"""## ğŸ“Š AnÃ¡lisis de Datos

"""

# FunciÃ³n para encontrar y analizar datasets (Unificada con HerdNet)
def find_and_analyze_datasets():
    """Encontrar y analizar datasets disponibles segÃºn el tipo de datos"""

    if not data_path:
        print("âŒ No hay ruta de datos configurada")
        return []

    datasets_found = []

    if data_type == "standard":
        # Estructura estÃ¡ndar COCO (igual que HerdNet)
        datasets_found = [TRAIN_ANN_FILE, VAL_ANN_FILE, TEST_ANN_FILE]
        print("ğŸ“Š Usando datasets estÃ¡ndar COCO:")
        print(f"  Train: {TRAIN_ANN_FILE}")
        print(f"  Val: {VAL_ANN_FILE}")
        print(f"  Test: {TEST_ANN_FILE}")

    elif data_type == "groundtruth":
        # Estructura alternativa groundtruth
        datasets_found = [TRAIN_ANN_FILE_ALT, VAL_ANN_FILE_ALT, TEST_ANN_FILE_ALT]
        print("ğŸ“Š Usando datasets groundtruth:")
        print(f"  Train: {TRAIN_ANN_FILE_ALT}")
        print(f"  Val: {VAL_ANN_FILE_ALT}")
        print(f"  Test: {TEST_ANN_FILE_ALT}")

    else:
        # Estructura legacy - buscar archivos JSON
        data_root = Path(data_path)
        json_patterns = [
            "**/train_*.json",
            "**/val_*.json",
            "**/test_*.json",
            "**/*_big_size_*.json",
            "**/*_subframes_*.json"
        ]

        for pattern in json_patterns:
            for json_file in data_root.glob(pattern):
                if json_file.is_file():
                    datasets_found.append(json_file)

        print(f"ğŸ“Š Datasets encontrados en estructura legacy ({len(datasets_found)}):")
        for i, dataset in enumerate(datasets_found):
            print(f"  {i+1}. {dataset}")

    # Analizar el primer dataset encontrado
    if datasets_found:
        sample_data = analyze_dataset(datasets_found[0])
        return datasets_found

    return []

# FunciÃ³n para analizar un dataset COCO
def analyze_dataset(json_path):
    """Analizar estadÃ­sticas de un dataset COCO"""

    with open(json_path, 'r') as f:
        data = json.load(f)

    print(f"\nğŸ“ˆ AnÃ¡lisis de {json_path.name}:")
    print(f"  ImÃ¡genes: {len(data['images'])}")
    print(f"  Anotaciones: {len(data['annotations'])}")
    print(f"  CategorÃ­as: {len(data['categories'])}")

    # EstadÃ­sticas por categorÃ­a
    cat_counts = {}
    for ann in data['annotations']:
        cat_id = ann['category_id']
        cat_counts[cat_id] = cat_counts.get(cat_id, 0) + 1

    print("\n  DistribuciÃ³n por categorÃ­a:")
    for cat in data['categories']:
        count = cat_counts.get(cat['id'], 0)
        print(f"    {cat['name']}: {count}")

    return data

# Buscar y analizar datasets
datasets = find_and_analyze_datasets()

"""## âš™ï¸ ConfiguraciÃ³n del Entrenamiento"""

# ConfiguraciÃ³n del entrenamiento
class YOLOConfig:
    def __init__(self):
        # ParÃ¡metros del modelo - OPTIMIZADOS PARA VELOCIDAD
        self.model = 'yolov8s.pt'  # yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt
        self.image_size = 512
        self.epochs = 5  # CORREGIDO: Cambiado de 100 a 5 para pruebas
        self.batch_size = 64  # AUMENTADO para mayor velocidad (ajustar segÃºn memoria)
        self.learning_rate = 0.01
        self.patience = 10
        self.device = 0  # GPU 0
        self.workers = 8  # AUMENTADO para mayor paralelizaciÃ³n
        self.project = '/content/runs_yolo'
        self.name = 'yolo_aerial_wildlife_v1'
        self.fp16 = True  # Mixed precision

        # NUEVAS CONFIGURACIONES V1
        self.save_period = 3  # Guardar cada 3 Ã©pocas (mÃ¡s frecuente)
        self.drive_backup_period = 3  # Backup en Drive cada 3 Ã©pocas
        self.drive_backup_dir = '/content/drive/MyDrive/aerial-wildlife-count-results/yolo_v1'
        self.resume_training = False  # CORREGIDO: Deshabilitado para evitar conflictos

        # Clases del dataset
        self.classes = [
            "Buffalo", "Elephant", "Kob",
            "Alcelaphinae", "Warthog", "Waterbuck"
        ]

    def print_config(self):
        print("ğŸ”§ ConfiguraciÃ³n YOLOv8 V1 (Optimizada):")
        print(f"  Modelo: {self.model}")
        print(f"  TamaÃ±o de imagen: {self.image_size}")
        print(f"  Ã‰pocas: {self.epochs}")
        print(f"  Batch size: {self.batch_size} (OPTIMIZADO)")
        print(f"  Workers: {self.workers} (OPTIMIZADO)")
        print(f"  Learning rate: {self.learning_rate}")
        print(f"  Dispositivo: {self.device}")
        print(f"  Mixed precision: {self.fp16}")
        print(f"  Save period: {self.save_period} Ã©pocas")
        print(f"  Drive backup: {self.drive_backup_period} Ã©pocas")
        print(f"  Resume training: {self.resume_training}")

# Crear instancia de configuraciÃ³n
yolo_config = YOLOConfig()
yolo_config.print_config()

"""## ğŸ”¥ Funciones de Backup AutomÃ¡tico en Drive

"""

# ============================================================
# FUNCIONES DE BACKUP AUTOMÃTICO EN DRIVE - V1 (TIEMPO REAL)
# ============================================================

import shutil
import time
from datetime import datetime
import threading
from ultralytics.utils.callbacks import default_callbacks

def create_drive_backup_dir():
    """Crear directorio de backup en Drive"""
    backup_dir = Path(yolo_config.drive_backup_dir)
    backup_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Directorio de backup creado: {backup_dir}")
    return backup_dir

def backup_to_drive(epoch=None, force=False):
    """Hacer backup de checkpoints a Drive"""
    try:
        # Crear directorio de backup
        backup_dir = create_drive_backup_dir()

        # Directorio fuente (Colab)
        source_dir = Path(f"{yolo_config.project}/{yolo_config.name}")

        if not source_dir.exists():
            print("âŒ No existe directorio de entrenamiento para hacer backup")
            return False

        # Crear subdirectorio con timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if epoch is not None:
            backup_subdir = backup_dir / f"epoch_{epoch}_{timestamp}"
        else:
            backup_subdir = backup_dir / f"backup_{timestamp}"

        backup_subdir.mkdir(exist_ok=True)

        # Copiar archivos importantes
        files_to_backup = [
            "weights/",
            "results.png",
            "confusion_matrix.png",
            "results.csv",
            "train_batch*.jpg",
            "val_batch*.jpg"
        ]

        copied_files = 0
        for pattern in files_to_backup:
            try:
                for file_path in source_dir.glob(pattern):
                    if file_path.is_file():
                        dest_path = backup_subdir / file_path.name
                        shutil.copy2(file_path, dest_path)
                        copied_files += 1
                    elif file_path.is_dir():
                        dest_path = backup_subdir / file_path.name
                        shutil.copytree(file_path, dest_path, dirs_exist_ok=True)
                        copied_files += 1
            except Exception as e:
                print(f"âš ï¸ Error copiando {pattern}: {e}")

        print(f"âœ… Backup completado: {copied_files} archivos copiados a {backup_subdir}")
        return True

    except Exception as e:
        print(f"âŒ Error en backup: {e}")
        return False

# ============================================================
# CALLBACK PERSONALIZADO PARA BACKUP EN TIEMPO REAL
# ============================================================

class RealtimeBackupCallback:
    """Callback personalizado para backup en tiempo real durante el entrenamiento"""

    def __init__(self, backup_period=3):
        self.backup_period = backup_period
        self.last_backup_epoch = 0
        self.backup_dir = None

    def on_train_epoch_end(self, trainer):
        """Se ejecuta al final de cada Ã©poca de entrenamiento"""
        try:
            current_epoch = trainer.epoch

            # Verificar si es momento de hacer backup
            if current_epoch - self.last_backup_epoch >= self.backup_period:
                print(f"ğŸ”„ Iniciando backup automÃ¡tico en tiempo real (Ã©poca {current_epoch})...")

                # Crear directorio de backup si no existe
                if self.backup_dir is None:
                    self.backup_dir = create_drive_backup_dir()

                # Hacer backup inmediato
                if backup_to_drive(epoch=current_epoch):
                    self.last_backup_epoch = current_epoch

                    # Actualizar archivo de control
                    backup_file = Path(f"{yolo_config.drive_backup_dir}/last_backup_epoch.txt")
                    backup_file.parent.mkdir(parents=True, exist_ok=True)
                    with open(backup_file, 'w') as f:
                        f.write(str(current_epoch))

                    print(f"âœ… Backup en tiempo real completado (Ã©poca {current_epoch})")
                else:
                    print(f"âŒ Error en backup en tiempo real (Ã©poca {current_epoch})")

        except Exception as e:
            print(f"âŒ Error en callback de backup: {e}")

    def on_val_end(self, validator):
        """Se ejecuta al final de la validaciÃ³n"""
        try:
            # Obtener la Ã©poca actual del trainer asociado al validator
            current_epoch = getattr(validator, 'epoch', None)
            if current_epoch is None and hasattr(validator, 'trainer'):
                current_epoch = getattr(validator.trainer, 'epoch', None)

            # Si no podemos obtener la Ã©poca, usar un valor por defecto
            if current_epoch is None:
                current_epoch = "unknown"
                print("âš ï¸ No se pudo obtener el nÃºmero de Ã©poca para backup")
                return

            # CORREGIDO: Acceso seguro a mÃ©tricas
            try:
                if hasattr(validator, 'metrics') and validator.metrics is not None:
                    # Usar results_dict en lugar de get()
                    if hasattr(validator.metrics, 'results_dict'):
                        metrics_dict = validator.metrics.results_dict
                        mAP50 = metrics_dict.get('metrics/mAP50(B)', 0.0)
                        mAP50_95 = metrics_dict.get('metrics/mAP50-95(B)', 0.0)
                        print(f"ğŸ“Š Ã‰poca {current_epoch}: mAP@0.5 = {mAP50:.4f}, mAP@0.5:0.95 = {mAP50_95:.4f}")
                    else:
                        print(f"ğŸ“Š Ã‰poca {current_epoch}: ValidaciÃ³n completada")
                else:
                    print(f"ğŸ“Š Ã‰poca {current_epoch}: ValidaciÃ³n completada")
            except Exception as metrics_error:
                print(f"âš ï¸ Error accediendo a mÃ©tricas: {metrics_error}")
                print(f"ğŸ“Š Ã‰poca {current_epoch}: ValidaciÃ³n completada")

        except Exception as e:
            print(f"âŒ Error en callback de validaciÃ³n: {e}")

# ============================================================
# FUNCIONES DE BACKUP LEGACY (MANTENIDAS PARA COMPATIBILIDAD)
# ============================================================

def auto_backup_thread():
    """Thread para backup automÃ¡tico cada X Ã©pocas (LEGACY - NO USAR)"""
    global backup_thread_running
    backup_thread_running = True

    print("ğŸ”„ Thread de backup automÃ¡tico iniciado (LEGACY)")

    while backup_thread_running:
        try:
            # Verificar si hay nuevos checkpoints
            weights_dir = Path(f"{yolo_config.project}/{yolo_config.name}/weights")
            if weights_dir.exists():
                checkpoints = list(weights_dir.glob("*.pt"))
                if checkpoints:
                    # Obtener el checkpoint mÃ¡s reciente
                    latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)

                    # Verificar si necesita backup
                    backup_file = Path(f"{yolo_config.drive_backup_dir}/last_backup_epoch.txt")
                    last_backup_epoch = 0

                    if backup_file.exists():
                        try:
                            with open(backup_file, 'r') as f:
                                last_backup_epoch = int(f.read().strip())
                        except:
                            last_backup_epoch = 0

                    # Extraer Ã©poca del nombre del archivo
                    current_epoch = 0
                    if "epoch" in latest_checkpoint.name:
                        try:
                            current_epoch = int(latest_checkpoint.name.split("epoch_")[1].split(".")[0])
                        except:
                            current_epoch = 0

                    # Hacer backup si ha pasado el perÃ­odo configurado
                    if current_epoch - last_backup_epoch >= yolo_config.drive_backup_period:
                        print(f"ğŸ”„ Iniciando backup automÃ¡tico (Ã©poca {current_epoch})...")
                        if backup_to_drive(epoch=current_epoch):
                            # Actualizar archivo de control
                            backup_file.parent.mkdir(parents=True, exist_ok=True)
                            with open(backup_file, 'w') as f:
                                f.write(str(current_epoch))
                            print(f"âœ… Backup automÃ¡tico completado (Ã©poca {current_epoch})")

            # Esperar 5 minutos antes de verificar nuevamente
            time.sleep(300)  # 5 minutos

        except Exception as e:
            print(f"âŒ Error en backup automÃ¡tico: {e}")
            time.sleep(60)  # Esperar 1 minuto en caso de error

    print("ğŸ›‘ Thread de backup automÃ¡tico detenido")

def start_auto_backup():
    """Iniciar backup automÃ¡tico en segundo plano (LEGACY - NO USAR)"""
    global backup_thread, backup_thread_running

    # Detener thread anterior si existe
    if backup_thread_running:
        stop_auto_backup()
        time.sleep(2)  # Esperar a que se detenga

    backup_thread_running = False

    # Verificar si ya hay un thread corriendo
    if 'backup_thread' in globals() and backup_thread is not None and backup_thread.is_alive():
        print("âš ï¸ Backup automÃ¡tico ya estÃ¡ ejecutÃ¡ndose")
        return

    backup_thread = threading.Thread(target=auto_backup_thread, daemon=True)
    backup_thread.start()
    print("ğŸš€ Backup automÃ¡tico iniciado en segundo plano")

def stop_auto_backup():
    """Detener backup automÃ¡tico (LEGACY - NO USAR)"""
    global backup_thread_running
    backup_thread_running = False
    print("ğŸ›‘ Backup automÃ¡tico detenido")

def resume_training_from_drive():
    """Reanudar entrenamiento desde el Ãºltimo checkpoint en Drive"""
    try:
        backup_dir = Path(yolo_config.drive_backup_dir)
        if not backup_dir.exists():
            print("âŒ No hay backups disponibles en Drive")
            return None

        # Buscar el backup mÃ¡s reciente
        backup_dirs = [d for d in backup_dir.iterdir() if d.is_dir() and "epoch_" in d.name]
        if not backup_dirs:
            print("âŒ No se encontraron backups con checkpoints")
            return None

        # Ordenar por fecha de modificaciÃ³n
        latest_backup = max(backup_dirs, key=lambda x: x.stat().st_mtime)

        # Buscar el mejor checkpoint
        weights_dir = latest_backup / "weights"
        if weights_dir.exists():
            checkpoints = list(weights_dir.glob("*.pt"))
            if checkpoints:
                # Priorizar best.pt, luego el mÃ¡s reciente
                best_checkpoint = None
                for ckpt in checkpoints:
                    if "best" in ckpt.name:
                        best_checkpoint = ckpt
                        break

                if not best_checkpoint:
                    best_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)

                print(f"âœ… Checkpoint encontrado para reanudar: {best_checkpoint}")
                return str(best_checkpoint)

        print("âŒ No se encontrÃ³ checkpoint vÃ¡lido en backups")
        return None

    except Exception as e:
        print(f"âŒ Error al buscar checkpoint para reanudar: {e}")
        return None

def clean_existing_checkpoints():
    """Limpiar checkpoints existentes para evitar conflictos de reanudaciÃ³n"""
    try:
        results_dir = Path(f"{yolo_config.project}/{yolo_config.name}")
        if results_dir.exists():
            print("ğŸ§¹ Limpiando checkpoints existentes para evitar conflictos...")
            
            # Limpiar directorio de resultados
            import shutil
            shutil.rmtree(results_dir)
            print(f"âœ… Directorio de resultados limpiado: {results_dir}")
            
            # Crear directorio vacÃ­o
            results_dir.mkdir(parents=True, exist_ok=True)
            print("âœ… Directorio de resultados recreado")
            
        return True
    except Exception as e:
        print(f"âŒ Error limpiando checkpoints: {e}")
        return False

def validate_training_config():
    """Validar configuraciÃ³n de entrenamiento para evitar errores"""
    try:
        print("ğŸ” Validando configuraciÃ³n de entrenamiento...")
        
        # Verificar que las Ã©pocas estÃ©n configuradas correctamente
        if yolo_config.epochs <= 0:
            print("âŒ Error: NÃºmero de Ã©pocas debe ser mayor a 0")
            return False
            
        if yolo_config.epochs > 1000:
            print("âš ï¸ Advertencia: NÃºmero de Ã©pocas muy alto, esto puede tomar mucho tiempo")
            
        # Verificar que el batch size sea vÃ¡lido
        if yolo_config.batch_size <= 0:
            print("âŒ Error: Batch size debe ser mayor a 0")
            return False
            
        # Verificar que el learning rate sea vÃ¡lido
        if yolo_config.learning_rate <= 0 or yolo_config.learning_rate > 1:
            print("âŒ Error: Learning rate debe estar entre 0 y 1")
            return False
            
        # Verificar que el save_period sea menor que las Ã©pocas
        if yolo_config.save_period >= yolo_config.epochs:
            print(f"âš ï¸ Advertencia: save_period ({yolo_config.save_period}) >= epochs ({yolo_config.epochs})")
            print("ğŸ”„ Ajustando save_period a epochs/2...")
            yolo_config.save_period = max(1, yolo_config.epochs // 2)
            
        print(f"âœ… ConfiguraciÃ³n validada:")
        print(f"  - Ã‰pocas: {yolo_config.epochs}")
        print(f"  - Batch size: {yolo_config.batch_size}")
        print(f"  - Learning rate: {yolo_config.learning_rate}")
        print(f"  - Save period: {yolo_config.save_period}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error validando configuraciÃ³n: {e}")
        return False

print("âœ… Funciones de backup automÃ¡tico cargadas")

"""## ğŸ“Š Funciones de Monitoreo en Tiempo Real

"""

# ============================================================
# FUNCIONES DE MONITOREO EN TIEMPO REAL
# ============================================================

def monitor_training_progress():
    """Monitorear el progreso del entrenamiento en tiempo real"""
    try:
        results_dir = Path(f"{yolo_config.project}/{yolo_config.name}")

        if not results_dir.exists():
            print("âŒ Directorio de resultados no encontrado")
            return

        # Verificar archivos de resultados
        results_csv = results_dir / "results.csv"
        if results_csv.exists():
            import pandas as pd
            df = pd.read_csv(results_csv)
            if not df.empty:
                latest_epoch = df.iloc[-1]
                print(f"ğŸ“Š Progreso actual:")
                print(f"  Ã‰poca: {latest_epoch.get('epoch', 'N/A')}")
                print(f"  mAP: {latest_epoch.get('metrics/mAP50(B)', 'N/A'):.4f}")
                print(f"  Loss: {latest_epoch.get('train/box_loss', 'N/A'):.4f}")
                print(f"  Val Loss: {latest_epoch.get('val/box_loss', 'N/A'):.4f}")

        # Verificar checkpoints
        weights_dir = results_dir / "weights"
        if weights_dir.exists():
            checkpoints = list(weights_dir.glob("*.pt"))
            print(f"ğŸ“ Checkpoints disponibles: {len(checkpoints)}")
            for ckpt in checkpoints:
                size_mb = ckpt.stat().st_size / (1024 * 1024)
                print(f"  - {ckpt.name} ({size_mb:.1f} MB)")

        # Verificar backups en Drive
        backup_dir = Path(yolo_config.drive_backup_dir)
        if backup_dir.exists():
            backups = list(backup_dir.glob("epoch_*"))
            print(f"ğŸ’¾ Backups en Drive: {len(backups)}")

    except Exception as e:
        print(f"âŒ Error en monitoreo: {e}")

def get_training_status():
    """Obtener estado actual del entrenamiento"""
    try:
        results_dir = Path(f"{yolo_config.project}/{yolo_config.name}")

        if not results_dir.exists():
            return "No iniciado"

        # Verificar si hay resultados
        results_csv = results_dir / "results.csv"
        if results_csv.exists():
            import pandas as pd
            df = pd.read_csv(results_csv)
            if not df.empty:
                latest_epoch = df.iloc[-1]['epoch']
                total_epochs = yolo_config.epochs
                progress = (latest_epoch / total_epochs) * 100
                return f"En progreso: {latest_epoch}/{total_epochs} Ã©pocas ({progress:.1f}%)"

        return "Iniciando"

    except Exception as e:
        return f"Error: {e}"

def estimate_remaining_time():
    """Estimar tiempo restante de entrenamiento"""
    try:
        results_dir = Path(f"{yolo_config.project}/{yolo_config.name}")
        results_csv = results_dir / "results.csv"

        if not results_csv.exists():
            return "No disponible"

        import pandas as pd
        df = pd.read_csv(results_csv)

        if len(df) < 2:
            return "Calculando..."

        # Calcular tiempo promedio por Ã©poca basado en el nÃºmero de Ã©pocas
        # Asumir que cada Ã©poca toma aproximadamente el mismo tiempo
        current_epoch = df.iloc[-1]['epoch']
        remaining_epochs = yolo_config.epochs - current_epoch

        # EstimaciÃ³n simple: asumir 2-5 minutos por Ã©poca
        estimated_minutes = remaining_epochs * 3  # 3 minutos promedio por Ã©poca

        if estimated_minutes < 60:
            return f"Tiempo estimado restante: {estimated_minutes:.0f} minutos"
        else:
            hours = estimated_minutes / 60
            return f"Tiempo estimado restante: {hours:.1f} horas"

    except Exception as e:
        return f"Error: {e}"

def show_training_status():
    """Mostrar estado completo del entrenamiento"""
    print("=" * 60)
    print("ğŸ“Š ESTADO DEL ENTRENAMIENTO YOLOv8 V1")
    print("=" * 60)
    print(f"Estado: {get_training_status()}")
    print(f"Tiempo restante: {estimate_remaining_time()}")
    print()
    monitor_training_progress()
    print("=" * 60)

# ============================================================
# CALLBACK DE MONITOREO DURANTE EL ENTRENAMIENTO
# ============================================================

class TrainingMonitorCallback:
    """Callback para monitorear el entrenamiento en tiempo real"""

    def __init__(self, monitor_period=5):
        self.monitor_period = monitor_period
        self.last_monitor_epoch = 0

    def on_train_epoch_end(self, trainer):
        """Se ejecuta al final de cada Ã©poca de entrenamiento"""
        try:
            current_epoch = trainer.epoch

            # Monitorear cada X Ã©pocas
            if current_epoch - self.last_monitor_epoch >= self.monitor_period:
                print(f"\nğŸ“Š MONITOREO Ã‰POCA {current_epoch}:")
                loss_val = getattr(trainer, 'loss', None)
                if loss_val is not None:
                    try:
                        print(f"  Loss: {float(loss_val):.4f}")
                    except Exception:
                        print(f"  Loss: {loss_val}")

                # Mostrar progreso
                progress = (current_epoch / yolo_config.epochs) * 100
                print(f"  Progreso: {current_epoch}/{yolo_config.epochs} Ã©pocas ({progress:.1f}%)")

                self.last_monitor_epoch = current_epoch

        except Exception as e:
            print(f"âŒ Error en callback de monitoreo: {e}")

    def on_val_end(self, validator):
        """Se ejecuta al final de la validaciÃ³n"""
        try:
            # Obtener la Ã©poca actual del trainer asociado al validator
            current_epoch = getattr(validator, 'epoch', None)
            if current_epoch is None and hasattr(validator, 'trainer'):
                current_epoch = getattr(validator.trainer, 'epoch', None)

            # Si no podemos obtener la Ã©poca, usar un valor por defecto
            if current_epoch is None:
                current_epoch = "unknown"

            # CORREGIDO: Acceso seguro a mÃ©tricas
            try:
                if hasattr(validator, 'metrics') and validator.metrics is not None:
                    print(f"ğŸ“ˆ Ã‰poca {current_epoch} - ValidaciÃ³n completada")
                    
                    # Usar results_dict en lugar de get()
                    if hasattr(validator.metrics, 'results_dict'):
                        metrics_dict = validator.metrics.results_dict
                        mAP50 = metrics_dict.get('metrics/mAP50(B)', 'N/A')
                        mAP50_95 = metrics_dict.get('metrics/mAP50-95(B)', 'N/A')
                        
                        if mAP50 != 'N/A':
                            print(f"  mAP@0.5: {mAP50:.4f}")
                        else:
                            print(f"  mAP@0.5: N/A")
                            
                        if mAP50_95 != 'N/A':
                            print(f"  mAP@0.5:0.95: {mAP50_95:.4f}")
                        else:
                            print(f"  mAP@0.5:0.95: N/A")
                    else:
                        print(f"  MÃ©tricas no disponibles")
                else:
                    print(f"ğŸ“ˆ Ã‰poca {current_epoch} - ValidaciÃ³n completada (sin mÃ©tricas)")
            except Exception as metrics_error:
                print(f"âš ï¸ Error accediendo a mÃ©tricas: {metrics_error}")
                print(f"ğŸ“ˆ Ã‰poca {current_epoch} - ValidaciÃ³n completada")

        except Exception as e:
            print(f"âŒ Error en callback de validaciÃ³n: {e}")

print("âœ… Funciones de monitoreo en tiempo real cargadas")

"""## ğŸ”„ ConversiÃ³n de Datos COCO a YOLO

"""

# FunciÃ³n para convertir COCO a YOLO
def coco_to_yolo(coco_json_path, images_dir, output_dir, class_names):
    """Convierte anotaciones COCO a formato YOLO"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Leer archivo COCO
    with open(coco_json_path, 'r') as f:
        coco_data = json.load(f)

    # Crear mapeo de categorÃ­as
    cat_id_to_class = {cat['id']: cat['name'] for cat in coco_data['categories']}
    class_to_id = {name: idx for idx, name in enumerate(class_names)}

    # Debug: Mostrar mapeo de clases
    print(f"ğŸ” Mapeo de clases COCO -> YOLO:")
    for cat_id, cat_name in cat_id_to_class.items():
        yolo_id = class_to_id.get(cat_name, -1)
        print(f"  COCO ID {cat_id} ({cat_name}) -> YOLO ID {yolo_id}")
    
    print(f"ğŸ” Clases esperadas en YOLO: {class_names}")
    print(f"ğŸ” Clases encontradas en COCO: {list(cat_id_to_class.values())}")

    # Crear mapeo de imÃ¡genes
    img_id_to_info = {img['id']: img for img in coco_data['images']}

    # Procesar anotaciones
    annotations_by_image = {}
    class_counts = {}
    for ann in coco_data['annotations']:
        img_id = ann['image_id']
        if img_id not in annotations_by_image:
            annotations_by_image[img_id] = []
        annotations_by_image[img_id].append(ann)
        
        # Contar clases
        cat_name = cat_id_to_class[ann['category_id']]
        class_counts[cat_name] = class_counts.get(cat_name, 0) + 1

    print(f"ğŸ” DistribuciÃ³n de clases en COCO:")
    for cat_name, count in class_counts.items():
        print(f"  {cat_name}: {count} anotaciones")

    # Convertir cada imagen
    converted_count = 0
    skipped_count = 0
    
    for img_id, img_info in tqdm(img_id_to_info.items(), desc="Convirtiendo a YOLO"):
        img_name = img_info['file_name']
        img_width = img_info['width']
        img_height = img_info['height']

        # Crear archivo de anotaciÃ³n YOLO
        txt_name = Path(img_name).stem + '.txt'
        txt_path = output_dir / txt_name

        with open(txt_path, 'w') as f:
            if img_id in annotations_by_image:
                for ann in annotations_by_image[img_id]:
                    cat_name = cat_id_to_class[ann['category_id']]
                    if cat_name in class_to_id:
                        class_id = class_to_id[cat_name]

                        # Convertir bbox [x, y, w, h] a [center_x, center_y, w, h] normalizado
                        x, y, w, h = ann['bbox']
                        center_x = (x + w/2) / img_width
                        center_y = (y + h/2) / img_height
                        norm_w = w / img_width
                        norm_h = h / img_height

                        f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {norm_w:.6f} {norm_h:.6f}\n")
                        converted_count += 1
                    else:
                        print(f"âš ï¸ Clase '{cat_name}' no encontrada en mapeo YOLO")
                        skipped_count += 1

    print(f"âœ… Convertido {len(img_id_to_info)} imÃ¡genes a formato YOLO")
    print(f"ğŸ“Š Anotaciones convertidas: {converted_count}")
    print(f"âš ï¸ Anotaciones omitidas: {skipped_count}")
    
    # Verificar integridad de los datos convertidos
    verify_yolo_data_integrity(output_dir, class_names)
    
    return output_dir

def verify_yolo_data_integrity(labels_dir, class_names):
    """Verificar la integridad de los datos YOLO convertidos"""
    
    print(f"\nğŸ” Verificando integridad de datos YOLO en {labels_dir}...")
    
    label_files = list(Path(labels_dir).glob("*.txt"))
    print(f"ğŸ“ Archivos de etiquetas encontrados: {len(label_files)}")
    
    class_counts = {i: 0 for i in range(len(class_names))}
    total_annotations = 0
    invalid_files = 0
    
    for label_file in label_files:
        try:
            with open(label_file, 'r') as f:
                lines = f.readlines()
                
            for line in lines:
                line = line.strip()
                if line:
                    parts = line.split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        if 0 <= class_id < len(class_names):
                            class_counts[class_id] += 1
                            total_annotations += 1
                        else:
                            print(f"âš ï¸ ID de clase invÃ¡lido {class_id} en {label_file.name}")
                    else:
                        print(f"âš ï¸ Formato invÃ¡lido en {label_file.name}: {line}")
        except Exception as e:
            print(f"âŒ Error leyendo {label_file.name}: {e}")
            invalid_files += 1
    
    print(f"\nğŸ“Š DistribuciÃ³n de clases en datos YOLO:")
    for i, class_name in enumerate(class_names):
        count = class_counts[i]
        percentage = (count / total_annotations * 100) if total_annotations > 0 else 0
        print(f"  {i}: {class_name}: {count} anotaciones ({percentage:.1f}%)")
    
    print(f"\nğŸ“ˆ Resumen:")
    print(f"  Total de anotaciones: {total_annotations}")
    print(f"  Archivos con errores: {invalid_files}")
    print(f"  Clases con datos: {sum(1 for count in class_counts.values() if count > 0)}/{len(class_names)}")
    
    if total_annotations == 0:
        print("âŒ Â¡ADVERTENCIA: No se encontraron anotaciones vÃ¡lidas!")
    elif any(count == 0 for count in class_counts.values()):
        print("âš ï¸ ADVERTENCIA: Algunas clases no tienen anotaciones")
    
    return class_counts, total_annotations

# Preparar datos para conversiÃ³n (Unificado con HerdNet)
def prepare_yolo_data():
    """Preparar datos para entrenamiento YOLO segÃºn el tipo de datos"""

    if data_type == "standard":
        # Usar rutas estÃ¡ndar COCO (igual que HerdNet)
        train_json = TRAIN_ANN_FILE
        val_json = VAL_ANN_FILE
        test_json = TEST_ANN_FILE

        # Verificar que existan
        if not all([train_json.exists(), val_json.exists()]):
            print("âŒ Faltan archivos de datos estÃ¡ndar COCO")
            return None, None, None

    elif data_type == "groundtruth":
        # Usar rutas groundtruth
        train_json = TRAIN_ANN_FILE_ALT
        val_json = VAL_ANN_FILE_ALT
        test_json = TEST_ANN_FILE_ALT

        # Verificar que existan
        if not all([train_json.exists(), val_json.exists()]):
            print("âŒ Faltan archivos de datos groundtruth")
            return None, None, None

    else:
        # Estructura legacy - buscar en datasets encontrados
        if not datasets:
            print("âŒ No hay datasets disponibles")
            return None, None, None

        # Buscar archivos de datos
        train_files = [d for d in datasets if 'train' in d.name.lower()]
        val_files = [d for d in datasets if 'val' in d.name.lower()]
        test_files = [d for d in datasets if 'test' in d.name.lower()]

        train_json = train_files[0] if train_files else None
        val_json = val_files[0] if val_files else None
        test_json = test_files[0] if test_files else None

        if not all([train_json, val_json]):
            print("âŒ Faltan archivos de datos. Necesitas al menos train y val JSON files.")
            return None, None, None

    print(f"âœ… Archivos seleccionados:")
    print(f"  Train: {train_json}")
    print(f"  Val: {val_json}")
    if test_json and test_json.exists():
        print(f"  Test: {test_json}")

    return train_json, val_json, test_json

# Preparar datos
train_json, val_json, test_json = prepare_yolo_data()

if train_json and val_json:
    print("âœ… Datos preparados para conversiÃ³n")
else:
    print("âŒ Error preparando datos")

# Convertir datos si estÃ¡n disponibles (ANTES del entrenamiento)
if train_json and val_json:

    # Determinar directorios de imÃ¡genes segÃºn el tipo de datos
    if data_type == "standard":
        train_img_dir = TRAIN_IMG_DIR
        val_img_dir = VAL_IMG_DIR
        test_img_dir = TEST_IMG_DIR
    elif data_type == "groundtruth":
        train_img_dir = TRAIN_IMG_DIR_ALT
        val_img_dir = VAL_IMG_DIR_ALT
        test_img_dir = TEST_IMG_DIR_ALT
    else:
        # Estructura legacy
        train_img_dir = train_json.parent / "images" if (train_json.parent / "images").exists() else train_json.parent.parent / "train"
        val_img_dir = val_json.parent.parent / "val"
        test_img_dir = None

    # Convertir datos de entrenamiento
    print("ğŸ”„ Convirtiendo datos de entrenamiento...")
    train_yolo_dir = coco_to_yolo(
        train_json,
        train_img_dir,
        '/content/yolo_data/train/labels',
        yolo_config.classes
    )

    # Convertir datos de validaciÃ³n
    print("ğŸ”„ Convirtiendo datos de validaciÃ³n...")
    val_yolo_dir = coco_to_yolo(
        val_json,
        val_img_dir,
        '/content/yolo_data/val/labels',
        yolo_config.classes
    )

    # Crear directorios de imÃ¡genes
    os.makedirs('/content/yolo_data/train/images', exist_ok=True)
    os.makedirs('/content/yolo_data/val/images', exist_ok=True)

    # Copiar imÃ¡genes
    print("ğŸ“ Copiando imÃ¡genes...")
    for img_file in tqdm(os.listdir(train_img_dir)):
        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
            src = os.path.join(train_img_dir, img_file)
            dst = os.path.join('/content/yolo_data/train/images', img_file)
            if os.path.exists(src):
                shutil.copy2(src, dst)

    for img_file in tqdm(os.listdir(val_img_dir)):
        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
            src = os.path.join(val_img_dir, img_file)
            dst = os.path.join('/content/yolo_data/val/images', img_file)
            if os.path.exists(src):
                shutil.copy2(src, dst)

    # Copiar imÃ¡genes de test si estÃ¡n disponibles
    if test_img_dir and test_img_dir.exists():
        print("ğŸ“ Copiando imÃ¡genes de test...")
        os.makedirs('/content/yolo_data/test/images', exist_ok=True)
        for img_file in tqdm(os.listdir(test_img_dir)):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                src = os.path.join(test_img_dir, img_file)
                dst = os.path.join('/content/yolo_data/test/images', img_file)
                if os.path.exists(src):
                    shutil.copy2(src, dst)

    # Crear archivo de configuraciÃ³n YOLO
    test_line = "test: test/images\n" if test_img_dir and test_img_dir.exists() else ""
    yolo_config_content = f"""# Dataset configuration for YOLOv8
path: /content/yolo_data
train: train/images
val: val/images
{test_line}
# Classes
nc: {len(yolo_config.classes)}
names: {yolo_config.classes}
"""

    with open('/content/yolo_data/dataset.yaml', 'w') as f:
        f.write(yolo_config_content)

    print("âœ… ConversiÃ³n completada")
    print(f"ğŸ“Š ImÃ¡genes de entrenamiento: {len(os.listdir('/content/yolo_data/train/images'))}")
    print(f"ğŸ“Š ImÃ¡genes de validaciÃ³n: {len(os.listdir('/content/yolo_data/val/images'))}")
    if test_img_dir and test_img_dir.exists():
        print(f"ğŸ“Š ImÃ¡genes de test: {len(os.listdir('/content/yolo_data/test/images'))}")

else:
    print("âŒ No se puede convertir sin datos vÃ¡lidos")

"""## ğŸš€ Entrenamiento Optimizado con Backup AutomÃ¡tico

"""

# Entrenar modelo YOLOv8 V1 con backup automÃ¡tico EN TIEMPO REAL
if train_json and val_json:

    print("ğŸš€ Iniciando entrenamiento YOLOv8 V1 (Optimizado con Backup en Tiempo Real)...")
    print("ğŸ”¥ CaracterÃ­sticas V1:")
    print(f"  - Batch size optimizado: {yolo_config.batch_size}")
    print(f"  - Workers optimizados: {yolo_config.workers}")
    print(f"  - Guardado cada {yolo_config.save_period} Ã©pocas")
    print(f"  - Backup en Drive cada {yolo_config.drive_backup_period} Ã©pocas (TIEMPO REAL)")
    print(f"  - RecuperaciÃ³n automÃ¡tica: {yolo_config.resume_training}")
    print("ğŸ”¥ NUEVO: Backup automÃ¡tico en tiempo real durante el entrenamiento")

    # CORREGIDO: Validar configuraciÃ³n antes del entrenamiento
    if not validate_training_config():
        print("âŒ Error en configuraciÃ³n de entrenamiento. Abortando...")
        exit(1)

    # CORREGIDO: Limpiar checkpoints existentes para evitar conflictos
    if not yolo_config.resume_training:
        clean_existing_checkpoints()

    # Inicializar modelo
    model = YOLO(yolo_config.model)

    # Crear callbacks para backup y monitoreo en tiempo real
    backup_callback = RealtimeBackupCallback(backup_period=yolo_config.drive_backup_period)
    monitor_callback = TrainingMonitorCallback(monitor_period=5)  # Monitorear cada 5 Ã©pocas

    print(f"âœ… Callback de backup en tiempo real creado (cada {yolo_config.drive_backup_period} Ã©pocas)")
    print(f"âœ… Callback de monitoreo en tiempo real creado (cada 5 Ã©pocas)")

    # Configurar parÃ¡metros de entrenamiento - OPTIMIZADOS V1
    train_args = {
        'data': '/content/yolo_data/dataset.yaml',
        'epochs': yolo_config.epochs,
        'imgsz': yolo_config.image_size,
        'batch': yolo_config.batch_size,
        'device': yolo_config.device,
        'workers': yolo_config.workers,
        'project': yolo_config.project,
        'name': yolo_config.name,
        'patience': yolo_config.patience,
        'lr0': yolo_config.learning_rate,
        'amp': yolo_config.fp16,
        'save': True,
        'save_period': yolo_config.save_period,  # Consistente: 5 Ã©pocas
        'val': True,
        'plots': True,
        'verbose': True,
    }

    # CORREGIDO: LÃ³gica de reanudaciÃ³n mejorada para evitar errores
    resume_path = None
    if yolo_config.resume_training:
        # Buscar en Colab primero
        weights_dir = Path(f"{yolo_config.project}/{yolo_config.name}/weights")
        if weights_dir.exists():
            checkpoints = list(weights_dir.glob("*.pt"))
            if checkpoints:
                # Verificar que el checkpoint no estÃ© completo
                latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)
                
                # Verificar si el entrenamiento ya estÃ¡ completo
                results_csv = Path(f"{yolo_config.project}/{yolo_config.name}/results.csv")
                if results_csv.exists():
                    import pandas as pd
                    try:
                        df = pd.read_csv(results_csv)
                        if not df.empty:
                            last_epoch = df.iloc[-1]['epoch']
                            if last_epoch >= yolo_config.epochs - 1:
                                print(f"âš ï¸ Entrenamiento ya completado ({last_epoch}/{yolo_config.epochs} Ã©pocas)")
                                print("ğŸ”„ Iniciando nuevo entrenamiento sin reanudar...")
                                resume_path = None
                            else:
                                resume_path = str(latest_checkpoint)
                                print(f"ğŸ”„ Reanudando desde checkpoint local: {resume_path}")
                        else:
                            resume_path = str(latest_checkpoint)
                            print(f"ğŸ”„ Reanudando desde checkpoint local: {resume_path}")
                    except Exception as e:
                        print(f"âš ï¸ Error leyendo results.csv: {e}")
                        resume_path = str(latest_checkpoint)
                        print(f"ğŸ”„ Reanudando desde checkpoint local: {resume_path}")
                else:
                    resume_path = str(latest_checkpoint)
                    print(f"ğŸ”„ Reanudando desde checkpoint local: {resume_path}")
        else:
            # Buscar en Drive solo si no hay checkpoints locales
            resume_path = resume_training_from_drive()
            if resume_path:
                print(f"ğŸ”„ Reanudando desde checkpoint en Drive: {resume_path}")

    if resume_path:
        train_args['resume'] = resume_path
    else:
        print("ğŸ”„ Iniciando entrenamiento desde cero...")

    print("ğŸ“‹ ParÃ¡metros de entrenamiento:")
    for key, value in train_args.items():
        print(f"  {key}: {value}")

    # NO usar el thread de backup legacy - usar callback en tiempo real
    print("ğŸ”„ Usando sistema de backup en tiempo real (NO thread legacy)")

    # Iniciar entrenamiento con callbacks de backup y monitoreo
    print("ğŸš€ Iniciando entrenamiento con backup y monitoreo en tiempo real...")

    # Registrar los callbacks personalizados
    model.add_callback('on_train_epoch_end', backup_callback.on_train_epoch_end)
    model.add_callback('on_val_end', backup_callback.on_val_end)

    # Registrar callback de monitoreo
    model.add_callback('on_train_epoch_end', monitor_callback.on_train_epoch_end)
    model.add_callback('on_val_end', monitor_callback.on_val_end)

    # CORREGIDO: Iniciar entrenamiento con manejo de errores robusto
    try:
        print("ğŸš€ Iniciando entrenamiento...")
        results = model.train(**train_args)
        print("âœ… Entrenamiento completado exitosamente!")
        print(f"ğŸ“ Resultados guardados en: {yolo_config.project}/{yolo_config.name}")

        # Hacer backup final
        print("ğŸ”„ Realizando backup final...")
        backup_to_drive(epoch="final")
        
    except AssertionError as e:
        if "training to" in str(e) and "epochs is finished" in str(e):
            print("âŒ Error: El entrenamiento ya se completÃ³ anteriormente")
            print("ğŸ’¡ SoluciÃ³n: Limpia los checkpoints existentes o cambia el nombre del experimento")
            print("ğŸ”„ Limpiando checkpoints y reiniciando...")
            
            # Limpiar y reiniciar
            clean_existing_checkpoints()
            
            # Reiniciar entrenamiento sin resume
            train_args.pop('resume', None)
            print("ğŸ”„ Reiniciando entrenamiento desde cero...")
            results = model.train(**train_args)
            print("âœ… Entrenamiento completado exitosamente!")
        else:
            print(f"âŒ Error de AssertionError: {e}")
            raise
            
    except Exception as e:
        print(f"âŒ Error durante el entrenamiento: {e}")
        print("ğŸ’¡ Verifica la configuraciÃ³n y los datos")
        raise

"""## ğŸ“Š MÃ©tricas de ClasificaciÃ³n Detalladas

"""

# ============================================================
# FUNCIONES PARA MÃ‰TRICAS DE CLASIFICACIÃ“N DETALLADAS
# ============================================================

import numpy as np
from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def calculate_detailed_metrics(model, dataset_path, conf_threshold=0.5, iou_threshold=0.5):
    """Calcular mÃ©tricas detalladas de clasificaciÃ³n para cada clase usando Ultralytics YOLOv8."""

    try:
        print("ğŸ”„ Calculando mÃ©tricas detalladas de clasificaciÃ³n...")

        # Realizar validaciÃ³n con el modelo
        results = model.val(data=dataset_path, conf=conf_threshold, iou=iou_threshold, verbose=False)

        class_names = yolo_config.classes
        class_metrics = {}

        # MÃ©tricas globales preferidas desde results.box
        try:
            mp = float(getattr(results.box, 'mp', 0.0))          # mean precision
            mr = float(getattr(results.box, 'mr', 0.0))          # mean recall
            map50 = float(getattr(results.box, 'map50', 0.0))    # mAP@0.5
            map5095 = float(getattr(results.box, 'map', 0.0))    # mAP@0.5:0.95
        except Exception:
            metrics = getattr(results, 'results_dict', {}) or {}
            mp = metrics.get('metrics/precision(B)', 0.0)
            mr = metrics.get('metrics/recall(B)', 0.0)
            map50 = metrics.get('metrics/mAP50(B)', 0.0)
            map5095 = metrics.get('metrics/mAP50-95(B)', 0.0)

        general_metrics = {
            'mAP50': map50,
            'mAP50-95': map5095,
            'precision': mp,
            'recall': mr,
            'f1': 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0
        }

        # AP por clase si disponible
        per_class_ap = getattr(results.box, 'maps', None)
        if per_class_ap is not None:
            for i, name in enumerate(class_names):
                ap50_95 = float(per_class_ap[i]) if i < len(per_class_ap) else 0.0
                class_metrics[name] = {
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'ap50_95': ap50_95,
                    'class_id': i
                }

        # Completar precision/recall/F1 por clase desde la matriz de confusiÃ³n si existe
        try:
            cm_obj = getattr(results, 'confusion_matrix', None)
            cm_matrix = None
            if cm_obj is not None and hasattr(cm_obj, 'matrix'):
                cm_matrix = cm_obj.matrix
            elif hasattr(results, 'confusion_matrix'):
                cm_matrix = results.confusion_matrix

            if cm_matrix is not None:
                for i, name in enumerate(class_names):
                    if i < cm_matrix.shape[0] and i < cm_matrix.shape[1]:
                        tp = float(cm_matrix[i, i])
                        fp = float(cm_matrix[:, i].sum() - tp)
                        fn = float(cm_matrix[i, :].sum() - tp)
                        prec_i = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                        rec_i = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                        f1_i = 2 * prec_i * rec_i / (prec_i + rec_i) if (prec_i + rec_i) > 0 else 0.0
                        class_metrics.setdefault(name, {'class_id': i})
                        class_metrics[name].update({'precision': prec_i, 'recall': rec_i, 'f1': f1_i})
        except Exception as cm_err:
            print(f"âš ï¸ Error usando matriz de confusiÃ³n: {cm_err}")

        return class_metrics, general_metrics, results

    except Exception as e:
        print(f"âŒ Error calculando mÃ©tricas detalladas: {e}")
        import traceback; traceback.print_exc()
        return {}, {}, None

def create_classification_report(class_metrics, general_metrics, save_path=None):
    """Crear reporte detallado de clasificaciÃ³n"""

    print("\n" + "="*80)
    print("ğŸ“Š REPORTE DETALLADO DE MÃ‰TRICAS DE CLASIFICACIÃ“N")
    print("="*80)

    # MÃ©tricas generales
    print("\nğŸ¯ MÃ‰TRICAS GENERALES:")
    print(f"  mAP@0.5:     {general_metrics.get('mAP50', 0.0):.4f}")
    print(f"  mAP@0.5:0.95: {general_metrics.get('mAP50-95', 0.0):.4f}")
    print(f"  Precision:   {general_metrics.get('precision', 0.0):.4f}")
    print(f"  Recall:      {general_metrics.get('recall', 0.0):.4f}")
    print(f"  F1-Score:    {general_metrics.get('f1', 0.0):.4f}")

    # MÃ©tricas por clase
    print("\nğŸ“‹ MÃ‰TRICAS POR CLASE:")
    print("-" * 60)
    print(f"{'Clase':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}")
    print("-" * 60)

    for class_name, metrics in class_metrics.items():
        print(f"{class_name:<15} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1']:<10.4f}")

    # Guardar reporte si se especifica ruta
    if save_path:
        with open(save_path, 'w') as f:
            f.write("REPORTE DETALLADO DE MÃ‰TRICAS DE CLASIFICACIÃ“N\n")
            f.write("="*50 + "\n\n")

            f.write("MÃ‰TRICAS GENERALES:\n")
            f.write(f"mAP@0.5:     {general_metrics.get('mAP50', 0.0):.4f}\n")
            f.write(f"mAP@0.5:0.95: {general_metrics.get('mAP50-95', 0.0):.4f}\n")
            f.write(f"Precision:   {general_metrics.get('precision', 0.0):.4f}\n")
            f.write(f"Recall:      {general_metrics.get('recall', 0.0):.4f}\n")
            f.write(f"F1-Score:    {general_metrics.get('f1', 0.0):.4f}\n\n")

            f.write("MÃ‰TRICAS POR CLASE:\n")
            f.write("-" * 60 + "\n")
            f.write(f"{'Clase':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\n")
            f.write("-" * 60 + "\n")

            for class_name, metrics in class_metrics.items():
                f.write(f"{class_name:<15} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1']:<10.4f}\n")

        print(f"\nâœ… Reporte guardado en: {save_path}")

    return class_metrics, general_metrics

def plot_class_metrics(class_metrics, save_path=None):
    """Crear grÃ¡ficos de mÃ©tricas por clase"""

    try:
        # Preparar datos para grÃ¡ficos
        classes = list(class_metrics.keys())
        precisions = [class_metrics[cls]['precision'] for cls in classes]
        recalls = [class_metrics[cls]['recall'] for cls in classes]
        f1_scores = [class_metrics[cls]['f1'] for cls in classes]

        # Crear figura con subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('MÃ©tricas de ClasificaciÃ³n por Clase', fontsize=16, fontweight='bold')

        # GrÃ¡fico de barras - Precision
        axes[0, 0].bar(classes, precisions, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('Precision por Clase')
        axes[0, 0].set_ylabel('Precision')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)

        # GrÃ¡fico de barras - Recall
        axes[0, 1].bar(classes, recalls, color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('Recall por Clase')
        axes[0, 1].set_ylabel('Recall')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)

        # GrÃ¡fico de barras - F1-Score
        axes[1, 0].bar(classes, f1_scores, color='lightcoral', alpha=0.7)
        axes[1, 0].set_title('F1-Score por Clase')
        axes[1, 0].set_ylabel('F1-Score')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

        # GrÃ¡fico combinado
        x = np.arange(len(classes))
        width = 0.25

        axes[1, 1].bar(x - width, precisions, width, label='Precision', alpha=0.7)
        axes[1, 1].bar(x, recalls, width, label='Recall', alpha=0.7)
        axes[1, 1].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.7)

        axes[1, 1].set_title('ComparaciÃ³n de MÃ©tricas por Clase')
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(classes, rotation=45)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        # Guardar grÃ¡fico si se especifica ruta
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… GrÃ¡fico de mÃ©tricas guardado en: {save_path}")

        plt.show()

    except Exception as e:
        print(f"âŒ Error creando grÃ¡ficos de mÃ©tricas: {e}")

def create_metrics_summary_table(class_metrics, general_metrics):
    """Crear tabla resumen de mÃ©tricas"""

    print("\n" + "="*80)
    print("ğŸ“Š TABLA RESUMEN DE MÃ‰TRICAS")
    print("="*80)

    # Crear DataFrame para mejor visualizaciÃ³n
    import pandas as pd

    # Datos por clase
    class_data = []
    for class_name, metrics in class_metrics.items():
        class_data.append({
            'Clase': class_name,
            'Precision': f"{metrics['precision']:.4f}",
            'Recall': f"{metrics['recall']:.4f}",
            'F1-Score': f"{metrics['f1']:.4f}"
        })

    # Agregar mÃ©tricas generales
    class_data.append({
        'Clase': 'PROMEDIO',
        'Precision': f"{general_metrics.get('precision', 0.0):.4f}",
        'Recall': f"{general_metrics.get('recall', 0.0):.4f}",
        'F1-Score': f"{general_metrics.get('f1', 0.0):.4f}"
    })

    df = pd.DataFrame(class_data)
    print(df.to_string(index=False))

    return df

def save_metrics_to_csv(class_metrics, general_metrics, save_path):
    """Guardar mÃ©tricas en archivo CSV"""

    try:
        import pandas as pd

        # Preparar datos
        data = []
        for class_name, metrics in class_metrics.items():
            data.append({
                'Clase': class_name,
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1'],
                'Tipo': 'Clase_Individual'
            })

        # Agregar mÃ©tricas generales
        data.append({
            'Clase': 'GENERAL',
            'Precision': general_metrics.get('precision', 0.0),
            'Recall': general_metrics.get('recall', 0.0),
            'F1-Score': general_metrics.get('f1', 0.0),
            'Tipo': 'Promedio_General'
        })

        # Crear DataFrame y guardar
        df = pd.DataFrame(data)
        df.to_csv(save_path, index=False)
        print(f"âœ… MÃ©tricas guardadas en CSV: {save_path}")

        return df

    except Exception as e:
        print(f"âŒ Error guardando mÃ©tricas en CSV: {e}")
        return None

def calculate_metrics_with_thresholds(model, dataset_path, thresholds=[0.3, 0.5, 0.7, 0.9]):
    """Calcular mÃ©tricas con diferentes umbrales de confianza"""

    print("\n" + "="*80)
    print("ğŸ“Š ANÃLISIS DE MÃ‰TRICAS CON DIFERENTES UMBRALES DE CONFIANZA")
    print("="*80)

    results_summary = []

    for threshold in thresholds:
        print(f"\nğŸ”„ Calculando mÃ©tricas con threshold = {threshold}")

        class_metrics, general_metrics, _ = calculate_robust_metrics(
            model, dataset_path, conf_threshold=threshold, iou_threshold=0.5
        )

        if class_metrics and general_metrics:
            results_summary.append({
                'threshold': threshold,
                'mAP50': general_metrics.get('mAP50', 0.0),
                'precision': general_metrics.get('precision', 0.0),
                'recall': general_metrics.get('recall', 0.0),
                'f1': general_metrics.get('f1', 0.0)
            })

            print(f"  mAP@0.5: {general_metrics.get('mAP50', 0.0):.4f}")
            print(f"  Precision: {general_metrics.get('precision', 0.0):.4f}")
            print(f"  Recall: {general_metrics.get('recall', 0.0):.4f}")
            print(f"  F1-Score: {general_metrics.get('f1', 0.0):.4f}")

    # Crear grÃ¡fico de mÃ©tricas vs threshold
    if results_summary:
        import pandas as pd
        df_thresholds = pd.DataFrame(results_summary)

        plt.figure(figsize=(12, 8))

        plt.subplot(2, 2, 1)
        plt.plot(df_thresholds['threshold'], df_thresholds['mAP50'], 'o-', linewidth=2, markersize=8)
        plt.title('mAP@0.5 vs Threshold')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('mAP@0.5')
        plt.grid(True, alpha=0.3)

        plt.subplot(2, 2, 2)
        plt.plot(df_thresholds['threshold'], df_thresholds['precision'], 'o-', linewidth=2, markersize=8, color='green')
        plt.title('Precision vs Threshold')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('Precision')
        plt.grid(True, alpha=0.3)

        plt.subplot(2, 2, 3)
        plt.plot(df_thresholds['threshold'], df_thresholds['recall'], 'o-', linewidth=2, markersize=8, color='red')
        plt.title('Recall vs Threshold')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('Recall')
        plt.grid(True, alpha=0.3)

        plt.subplot(2, 2, 4)
        plt.plot(df_thresholds['threshold'], df_thresholds['f1'], 'o-', linewidth=2, markersize=8, color='purple')
        plt.title('F1-Score vs Threshold')
        plt.xlabel('Confidence Threshold')
        plt.ylabel('F1-Score')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # Guardar grÃ¡fico
        threshold_plot_path = f"{yolo_config.project}/{yolo_config.name}/metrics_vs_threshold.png"
        plt.savefig(threshold_plot_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… GrÃ¡fico de mÃ©tricas vs threshold guardado en: {threshold_plot_path}")
        plt.show()

        # Guardar datos en CSV
        threshold_csv_path = f"{yolo_config.project}/{yolo_config.name}/metrics_vs_threshold.csv"
        df_thresholds.to_csv(threshold_csv_path, index=False)
        print(f"âœ… Datos de mÃ©tricas vs threshold guardados en: {threshold_csv_path}")

        return df_thresholds

    return None

def create_comprehensive_metrics_report():
    """Crear reporte comprensivo de todas las mÃ©tricas"""

    print("\n" + "="*80)
    print("ğŸ“‹ CREANDO REPORTE COMPRENSIVO DE MÃ‰TRICAS")
    print("="*80)

    try:
        # Cargar modelo
        best_model_path = f"{yolo_config.project}/{yolo_config.name}/weights/best.pt"
        if not os.path.exists(best_model_path):
            print("âŒ No se encontrÃ³ el modelo entrenado")
            return

        model = YOLO(best_model_path)

        # 1. MÃ©tricas con threshold por defecto
        print("\n1ï¸âƒ£ Calculando mÃ©tricas con threshold por defecto (0.5)...")
        class_metrics, general_metrics, _ = calculate_robust_metrics(
            model, '/content/yolo_data/dataset.yaml', conf_threshold=0.5
        )

        if class_metrics and general_metrics:
            # Crear reporte detallado
            report_path = f"{yolo_config.project}/{yolo_config.name}/comprehensive_metrics_report.txt"
            create_classification_report(class_metrics, general_metrics, save_path=report_path)

            # Crear grÃ¡ficos
            plot_path = f"{yolo_config.project}/{yolo_config.name}/comprehensive_class_metrics.png"
            plot_class_metrics(class_metrics, save_path=plot_path)

            # 2. AnÃ¡lisis con diferentes thresholds
            print("\n2ï¸âƒ£ Analizando mÃ©tricas con diferentes thresholds...")
            df_thresholds = calculate_metrics_with_thresholds(model, '/content/yolo_data/dataset.yaml')

            # 3. AnÃ¡lisis de rendimiento por clase
            print("\n3ï¸âƒ£ Analizando rendimiento por clase...")
            analyze_class_performance(class_metrics, general_metrics)

            print(f"\nâœ… Reporte comprensivo completado:")
            print(f"  ğŸ“„ Reporte detallado: {report_path}")
            print(f"  ğŸ“Š GrÃ¡ficos por clase: {plot_path}")
            if df_thresholds is not None:
                print(f"  ğŸ“ˆ AnÃ¡lisis de thresholds: {yolo_config.project}/{yolo_config.name}/metrics_vs_threshold.png")

        else:
            print("âŒ No se pudieron calcular las mÃ©tricas")

    except Exception as e:
        print(f"âŒ Error creando reporte comprensivo: {e}")

def calculate_metrics_from_predictions(model, dataset_path, conf_threshold=0.5, iou_threshold=0.5):
    """Calcular mÃ©tricas de forma mÃ¡s robusta usando predicciones directas"""
    
    try:
        print("ğŸ”„ Calculando mÃ©tricas desde predicciones directas...")
        
        # CORREGIDO: Cargar el dataset de validaciÃ³n de forma segura
        try:
            from ultralytics.data import YOLODataset
            from ultralytics.utils.metrics import ConfusionMatrix
            
            # Crear dataset de validaciÃ³n con configuraciÃ³n explÃ­cita
            val_dataset = YOLODataset(
                data=dataset_path, 
                imgsz=512, 
                task='detect',
                augment=False,
                cache=False
            )
        except Exception as dataset_error:
            print(f"âŒ Error creando dataset: {dataset_error}")
            return {}, {}, None
        
        # Listas para almacenar predicciones y ground truth
        all_predictions = []
        all_targets = []
        
        # Obtener nombres de clases
        class_names = yolo_config.classes
        
        # Procesar cada imagen del dataset de validaciÃ³n
        for i in range(len(val_dataset)):
            # Obtener imagen y anotaciones
            img, target = val_dataset[i]
            
            # Realizar predicciÃ³n
            results = model.predict(img, conf=conf_threshold, iou=iou_threshold, verbose=False)
            
            # Extraer predicciones
            if len(results) > 0 and results[0].boxes is not None:
                boxes = results[0].boxes
                pred_classes = boxes.cls.cpu().numpy() if boxes.cls is not None else []
                pred_conf = boxes.conf.cpu().numpy() if boxes.conf is not None else []
                
                # Filtrar por confianza
                valid_indices = pred_conf >= conf_threshold
                pred_classes = pred_classes[valid_indices]
                
                all_predictions.extend(pred_classes.tolist())
            else:
                all_predictions.extend([])
            
            # Extraer ground truth
            if target is not None and len(target) > 0:
                gt_classes = target[:, 0].cpu().numpy() if hasattr(target, 'cpu') else target[:, 0]
                all_targets.extend(gt_classes.tolist())
            else:
                all_targets.extend([])
        
        # Calcular mÃ©tricas usando sklearn
        from sklearn.metrics import precision_recall_fscore_support, classification_report
        
        # Convertir a arrays numpy
        y_true = np.array(all_targets)
        y_pred = np.array(all_predictions)
        
        # Asegurar que ambos arrays tengan la misma longitud
        min_len = min(len(y_true), len(y_pred))
        y_true = y_true[:min_len]
        y_pred = y_pred[:min_len]
        
        # Calcular mÃ©tricas por clase
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )
        
        # Crear diccionario de mÃ©tricas por clase
        class_metrics = {}
        for i, class_name in enumerate(class_names):
            if i < len(precision):
                class_metrics[class_name] = {
                    'precision': float(precision[i]),
                    'recall': float(recall[i]),
                    'f1': float(f1[i]),
                    'support': int(support[i]) if i < len(support) else 0,
                    'class_id': i
                }
            else:
                class_metrics[class_name] = {
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'support': 0,
                    'class_id': i
                }
        
        # Calcular mÃ©tricas generales
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            y_true, y_pred, average='macro', zero_division=0
        )
        
        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
            y_true, y_pred, average='micro', zero_division=0
        )
        
        general_metrics = {
            'mAP50': 0.0,  # No calculamos mAP aquÃ­
            'mAP50-95': 0.0,
            'precision': float(precision_macro),
            'recall': float(recall_macro),
            'f1': float(f1_macro),
            'precision_micro': float(precision_micro),
            'recall_micro': float(recall_micro),
            'f1_micro': float(f1_micro)
        }
        
        print(f"âœ… MÃ©tricas calculadas desde {len(y_true)} predicciones")
        print(f"ğŸ“Š Clases detectadas: {len(set(y_pred))}")
        print(f"ğŸ“Š Clases en ground truth: {len(set(y_true))}")
        
        return class_metrics, general_metrics, None
        
    except Exception as e:
        print(f"âŒ Error calculando mÃ©tricas desde predicciones: {e}")
        import traceback
        traceback.print_exc()
        return {}, {}, None

def calculate_robust_metrics(model, dataset_path, conf_threshold=0.5, iou_threshold=0.5):
    """FunciÃ³n principal que delega al cÃ¡lculo detallado robusto."""
    print("ğŸ”„ Iniciando cÃ¡lculo robusto de mÃ©tricas...")
    return calculate_detailed_metrics(model, dataset_path, conf_threshold, iou_threshold)

print("âœ… Funciones de mÃ©tricas de clasificaciÃ³n cargadas")

"""## ğŸ“Š AnÃ¡lisis Detallado de MÃ©tricas de ClasificaciÃ³n"""

# ============================================================
# ANÃLISIS DETALLADO DE MÃ‰TRICAS (EJECUTAR DESPUÃ‰S DEL ENTRENAMIENTO)
# ============================================================

def analyze_trained_model_metrics():
    """Analizar mÃ©tricas del modelo entrenado"""

    try:
        print("ğŸ”„ Analizando mÃ©tricas del modelo entrenado...")

        # Cargar el mejor modelo entrenado
        best_model_path = f"{yolo_config.project}/{yolo_config.name}/weights/best.pt"
        if not os.path.exists(best_model_path):
            print("âŒ No se encontrÃ³ el modelo entrenado")
            return None, None, None

        model = YOLO(best_model_path)

        # Calcular mÃ©tricas detalladas usando la funciÃ³n robusta
        class_metrics, general_metrics, validation_results = calculate_robust_metrics(
            model,
            '/content/yolo_data/dataset.yaml',
            conf_threshold=0.5,
            iou_threshold=0.5
        )

        if class_metrics and general_metrics:
            print("âœ… MÃ©tricas calculadas exitosamente")

            # Crear reporte detallado
            report_path = f"{yolo_config.project}/{yolo_config.name}/final_classification_report.txt"
            create_classification_report(class_metrics, general_metrics, save_path=report_path)

            # Crear tabla resumen
            metrics_df = create_metrics_summary_table(class_metrics, general_metrics)

            # Crear grÃ¡ficos de mÃ©tricas
            plot_path = f"{yolo_config.project}/{yolo_config.name}/final_class_metrics_plot.png"
            plot_class_metrics(class_metrics, save_path=plot_path)

            # Guardar mÃ©tricas en CSV
            csv_path = f"{yolo_config.project}/{yolo_config.name}/final_detailed_metrics.csv"
            save_metrics_to_csv(class_metrics, general_metrics, csv_path)

            # AnÃ¡lisis adicional
            analyze_class_performance(class_metrics, general_metrics)

            return class_metrics, general_metrics, model

        else:
            print("âŒ No se pudieron calcular las mÃ©tricas")
            return None, None, None

    except Exception as e:
        print(f"âŒ Error analizando mÃ©tricas: {e}")
        return None, None, None

def analyze_class_performance(class_metrics, general_metrics):
    """Analizar rendimiento por clase"""

    print("\n" + "="*80)
    print("ğŸ” ANÃLISIS DE RENDIMIENTO POR CLASE")
    print("="*80)

    # Encontrar la mejor y peor clase
    best_class = max(class_metrics.items(), key=lambda x: x[1]['f1'])
    worst_class = min(class_metrics.items(), key=lambda x: x[1]['f1'])

    print(f"\nğŸ† MEJOR CLASE: {best_class[0]}")
    print(f"  F1-Score: {best_class[1]['f1']:.4f}")
    print(f"  Precision: {best_class[1]['precision']:.4f}")
    print(f"  Recall: {best_class[1]['recall']:.4f}")

    print(f"\nâš ï¸ PEOR CLASE: {worst_class[0]}")
    print(f"  F1-Score: {worst_class[1]['f1']:.4f}")
    print(f"  Precision: {worst_class[1]['precision']:.4f}")
    print(f"  Recall: {worst_class[1]['recall']:.4f}")

    # AnÃ¡lisis de balance
    f1_scores = [metrics['f1'] for metrics in class_metrics.values()]
    f1_std = np.std(f1_scores)
    f1_mean = np.mean(f1_scores)

    print(f"\nğŸ“Š ANÃLISIS DE BALANCE:")
    print(f"  F1-Score promedio: {f1_mean:.4f}")
    print(f"  DesviaciÃ³n estÃ¡ndar: {f1_std:.4f}")
    print(f"  Coeficiente de variaciÃ³n: {(f1_std/f1_mean)*100:.2f}%")

    if f1_std < 0.1:
        print("  âœ… Modelo balanceado (baja variaciÃ³n entre clases)")
    elif f1_std < 0.2:
        print("  âš ï¸ Modelo moderadamente balanceado")
    else:
        print("  âŒ Modelo desbalanceado (alta variaciÃ³n entre clases)")

    # Recomendaciones
    print(f"\nğŸ’¡ RECOMENDACIONES:")
    if worst_class[1]['recall'] < 0.5:
        print(f"  - Considerar aumentar datos de entrenamiento para '{worst_class[0]}'")
    if worst_class[1]['precision'] < 0.5:
        print(f"  - Revisar anotaciones de '{worst_class[0]}' para mejorar precisiÃ³n")
    if f1_std > 0.2:
        print("  - Considerar tÃ©cnicas de balanceo de clases (focal loss, class weights)")

# Ejecutar anÃ¡lisis si el modelo estÃ¡ entrenado
if train_json and val_json:
    print("ğŸ”„ Ejecutando anÃ¡lisis de mÃ©tricas...")
    class_metrics, general_metrics, trained_model = analyze_trained_model_metrics()
else:
    print("âš ï¸ Ejecuta el entrenamiento primero para analizar mÃ©tricas")

"""## ğŸ“Š VisualizaciÃ³n de Resultados
"""

# Cargar el mejor modelo entrenado (si existe)
best_model_path = f"{yolo_config.project}/{yolo_config.name}/weights/best.pt"
if os.path.exists(best_model_path):
    model = YOLO(best_model_path)
    print("âœ… Modelo entrenado cargado para visualizaciÃ³n")
else:
    print("âš ï¸ No hay modelo entrenado disponible para visualizaciÃ³n")

# Visualizar curvas de entrenamiento
results_dir = f"{yolo_config.project}/{yolo_config.name}"
if os.path.exists(f"{results_dir}/results.png"):
    display(IPImage(f"{results_dir}/results.png"))

# Mostrar mÃ©tricas finales
if os.path.exists(f"{results_dir}/results.csv"):
    results_df = pd.read_csv(f"{results_dir}/results.csv")
    print("ğŸ“Š MÃ©tricas de entrenamiento:")
    print(results_df.tail(10))

# Mostrar matriz de confusiÃ³n
if os.path.exists(f"{results_dir}/confusion_matrix.png"):
    print("\nğŸ“Š Matriz de ConfusiÃ³n:")
    display(IPImage(f"{results_dir}/confusion_matrix.png"))

"""## ğŸ“Š MÃ©tricas de ClasificaciÃ³n Detalladas (Precision, Recall, F1-Score)"""

# ============================================================
# CALCULAR Y MOSTRAR MÃ‰TRICAS DE CLASIFICACIÃ“N DETALLADAS
# ============================================================

def show_detailed_classification_metrics():
    """Calcular y mostrar mÃ©tricas de clasificaciÃ³n detalladas"""
    
    try:
        # Verificar que el modelo entrenado existe
        best_model_path = f"{yolo_config.project}/{yolo_config.name}/weights/best.pt"
        if not os.path.exists(best_model_path):
            print("âš ï¸ No se encontrÃ³ el modelo entrenado. Ejecuta el entrenamiento primero.")
            return None, None
        
        # Verificar que el dataset.yaml existe
        dataset_yaml = '/content/yolo_data/dataset.yaml'
        if not os.path.exists(dataset_yaml):
            print("âš ï¸ No se encontrÃ³ el archivo dataset.yaml. Verifica que la conversiÃ³n de datos se haya completado.")
            return None, None
        
        print("\n" + "="*80)
        print("ğŸ“Š CALCULANDO MÃ‰TRICAS DE CLASIFICACIÃ“N DETALLADAS")
        print("="*80)
        
        # Cargar el modelo entrenado
        model = YOLO(best_model_path)
        print("âœ… Modelo cargado correctamente")
        
        # Calcular mÃ©tricas detalladas
        print("\nğŸ”„ Ejecutando validaciÃ³n y cÃ¡lculo de mÃ©tricas...")
        class_metrics, general_metrics, validation_results = calculate_detailed_metrics(
            model,
            dataset_yaml,
            conf_threshold=0.5,
            iou_threshold=0.5
        )
        
        if not class_metrics or not general_metrics:
            print("âŒ No se pudieron calcular las mÃ©tricas. Verifica que el modelo estÃ© entrenado y los datos sean vÃ¡lidos.")
            return None, None
        
        # Mostrar mÃ©tricas generales
        print("\n" + "="*80)
        print("ğŸ¯ MÃ‰TRICAS GENERALES DEL MODELO")
        print("="*80)
        print(f"  mAP@0.5:     {general_metrics.get('mAP50', 0.0):.4f}")
        print(f"  mAP@0.5:0.95: {general_metrics.get('mAP50-95', 0.0):.4f}")
        print(f"  Precision:   {general_metrics.get('precision', 0.0):.4f}")
        print(f"  Recall:      {general_metrics.get('recall', 0.0):.4f}")
        print(f"  F1-Score:    {general_metrics.get('f1', 0.0):.4f}")
        
        # Mostrar mÃ©tricas por clase
        print("\n" + "="*80)
        print("ğŸ“‹ MÃ‰TRICAS POR CLASE")
        print("="*80)
        print(f"{'Clase':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'AP@0.5:0.95':<12}")
        print("-" * 80)
        
        for class_name in sorted(class_metrics.keys()):
            metrics = class_metrics[class_name]
            precision = metrics.get('precision', 0.0)
            recall = metrics.get('recall', 0.0)
            f1 = metrics.get('f1', 0.0)
            ap50_95 = metrics.get('ap50_95', 0.0)
            
            print(f"{class_name:<20} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {ap50_95:<12.4f}")
        
        # Crear y guardar reporte detallado
        report_path = f"{results_dir}/detailed_classification_report.txt"
        create_classification_report(class_metrics, general_metrics, save_path=report_path)
        
        # Crear tabla resumen
        print("\n" + "="*80)
        print("ğŸ“Š TABLA RESUMEN DE MÃ‰TRICAS")
        print("="*80)
        metrics_df = create_metrics_summary_table(class_metrics, general_metrics)
        
        # Guardar mÃ©tricas en CSV
        csv_path = f"{results_dir}/detailed_classification_metrics.csv"
        save_metrics_to_csv(class_metrics, general_metrics, csv_path)
        
        # Crear grÃ¡ficos de mÃ©tricas por clase
        plot_path = f"{results_dir}/class_metrics_visualization.png"
        plot_class_metrics(class_metrics, save_path=plot_path)
        
        # AnÃ¡lisis de rendimiento por clase
        analyze_class_performance(class_metrics, general_metrics)
        
        print("\n" + "="*80)
        print("âœ… MÃ‰TRICAS DE CLASIFICACIÃ“N CALCULADAS Y GUARDADAS")
        print("="*80)
        print(f"ğŸ“„ Reporte detallado: {report_path}")
        print(f"ğŸ“Š Tabla CSV: {csv_path}")
        print(f"ğŸ“ˆ GrÃ¡ficos: {plot_path}")
        
        return class_metrics, general_metrics
        
    except Exception as e:
        print(f"âŒ Error calculando mÃ©tricas detalladas: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# Ejecutar cÃ¡lculo de mÃ©tricas detalladas
best_model_path_check = f"{yolo_config.project}/{yolo_config.name}/weights/best.pt"
if os.path.exists(best_model_path_check):
    classification_metrics, general_metrics_summary = show_detailed_classification_metrics()
else:
    print("âš ï¸ No se puede calcular mÃ©tricas: modelo entrenado no encontrado")

"""## ğŸ” Inferencia y Pruebas"""

# Realizar inferencia en imÃ¡genes de prueba
if test_json and test_json.exists():
    # Determinar directorio de imÃ¡genes de test segÃºn el tipo de datos
    if data_type == "standard":
        test_images_dir = TEST_IMG_DIR
    elif data_type == "groundtruth":
        test_images_dir = TEST_IMG_DIR_ALT
    else:
        # Estructura legacy
        test_images_dir = test_json.parent.parent / "test"

    if test_images_dir and test_images_dir.exists():
        test_images = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        # Seleccionar algunas imÃ¡genes para prueba
        sample_images = test_images[:5]  # Primeras 5 imÃ¡genes

        print(f"ğŸ” Realizando inferencia en {len(sample_images)} imÃ¡genes de prueba...")

        for img_name in sample_images:
            img_path = os.path.join(test_images_dir, img_name)

            # Realizar predicciÃ³n
            results = model(img_path, conf=0.5)

            # Mostrar resultado
            for r in results:
                # Guardar imagen con predicciones
                output_path = f"/content/test_results_{img_name}"
                r.save(output_path)

                # Mostrar imagen
                display(IPImage(output_path))

                # Mostrar estadÃ­sticas
                print(f"ğŸ“Š {img_name}: {len(r.boxes)} objetos detectados")
                if len(r.boxes) > 0:
                    for box in r.boxes:
                        class_id = int(box.cls[0])
                        confidence = float(box.conf[0])
                        class_name = yolo_config.classes[class_id]
                        print(f"  - {class_name}: {confidence:.2f}")
                print()
    else:
        print("âš ï¸  No se encontrÃ³ directorio de imÃ¡genes de test")
else:
    print("âš ï¸  No hay datos de test disponibles para inferencia")

"""## ğŸ’¾ Guardar y Exportar Modelo"""

# ============================================================
# EXPORTAR MODELO A ONNX CON VERIFICACIÃ“N DE DEPENDENCIAS
# ============================================================

def export_model_to_onnx(model, output_size=512, max_retries=2):
    """Exportar modelo a ONNX con verificaciÃ³n de dependencias y manejo de errores"""
    
    try:
        # Verificar que el modelo existe
        if model is None:
            print("âŒ No hay modelo disponible para exportar")
            return None
        
        # Verificar/instalar dependencias necesarias
        print("ğŸ” Verificando dependencias para exportaciÃ³n ONNX...")
        try:
            import subprocess
            import sys
            
            required_packages = ['onnx>=1.12.0', 'onnxslim>=0.1.71', 'onnxruntime-gpu']
            missing_packages = []
            
            for package in required_packages:
                package_name = package.split('>=')[0].split('>')[0]
                try:
                    __import__(package_name.replace('-', '_'))
                except ImportError:
                    missing_packages.append(package)
            
            if missing_packages:
                print(f"âš ï¸ Faltan dependencias: {missing_packages}")
                print("ğŸ”„ Instalando dependencias necesarias...")
                for package in missing_packages:
                    try:
                        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"], 
                                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    except Exception as e:
                        print(f"âš ï¸ Error instalando {package}: {e}")
                
                print("âœ… Dependencias instaladas. Continuando con exportaciÃ³n...")
            else:
                print("âœ… Todas las dependencias estÃ¡n instaladas")
                
        except Exception as dep_error:
            print(f"âš ï¸ Error verificando dependencias: {dep_error}")
            print("ğŸ”„ Intentando exportaciÃ³n de todas formas...")
        
        # Intentar exportar el modelo
        print(f"\nğŸ”„ Exportando modelo a ONNX (tamaÃ±o: {output_size}x{output_size})...")
        
        onnx_path = None
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ Reintento {attempt + 1} de {max_retries}...")
                
                # Exportar modelo
                onnx_path = model.export(format='onnx', imgsz=output_size, simplify=True)
                
                # Verificar que el archivo se creÃ³
                if onnx_path and os.path.exists(onnx_path):
                    file_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)
                    print(f"âœ… Modelo exportado exitosamente a: {onnx_path}")
                    print(f"ğŸ“Š TamaÃ±o del archivo: {file_size_mb:.2f} MB")
                    return onnx_path
                else:
                    raise FileNotFoundError("El archivo ONNX no se creÃ³ correctamente")
                    
            except Exception as export_error:
                error_msg = str(export_error)
                print(f"âš ï¸ Error en intento {attempt + 1}: {error_msg}")
                
                # Si es un error de dependencias y no es el Ãºltimo intento, intentar instalar
                if "requirements" in error_msg.lower() or "not found" in error_msg.lower():
                    if attempt < max_retries - 1:
                        print("ğŸ”„ Reinstalando dependencias y reintentando...")
                        try:
                            subprocess.check_call([sys.executable, "-m", "pip", "install", 
                                                 "onnx>=1.12.0", "onnxslim>=0.1.71", 
                                                 "onnxruntime-gpu", "-q", "--upgrade"],
                                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                            print("âœ… Dependencias reinstaladas")
                        except:
                            pass
                    continue
                
                # Si es el Ãºltimo intento, lanzar el error
                if attempt == max_retries - 1:
                    raise
        
        return None
        
    except Exception as e:
        print(f"âŒ Error exportando modelo a ONNX: {e}")
        print("ğŸ’¡ Sugerencias:")
        print("  1. Reinicia el runtime de Colab (Runtime > Restart runtime)")
        print("  2. Verifica que todas las dependencias estÃ©n instaladas:")
        print("     !pip install onnx>=1.12.0 onnxslim>=0.1.71 onnxruntime-gpu")
        print("  3. El modelo .pt seguirÃ¡ disponible aunque no se exporte a ONNX")
        import traceback
        traceback.print_exc()
        return None

# Exportar modelo a ONNX
onnx_path = None
if 'model' in locals() and model is not None:
    onnx_path = export_model_to_onnx(model, output_size=yolo_config.image_size)
    
    if onnx_path is None:
        print("âš ï¸ La exportaciÃ³n a ONNX fallÃ³, pero el modelo .pt sigue disponible")
        print(f"ğŸ“ Modelo PyTorch disponible en: {best_model_path}")
else:
    print("âš ï¸ No hay modelo disponible para exportar")

# Copiar resultados a Google Drive
drive_results_dir = f"/content/drive/MyDrive/aerial-wildlife-count/results/yolov8_{yolo_config.name}"
os.makedirs(drive_results_dir, exist_ok=True)

# Copiar archivos importantes
files_to_copy = [
    f"{results_dir}/weights/best.pt",
    f"{results_dir}/weights/last.pt",
    f"{results_dir}/results.png",
    f"{results_dir}/confusion_matrix.png",
    f"{results_dir}/results.csv",
]

# Agregar archivos adicionales si existen
additional_files = [
    f"{results_dir}/detailed_classification_report.txt",
    f"{results_dir}/detailed_classification_metrics.csv",
    f"{results_dir}/class_metrics_visualization.png",
]

for additional_file in additional_files:
    if os.path.exists(additional_file):
        files_to_copy.append(additional_file)

# Agregar ONNX si se exportÃ³ correctamente
if onnx_path and os.path.exists(onnx_path):
    files_to_copy.append(onnx_path)

copied_count = 0
for file_path in files_to_copy:
    if file_path and os.path.exists(file_path):
        try:
            filename = os.path.basename(file_path)
            dest_path = os.path.join(drive_results_dir, filename)
            shutil.copy2(file_path, dest_path)
            print(f"ğŸ“ Copiado: {filename}")
            copied_count += 1
        except Exception as e:
            print(f"âš ï¸ Error copiando {os.path.basename(file_path)}: {e}")

print(f"\nâœ… {copied_count} archivo(s) copiado(s) a Google Drive")

print(f"âœ… Resultados guardados en Google Drive: {drive_results_dir}")

# Mostrar resumen final
print("\nğŸ‰ RESUMEN DEL ENTRENAMIENTO")
print("=" * 50)
print(f"Modelo: {yolo_config.model}")
print(f"Ã‰pocas: {yolo_config.epochs}")
print(f"TamaÃ±o de imagen: {yolo_config.image_size}")
print(f"Batch size: {yolo_config.batch_size}")
print(f"Clases: {yolo_config.classes}")
print(f"Mejor modelo: {best_model_path}")
if onnx_path and os.path.exists(onnx_path):
    print(f"Modelo ONNX: {onnx_path}")
else:
    print("Modelo ONNX: âš ï¸ No exportado (ver errores anteriores)")
print(f"Resultados en Drive: {drive_results_dir}")



