{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Entrenamiento Cascade R-CNN en Google Colab - V1 (Optimizado)\n",
        "\n",
        "Este notebook entrena un modelo Cascade R-CNN con MMDetection en Google Colab para detecci√≥n de vida silvestre a√©rea.\n",
        "\n",
        "## üî• **MEJORAS V1:**\n",
        "- ‚úÖ **Guardado autom√°tico en Drive** cada X √©pocas\n",
        "- ‚úÖ **Recuperaci√≥n de entrenamiento** interrumpido\n",
        "- ‚úÖ **Configuraci√≥n optimizada** para velocidad\n",
        "- ‚úÖ **Monitoreo en tiempo real** del progreso\n",
        "- ‚úÖ **Backup autom√°tico** de checkpoints\n",
        "\n",
        "## üìã Caracter√≠sticas\n",
        "- **Modelo**: Cascade R-CNN con backbone Swin-T o ResNeXt-101\n",
        "- **Framework**: MMDetection (OpenMMLab)\n",
        "- **Detecci√≥n autom√°tica de GPU**\n",
        "- **Configuraci√≥n autom√°tica de datasets**\n",
        "- **Visualizaci√≥n de resultados**\n",
        "- **Exportaci√≥n de modelos**\n",
        "\n",
        "## üéØ Clases de Animales\n",
        "- Buffalo\n",
        "- Elephant  \n",
        "- Kob\n",
        "- Alcelaphinae\n",
        "- Warthog\n",
        "- Waterbuck\n",
        "\n",
        "## ‚öôÔ∏è Configuraci√≥n por Defecto\n",
        "- **Backbone**: Swin-T (recomendado) o ResNeXt-101\n",
        "- **Tama√±o de imagen**: 896x896\n",
        "- **√âpocas**: 48\n",
        "- **Batch size**: 4 (train), 2 (val/test)\n",
        "- **Optimizador**: SGD con momentum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Instalaci√≥n de Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar dependencias principales\n",
        "%pip install -q openmim\n",
        "!mim install -q mmcv-full\n",
        "!mim install -q mmdet\n",
        "%pip install -q pyyaml opencv-python pillow tqdm matplotlib seaborn pandas\n",
        "\n",
        "# Verificar instalaci√≥n\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Importar Librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad96b1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VARIABLES GLOBALES PARA BACKUP AUTOM√ÅTICO\n",
        "# ============================================================\n",
        "\n",
        "# Variables globales para el sistema de backup\n",
        "backup_thread_running = False\n",
        "backup_thread = None\n",
        "\n",
        "print(\"‚úÖ Variables globales de backup inicializadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MMDetection imports\n",
        "from mmengine.config import Config\n",
        "from mmengine.runner import Runner\n",
        "import mmdet\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "from mmdet.utils import register_all_modules\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Configurar matplotlib\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"MMDetection version: {mmdet.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Configuraci√≥n de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURACI√ìN DE RUTAS PRINCIPALES (Unificado con HerdNet)\n",
        "# ============================================================\n",
        "\n",
        "# Definir la ruta base principal (ajustar seg√∫n el entorno: Drive o local)\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/aerial-wildlife-count\")\n",
        "\n",
        "# ============================================================\n",
        "# RUTAS DE IM√ÅGENES Y ANOTACIONES (COCO JSON) - Igual que HerdNet\n",
        "# ============================================================\n",
        "\n",
        "# Rutas de los archivos de anotaciones en formato COCO\n",
        "TRAIN_ANN_FILE = BASE_DIR / \"data\" / \"coco\" / \"train\" / \"train_annotations.json\"\n",
        "VAL_ANN_FILE = BASE_DIR / \"data\" / \"coco\" / \"val\" / \"val_annotations.json\"\n",
        "TEST_ANN_FILE = BASE_DIR / \"data\" / \"coco\" / \"test\" / \"test_annotations.json\"\n",
        "\n",
        "# Rutas de las carpetas de im√°genes correspondientes a cada conjunto\n",
        "TRAIN_IMG_DIR = BASE_DIR / \"data\" / \"images\" / \"train\"\n",
        "VAL_IMG_DIR = BASE_DIR / \"data\" / \"images\" / \"val\"\n",
        "TEST_IMG_DIR = BASE_DIR / \"data\" / \"images\" / \"test\"\n",
        "\n",
        "# ============================================================\n",
        "# RUTAS ALTERNATIVAS (Fallback para compatibilidad)\n",
        "# ============================================================\n",
        "\n",
        "# Rutas alternativas si no existe la estructura est√°ndar\n",
        "TRAIN_ANN_FILE_ALT = BASE_DIR / \"data\" / \"groundtruth\" / \"json\" / \"big_size\" / \"train_big_size_A_B_E_K_WH_WB.json\"\n",
        "VAL_ANN_FILE_ALT = BASE_DIR / \"data\" / \"groundtruth\" / \"json\" / \"big_size\" / \"val_big_size_A_B_E_K_WH_WB.json\"\n",
        "TEST_ANN_FILE_ALT = BASE_DIR / \"data\" / \"groundtruth\" / \"json\" / \"big_size\" / \"test_big_size_A_B_E_K_WH_WB.json\"\n",
        "\n",
        "# Rutas alternativas para im√°genes\n",
        "TRAIN_IMG_DIR_ALT = BASE_DIR / \"data\" / \"train\"\n",
        "VAL_IMG_DIR_ALT = BASE_DIR / \"data\" / \"val\"\n",
        "TEST_IMG_DIR_ALT = BASE_DIR / \"data\" / \"test\"\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURACI√ìN DEL MODELO CASCADE R-CNN\n",
        "# ============================================================\n",
        "\n",
        "class CascadeConfig:\n",
        "    def __init__(self):\n",
        "        # Par√°metros del modelo - OPTIMIZADOS PARA VELOCIDAD\n",
        "        self.backbone = \"swin_t\"  # \"swin_t\" o \"resnext\"\n",
        "        self.imgsz = 896\n",
        "        self.epochs = 48\n",
        "        self.batch_size = 8  # AUMENTADO para mayor velocidad (vs 4 original)\n",
        "        self.val_batch_size = 4  # AUMENTADO para mayor velocidad (vs 2 original)\n",
        "        self.patience = 5  # Early stopping: detener si no mejora en 5 √©pocas\n",
        "        \n",
        "        # Directorio de trabajo\n",
        "        self.work_dir = \"/content/work_dirs/cascade_rcnn_v1\"\n",
        "        \n",
        "        # NUEVAS CONFIGURACIONES V1\n",
        "        self.save_period = 1  # MMDetection guarda cada √©poca\n",
        "        self.drive_backup_period = 5  # Backup en Drive cada 5 √©pocas\n",
        "        self.drive_backup_dir = '/content/drive/MyDrive/aerial-wildlife-count-results/cascade_rcnn_v1'\n",
        "        self.resume_training = True  # Permitir reanudar entrenamiento\n",
        "        self.workers = 8  # AUMENTADO para mayor paralelizaci√≥n\n",
        "        \n",
        "        # Clases del dataset\n",
        "        self.classes = [\n",
        "            \"Buffalo\", \"Elephant\", \"Kob\", \n",
        "            \"Alcelaphinae\", \"Warthog\", \"Waterbuck\"\n",
        "        ]\n",
        "        \n",
        "        # Configuraci√≥n de GPU\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        \n",
        "    def print_config(self):\n",
        "        print(\"üîß Configuraci√≥n Cascade R-CNN V1 (Optimizada):\")\n",
        "        print(f\"  Backbone: {self.backbone}\")\n",
        "        print(f\"  Tama√±o de imagen: {self.imgsz}\")\n",
        "        print(f\"  √âpocas: {self.epochs}\")\n",
        "        print(f\"  Batch size: {self.batch_size} (OPTIMIZADO)\")\n",
        "        print(f\"  Val batch size: {self.val_batch_size} (OPTIMIZADO)\")\n",
        "        print(f\"  Workers: {self.workers} (OPTIMIZADO)\")\n",
        "        print(f\"  Early stopping patience: {self.patience}\")\n",
        "        print(f\"  Dispositivo: {self.device}\")\n",
        "        print(f\"  Directorio de trabajo: {self.work_dir}\")\n",
        "        print(f\"  Save period: {self.save_period} √©pocas\")\n",
        "        print(f\"  Drive backup: {self.drive_backup_period} √©pocas\")\n",
        "        print(f\"  Resume training: {self.resume_training}\")\n",
        "\n",
        "# Crear instancia de configuraci√≥n\n",
        "cascade_config = CascadeConfig()\n",
        "cascade_config.print_config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fe8acf",
      "metadata": {},
      "source": [
        "## üî• Funciones de Backup Autom√°tico en Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f1b575",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FUNCIONES DE BACKUP AUTOM√ÅTICO EN DRIVE - V1 (MMDetection)\n",
        "# ============================================================\n",
        "\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "import threading\n",
        "\n",
        "def create_drive_backup_dir():\n",
        "    \"\"\"Crear directorio de backup en Drive\"\"\"\n",
        "    backup_dir = Path(cascade_config.drive_backup_dir)\n",
        "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"‚úÖ Directorio de backup creado: {backup_dir}\")\n",
        "    return backup_dir\n",
        "\n",
        "def backup_to_drive(epoch=None, force=False):\n",
        "    \"\"\"Hacer backup de checkpoints a Drive (MMDetection)\"\"\"\n",
        "    try:\n",
        "        # Crear directorio de backup\n",
        "        backup_dir = create_drive_backup_dir()\n",
        "        \n",
        "        # Directorio fuente (Colab) - MMDetection usa work_dir\n",
        "        source_dir = Path(cascade_config.work_dir)\n",
        "        \n",
        "        if not source_dir.exists():\n",
        "            print(\"‚ùå No existe directorio de entrenamiento para hacer backup\")\n",
        "            return False\n",
        "        \n",
        "        # Crear subdirectorio con timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        if epoch is not None:\n",
        "            backup_subdir = backup_dir / f\"epoch_{epoch}_{timestamp}\"\n",
        "        else:\n",
        "            backup_subdir = backup_dir / f\"backup_{timestamp}\"\n",
        "        \n",
        "        backup_subdir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Copiar archivos importantes de MMDetection\n",
        "        files_to_backup = [\n",
        "            \"epoch_*.pth\",\n",
        "            \"best_*.pth\", \n",
        "            \"latest.pth\",\n",
        "            \"*.log.json\",\n",
        "            \"*.log\",\n",
        "            \"*.py\",  # Archivos de configuraci√≥n\n",
        "            \"vis_data/\",\n",
        "            \"*.png\",\n",
        "            \"*.jpg\"\n",
        "        ]\n",
        "        \n",
        "        copied_files = 0\n",
        "        for pattern in files_to_backup:\n",
        "            try:\n",
        "                for file_path in source_dir.glob(pattern):\n",
        "                    if file_path.is_file():\n",
        "                        dest_path = backup_subdir / file_path.name\n",
        "                        shutil.copy2(file_path, dest_path)\n",
        "                        copied_files += 1\n",
        "                    elif file_path.is_dir():\n",
        "                        dest_path = backup_subdir / file_path.name\n",
        "                        shutil.copytree(file_path, dest_path, dirs_exist_ok=True)\n",
        "                        copied_files += 1\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error copiando {pattern}: {e}\")\n",
        "        \n",
        "        print(f\"‚úÖ Backup completado: {copied_files} archivos copiados a {backup_subdir}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en backup: {e}\")\n",
        "        return False\n",
        "\n",
        "def auto_backup_thread():\n",
        "    \"\"\"Thread para backup autom√°tico cada X √©pocas (MMDetection)\"\"\"\n",
        "    global backup_thread_running\n",
        "    backup_thread_running = True\n",
        "    \n",
        "    print(\"üîÑ Thread de backup autom√°tico iniciado (MMDetection)\")\n",
        "    \n",
        "    while backup_thread_running:\n",
        "        try:\n",
        "            # Verificar si hay nuevos checkpoints en work_dir\n",
        "            work_dir = Path(cascade_config.work_dir)\n",
        "            if work_dir.exists():\n",
        "                checkpoints = list(work_dir.glob(\"epoch_*.pth\"))\n",
        "                if checkpoints:\n",
        "                    # Obtener el checkpoint m√°s reciente\n",
        "                    latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
        "                    \n",
        "                    # Verificar si necesita backup\n",
        "                    backup_file = Path(f\"{cascade_config.drive_backup_dir}/last_backup_epoch.txt\")\n",
        "                    last_backup_epoch = 0\n",
        "                    \n",
        "                    if backup_file.exists():\n",
        "                        try:\n",
        "                            with open(backup_file, 'r') as f:\n",
        "                                last_backup_epoch = int(f.read().strip())\n",
        "                        except:\n",
        "                            last_backup_epoch = 0\n",
        "                    \n",
        "                    # Extraer √©poca del nombre del archivo (epoch_1.pth, epoch_2.pth, etc.)\n",
        "                    current_epoch = 0\n",
        "                    if \"epoch_\" in latest_checkpoint.name:\n",
        "                        try:\n",
        "                            current_epoch = int(latest_checkpoint.name.split(\"epoch_\")[1].split(\".\")[0])\n",
        "                        except:\n",
        "                            current_epoch = 0\n",
        "                    \n",
        "                    # Hacer backup si ha pasado el per√≠odo configurado\n",
        "                    if current_epoch - last_backup_epoch >= cascade_config.drive_backup_period:\n",
        "                        print(f\"üîÑ Iniciando backup autom√°tico (√©poca {current_epoch})...\")\n",
        "                        if backup_to_drive(epoch=current_epoch):\n",
        "                            # Actualizar archivo de control\n",
        "                            backup_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "                            with open(backup_file, 'w') as f:\n",
        "                                f.write(str(current_epoch))\n",
        "                            print(f\"‚úÖ Backup autom√°tico completado (√©poca {current_epoch})\")\n",
        "            \n",
        "            # Esperar 5 minutos antes de verificar nuevamente\n",
        "            time.sleep(300)  # 5 minutos\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en backup autom√°tico: {e}\")\n",
        "            time.sleep(60)  # Esperar 1 minuto en caso de error\n",
        "    \n",
        "    print(\"üõë Thread de backup autom√°tico detenido\")\n",
        "\n",
        "def start_auto_backup():\n",
        "    \"\"\"Iniciar backup autom√°tico en segundo plano\"\"\"\n",
        "    global backup_thread, backup_thread_running\n",
        "    \n",
        "    # Detener thread anterior si existe\n",
        "    if backup_thread_running:\n",
        "        stop_auto_backup()\n",
        "        time.sleep(2)  # Esperar a que se detenga\n",
        "    \n",
        "    backup_thread_running = False\n",
        "    \n",
        "    # Verificar si ya hay un thread corriendo\n",
        "    if 'backup_thread' in globals() and backup_thread is not None and backup_thread.is_alive():\n",
        "        print(\"‚ö†Ô∏è Backup autom√°tico ya est√° ejecut√°ndose\")\n",
        "        return\n",
        "    \n",
        "    backup_thread = threading.Thread(target=auto_backup_thread, daemon=True)\n",
        "    backup_thread.start()\n",
        "    print(\"üöÄ Backup autom√°tico iniciado en segundo plano\")\n",
        "\n",
        "def stop_auto_backup():\n",
        "    \"\"\"Detener backup autom√°tico\"\"\"\n",
        "    global backup_thread_running\n",
        "    backup_thread_running = False\n",
        "    print(\"üõë Backup autom√°tico detenido\")\n",
        "\n",
        "def resume_training_from_drive():\n",
        "    \"\"\"Reanudar entrenamiento desde el √∫ltimo checkpoint en Drive (MMDetection)\"\"\"\n",
        "    try:\n",
        "        backup_dir = Path(cascade_config.drive_backup_dir)\n",
        "        if not backup_dir.exists():\n",
        "            print(\"‚ùå No hay backups disponibles en Drive\")\n",
        "            return None\n",
        "        \n",
        "        # Buscar el backup m√°s reciente\n",
        "        backup_dirs = [d for d in backup_dir.iterdir() if d.is_dir() and \"epoch_\" in d.name]\n",
        "        if not backup_dirs:\n",
        "            print(\"‚ùå No se encontraron backups con checkpoints\")\n",
        "            return None\n",
        "        \n",
        "        # Ordenar por fecha de modificaci√≥n\n",
        "        latest_backup = max(backup_dirs, key=lambda x: x.stat().st_mtime)\n",
        "        \n",
        "        # Buscar el mejor checkpoint (priorizar best_*.pth, luego latest.pth, luego epoch_*.pth)\n",
        "        checkpoints = []\n",
        "        for pattern in [\"best_*.pth\", \"latest.pth\", \"epoch_*.pth\"]:\n",
        "            checkpoints.extend(list(latest_backup.glob(pattern)))\n",
        "        \n",
        "        if checkpoints:\n",
        "            # Priorizar best checkpoint\n",
        "            best_checkpoint = None\n",
        "            for ckpt in checkpoints:\n",
        "                if \"best\" in ckpt.name:\n",
        "                    best_checkpoint = ckpt\n",
        "                    break\n",
        "            \n",
        "            if not best_checkpoint:\n",
        "                best_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
        "            \n",
        "            print(f\"‚úÖ Checkpoint encontrado para reanudar: {best_checkpoint}\")\n",
        "            return str(best_checkpoint)\n",
        "        \n",
        "        print(\"‚ùå No se encontr√≥ checkpoint v√°lido en backups\")\n",
        "        return None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al buscar checkpoint para reanudar: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Funciones de backup autom√°tico cargadas (MMDetection)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1ac9e9",
      "metadata": {},
      "source": [
        "## üìä Monitoreo en Tiempo Real\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55e3ebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funciones de monitoreo en tiempo real (MMDetection)\n",
        "def monitor_training_progress():\n",
        "    \"\"\"Monitorear el progreso del entrenamiento en tiempo real (MMDetection)\"\"\"\n",
        "    try:\n",
        "        work_dir = Path(cascade_config.work_dir)\n",
        "        \n",
        "        if not work_dir.exists():\n",
        "            print(\"‚ùå Directorio de trabajo no encontrado\")\n",
        "            return\n",
        "        \n",
        "        # Verificar archivos de logs de MMDetection\n",
        "        log_files = list(work_dir.glob(\"*.log.json\"))\n",
        "        if log_files:\n",
        "            latest_log = max(log_files, key=lambda x: x.stat().st_mtime)\n",
        "            print(f\"üìä Log m√°s reciente: {latest_log.name}\")\n",
        "            \n",
        "            # Leer y parsear el log JSON\n",
        "            try:\n",
        "                with open(latest_log, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "                    if lines:\n",
        "                        # Obtener la √∫ltima l√≠nea (m√°s reciente)\n",
        "                        last_line = lines[-1].strip()\n",
        "                        if last_line:\n",
        "                            import json\n",
        "                            log_data = json.loads(last_line)\n",
        "                            \n",
        "                            # Extraer m√©tricas relevantes\n",
        "                            epoch = log_data.get('epoch', 'N/A')\n",
        "                            mode = log_data.get('mode', 'N/A')\n",
        "                            \n",
        "                            if mode == 'val':\n",
        "                                bbox_mAP = log_data.get('bbox_mAP', 'N/A')\n",
        "                                print(f\"üìä Progreso actual:\")\n",
        "                                print(f\"  √âpoca: {epoch}\")\n",
        "                                print(f\"  bbox_mAP: {bbox_mAP}\")\n",
        "                            else:\n",
        "                                loss_cls = log_data.get('loss_cls', 'N/A')\n",
        "                                loss_bbox = log_data.get('loss_bbox', 'N/A')\n",
        "                                print(f\"üìä Progreso actual:\")\n",
        "                                print(f\"  √âpoca: {epoch}\")\n",
        "                                print(f\"  Loss Cls: {loss_cls}\")\n",
        "                                print(f\"  Loss Bbox: {loss_bbox}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error leyendo log: {e}\")\n",
        "        \n",
        "        # Verificar checkpoints\n",
        "        checkpoints = list(work_dir.glob(\"epoch_*.pth\"))\n",
        "        best_checkpoints = list(work_dir.glob(\"best_*.pth\"))\n",
        "        latest_checkpoint = list(work_dir.glob(\"latest.pth\"))\n",
        "        \n",
        "        print(f\"üìÅ Checkpoints disponibles:\")\n",
        "        print(f\"  √âpocas: {len(checkpoints)}\")\n",
        "        print(f\"  Mejores: {len(best_checkpoints)}\")\n",
        "        print(f\"  Latest: {len(latest_checkpoint)}\")\n",
        "        \n",
        "        # Verificar backups en Drive\n",
        "        backup_dir = Path(cascade_config.drive_backup_dir)\n",
        "        if backup_dir.exists():\n",
        "            backups = list(backup_dir.glob(\"epoch_*\"))\n",
        "            print(f\"üíæ Backups en Drive: {len(backups)}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en monitoreo: {e}\")\n",
        "\n",
        "def get_training_status():\n",
        "    \"\"\"Obtener estado actual del entrenamiento (MMDetection)\"\"\"\n",
        "    try:\n",
        "        work_dir = Path(cascade_config.work_dir)\n",
        "        \n",
        "        if not work_dir.exists():\n",
        "            return \"No iniciado\"\n",
        "        \n",
        "        # Verificar si hay logs\n",
        "        log_files = list(work_dir.glob(\"*.log.json\"))\n",
        "        if log_files:\n",
        "            latest_log = max(log_files, key=lambda x: x.stat().st_mtime)\n",
        "            try:\n",
        "                with open(latest_log, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "                    if lines:\n",
        "                        import json\n",
        "                        last_line = lines[-1].strip()\n",
        "                        if last_line:\n",
        "                            log_data = json.loads(last_line)\n",
        "                            current_epoch = log_data.get('epoch', 0)\n",
        "                            total_epochs = cascade_config.epochs\n",
        "                            progress = (current_epoch / total_epochs) * 100\n",
        "                            return f\"En progreso: {current_epoch}/{total_epochs} √©pocas ({progress:.1f}%)\"\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        return \"Iniciando\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def estimate_remaining_time():\n",
        "    \"\"\"Estimar tiempo restante de entrenamiento (MMDetection)\"\"\"\n",
        "    try:\n",
        "        work_dir = Path(cascade_config.work_dir)\n",
        "        log_files = list(work_dir.glob(\"*.log.json\"))\n",
        "        \n",
        "        if not log_files:\n",
        "            return \"No disponible\"\n",
        "        \n",
        "        latest_log = max(log_files, key=lambda x: x.stat().st_mtime)\n",
        "        \n",
        "        try:\n",
        "            with open(latest_log, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            if len(lines) < 2:\n",
        "                return \"Calculando...\"\n",
        "            \n",
        "            # Calcular tiempo promedio por √©poca basado en el n√∫mero de √©pocas\n",
        "            import json\n",
        "            last_line = lines[-1].strip()\n",
        "            if last_line:\n",
        "                log_data = json.loads(last_line)\n",
        "                current_epoch = log_data.get('epoch', 0)\n",
        "                remaining_epochs = cascade_config.epochs - current_epoch\n",
        "                \n",
        "                # Estimaci√≥n simple: asumir 3-8 minutos por √©poca para MMDetection\n",
        "                estimated_minutes = remaining_epochs * 5  # 5 minutos promedio por √©poca\n",
        "                \n",
        "                if estimated_minutes < 60:\n",
        "                    return f\"Tiempo estimado restante: {estimated_minutes:.0f} minutos\"\n",
        "                else:\n",
        "                    hours = estimated_minutes / 60\n",
        "                    return f\"Tiempo estimado restante: {hours:.1f} horas\"\n",
        "        \n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Funci√≥n para mostrar estado completo\n",
        "def show_training_status():\n",
        "    \"\"\"Mostrar estado completo del entrenamiento (MMDetection)\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìä ESTADO DEL ENTRENAMIENTO CASCADE R-CNN V1\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Estado: {get_training_status()}\")\n",
        "    print(f\"Tiempo restante: {estimate_remaining_time()}\")\n",
        "    print()\n",
        "    monitor_training_progress()\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print(\"‚úÖ Funciones de monitoreo cargadas (MMDetection)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4cf30ee",
      "metadata": {},
      "source": [
        "## üöÄ Inicializaci√≥n del Sistema de Backup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c94b935",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# INICIALIZACI√ìN DEL SISTEMA DE BACKUP - EJECUTAR ANTES DEL ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "def initialize_backup_system_complete():\n",
        "    \"\"\"Inicializar completamente el sistema de backup\"\"\"\n",
        "    print(\"üöÄ INICIALIZANDO SISTEMA DE BACKUP COMPLETO...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. Verificar y montar Google Drive\n",
        "    print(\"1Ô∏è‚É£ Verificando Google Drive...\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"‚ùå Google Drive no est√° montado. Montando...\")\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"‚úÖ Google Drive montado correctamente\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error montando Drive: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"‚úÖ Google Drive ya est√° montado\")\n",
        "    \n",
        "    # 2. Crear directorio de backup\n",
        "    print(\"\\n2Ô∏è‚É£ Creando directorio de backup...\")\n",
        "    backup_dir = Path(cascade_config.drive_backup_dir)\n",
        "    try:\n",
        "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"‚úÖ Directorio creado: {backup_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creando directorio: {e}\")\n",
        "        return False\n",
        "    \n",
        "    # 3. Verificar permisos de escritura\n",
        "    print(\"\\n3Ô∏è‚É£ Verificando permisos de escritura...\")\n",
        "    test_file = backup_dir / \"test_write.txt\"\n",
        "    try:\n",
        "        test_file.write_text(\"Test de escritura - \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        test_file.unlink()\n",
        "        print(\"‚úÖ Permisos de escritura verificados\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error de permisos: {e}\")\n",
        "        return False\n",
        "    \n",
        "    # 4. Crear archivo de control\n",
        "    print(\"\\n4Ô∏è‚É£ Creando archivo de control...\")\n",
        "    control_file = backup_dir / \"backup_control.txt\"\n",
        "    try:\n",
        "        control_file.write_text(f\"Backup inicializado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        control_file.write_text(f\"Configuraci√≥n: {cascade_config.drive_backup_period} √©pocas\\n\")\n",
        "        control_file.write_text(f\"Directorio: {cascade_config.drive_backup_dir}\\n\")\n",
        "        print(\"‚úÖ Archivo de control creado\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creando archivo de control: {e}\")\n",
        "        return False\n",
        "    \n",
        "    # 5. Mostrar informaci√≥n del sistema\n",
        "    print(\"\\n5Ô∏è‚É£ Informaci√≥n del sistema de backup:\")\n",
        "    print(f\"   üìÅ Directorio de backup: {backup_dir}\")\n",
        "    print(f\"   ‚è∞ Frecuencia de backup: cada {cascade_config.drive_backup_period} √©pocas\")\n",
        "    print(f\"   üîÑ Backup autom√°tico: HABILITADO\")\n",
        "    print(f\"   üìä Monitoreo: HABILITADO\")\n",
        "    \n",
        "    print(\"\\n‚úÖ SISTEMA DE BACKUP INICIALIZADO CORRECTAMENTE\")\n",
        "    print(\"=\" * 60)\n",
        "    return True\n",
        "\n",
        "# EJECUTAR INICIALIZACI√ìN COMPLETA\n",
        "print(\"üöÄ INICIANDO INICIALIZACI√ìN DEL SISTEMA DE BACKUP...\")\n",
        "backup_initialized = initialize_backup_system_complete()\n",
        "\n",
        "if backup_initialized:\n",
        "    print(\"\\nüéâ SISTEMA DE BACKUP LISTO PARA ENTRENAMIENTO\")\n",
        "    print(\"‚úÖ Puedes proceder con el entrenamiento\")\n",
        "else:\n",
        "    print(\"\\n‚ùå NO SE PUDO INICIALIZAR EL SISTEMA DE BACKUP\")\n",
        "    print(\"‚ö†Ô∏è Revisa la configuraci√≥n de Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cargar y Preparar Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FUNCI√ìN PARA CONFIGURAR DATOS (Unificada con HerdNet)\n",
        "# ============================================================\n",
        "\n",
        "def setup_data():\n",
        "    \"\"\"Configurar datos desde Google Drive con detecci√≥n autom√°tica de estructura\"\"\"\n",
        "    \n",
        "    # Montar Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive montado\")\n",
        "    \n",
        "    # Verificar si existe la estructura est√°ndar (igual que HerdNet)\n",
        "    if TRAIN_ANN_FILE.exists() and TRAIN_IMG_DIR.exists():\n",
        "        print(\"‚úÖ Datos encontrados (estructura est√°ndar COCO)\")\n",
        "        return True, \"standard\"\n",
        "    elif TRAIN_ANN_FILE_ALT.exists() and TRAIN_IMG_DIR_ALT.exists():\n",
        "        print(\"‚úÖ Datos encontrados (estructura alternativa groundtruth)\")\n",
        "        return True, \"groundtruth\"\n",
        "    else:\n",
        "        # Buscar en ubicaciones alternativas\n",
        "        drive_path = \"/content/drive/MyDrive\"\n",
        "        possible_paths = [\n",
        "            f\"{drive_path}/aerial-wildlife-count\",\n",
        "            f\"{drive_path}/datasets/aerial-wildlife-count\",\n",
        "            f\"{drive_path}/Colab Notebooks/aerial-wildlife-count\"\n",
        "        ]\n",
        "        \n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                print(f\"‚úÖ Dataset encontrado en ubicaci√≥n alternativa: {path}\")\n",
        "                return True, \"legacy\"\n",
        "        \n",
        "        print(\"‚ùå No se encontr√≥ el dataset en Google Drive\")\n",
        "        return False, None\n",
        "\n",
        "# Configurar datos\n",
        "success, data_type = setup_data()\n",
        "if success:\n",
        "    print(f\"üìÅ Ruta de datos configurada: {BASE_DIR}\")\n",
        "    print(f\"üìä Tipo de datos: {data_type}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Configura los datos manualmente antes de continuar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para preparar datos para MMDetection\n",
        "def prepare_mmdet_data():\n",
        "    \"\"\"Preparar y validar datos para entrenamiento MMDetection seg√∫n el tipo de datos\"\"\"\n",
        "    \n",
        "    if data_type == \"standard\":\n",
        "        # Usar rutas est√°ndar COCO\n",
        "        train_json = TRAIN_ANN_FILE\n",
        "        val_json = VAL_ANN_FILE\n",
        "        test_json = TEST_ANN_FILE\n",
        "        train_img_dir = TRAIN_IMG_DIR\n",
        "        val_img_dir = VAL_IMG_DIR\n",
        "        test_img_dir = TEST_IMG_DIR\n",
        "        \n",
        "        # Verificar que existan\n",
        "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
        "            print(\"‚ùå Faltan archivos de datos est√°ndar COCO\")\n",
        "            return None, None, None, None, None, None\n",
        "            \n",
        "    elif data_type == \"groundtruth\":\n",
        "        # Usar rutas groundtruth\n",
        "        train_json = TRAIN_ANN_FILE_ALT\n",
        "        val_json = VAL_ANN_FILE_ALT\n",
        "        test_json = TEST_ANN_FILE_ALT\n",
        "        train_img_dir = TRAIN_IMG_DIR_ALT\n",
        "        val_img_dir = VAL_IMG_DIR_ALT\n",
        "        test_img_dir = TEST_IMG_DIR_ALT\n",
        "        \n",
        "        # Verificar que existan\n",
        "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
        "            print(\"‚ùå Faltan archivos de datos groundtruth\")\n",
        "            return None, None, None, None, None, None\n",
        "            \n",
        "    else:\n",
        "        # Estructura legacy - buscar archivos JSON\n",
        "        data_root = Path(BASE_DIR)\n",
        "        json_patterns = [\n",
        "            \"**/train_*.json\",\n",
        "            \"**/val_*.json\", \n",
        "            \"**/test_*.json\",\n",
        "            \"**/*_big_size_*.json\",\n",
        "            \"**/*_subframes_*.json\"\n",
        "        ]\n",
        "        \n",
        "        datasets_found = []\n",
        "        for pattern in json_patterns:\n",
        "            for json_file in data_root.glob(pattern):\n",
        "                if json_file.is_file():\n",
        "                    datasets_found.append(json_file)\n",
        "        \n",
        "        # Buscar archivos espec√≠ficos\n",
        "        train_files = [d for d in datasets_found if 'train' in d.name.lower()]\n",
        "        val_files = [d for d in datasets_found if 'val' in d.name.lower()]\n",
        "        test_files = [d for d in datasets_found if 'test' in d.name.lower()]\n",
        "        \n",
        "        train_json = train_files[0] if train_files else None\n",
        "        val_json = val_files[0] if val_files else None\n",
        "        test_json = test_files[0] if test_files else None\n",
        "        \n",
        "        # Determinar directorios de im√°genes\n",
        "        train_img_dir = train_json.parent / \"images\" if (train_json.parent / \"images\").exists() else train_json.parent.parent / \"train\"\n",
        "        val_img_dir = val_json.parent.parent / \"val\"\n",
        "        test_img_dir = test_json.parent.parent / \"test\"\n",
        "        \n",
        "        if not all([train_json, val_json, test_json]):\n",
        "            print(\"‚ùå Faltan archivos de datos. Necesitas train, val y test JSON files.\")\n",
        "            return None, None, None, None, None, None\n",
        "    \n",
        "    print(f\"‚úÖ Archivos seleccionados:\")\n",
        "    print(f\"  Train: {train_json}\")\n",
        "    print(f\"  Val: {val_json}\")\n",
        "    print(f\"  Test: {test_json}\")\n",
        "    print(f\"  Train images: {train_img_dir}\")\n",
        "    print(f\"  Val images: {val_img_dir}\")\n",
        "    print(f\"  Test images: {test_img_dir}\")\n",
        "    \n",
        "    return train_json, val_json, test_json, train_img_dir, val_img_dir, test_img_dir\n",
        "\n",
        "# Funci√≥n para analizar un dataset COCO\n",
        "def analyze_dataset(json_path):\n",
        "    \"\"\"Analizar estad√≠sticas de un dataset COCO\"\"\"\n",
        "    \n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    print(f\"\\nüìà An√°lisis de {json_path.name}:\")\n",
        "    print(f\"  Im√°genes: {len(data['images'])}\")\n",
        "    print(f\"  Anotaciones: {len(data['annotations'])}\")\n",
        "    print(f\"  Categor√≠as: {len(data['categories'])}\")\n",
        "    \n",
        "    # Estad√≠sticas por categor√≠a\n",
        "    cat_counts = {}\n",
        "    for ann in data['annotations']:\n",
        "        cat_id = ann['category_id']\n",
        "        cat_counts[cat_id] = cat_counts.get(cat_id, 0) + 1\n",
        "    \n",
        "    print(\"\\n  Distribuci√≥n por categor√≠a:\")\n",
        "    for cat in data['categories']:\n",
        "        count = cat_counts.get(cat['id'], 0)\n",
        "        print(f\"    {cat['name']}: {count}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Preparar datos\n",
        "train_json, val_json, test_json, train_img_dir, val_img_dir, test_img_dir = prepare_mmdet_data()\n",
        "\n",
        "if train_json and val_json and test_json:\n",
        "    print(\"‚úÖ Datos preparados correctamente para MMDetection\")\n",
        "    # Analizar el dataset de entrenamiento\n",
        "    analyze_dataset(train_json)\n",
        "else:\n",
        "    print(\"‚ùå Error preparando datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Configuraci√≥n del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para construir configuraci√≥n de MMDetection\n",
        "def build_cascade_config():\n",
        "    \"\"\"Construir configuraci√≥n de Cascade R-CNN para MMDetection\"\"\"\n",
        "    \n",
        "    # Configuraci√≥n base seg√∫n backbone\n",
        "    if cascade_config.backbone.lower().startswith(\"swin\"):\n",
        "        base_configs = [\n",
        "            \"mmdet::_base_/models/cascade-rcnn_swin-t-p4-w7_fpn.py\",\n",
        "            \"mmdet::_base_/datasets/coco_instance.py\", \n",
        "            \"mmdet::_base_/schedules/schedule_1x.py\",\n",
        "            \"mmdet::_base_/default_runtime.py\",\n",
        "        ]\n",
        "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/swin/cascade_rcnn_swin-t-p4-w7_fpn_1x_coco/cascade_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120925-64dfb01c.pth\"\n",
        "    else:\n",
        "        base_configs = [\n",
        "            \"mmdet::_base_/models/cascade_rcnn_x101_32x4d_fpn.py\",\n",
        "            \"mmdet::_base_/datasets/coco_instance.py\",\n",
        "            \"mmdet::_base_/schedules/schedule_1x.py\", \n",
        "            \"mmdet::_base_/default_runtime.py\",\n",
        "        ]\n",
        "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco/cascade_rcnn_x101_32x4d_fpn_1x_coco_20200315-f6f0ac5e.pth\"\n",
        "    \n",
        "    config = {\n",
        "        \"_base_\": base_configs,\n",
        "        \"default_scope\": \"mmdet\",\n",
        "        \n",
        "        # Dataset configuration\n",
        "        \"dataset_type\": \"CocoDataset\",\n",
        "        \"data_root\": str(train_json.parent.parent),\n",
        "        \n",
        "        # Train dataloader\n",
        "        \"train_dataloader\": {\n",
        "            \"batch_size\": cascade_config.batch_size,\n",
        "            \"num_workers\": 4,\n",
        "            \"persistent_workers\": True,\n",
        "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": True},\n",
        "            \"dataset\": {\n",
        "                \"type\": \"CocoDataset\",\n",
        "                \"ann_file\": str(train_json),\n",
        "                \"data_prefix\": {\"img\": str(train_img_dir)},\n",
        "                \"filter_cfg\": {\"filter_empty\": True, \"min_size\": 1},\n",
        "            },\n",
        "        },\n",
        "        \n",
        "        # Validation dataloader\n",
        "        \"val_dataloader\": {\n",
        "            \"batch_size\": cascade_config.val_batch_size,\n",
        "            \"num_workers\": 2,\n",
        "            \"persistent_workers\": True,\n",
        "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
        "            \"dataset\": {\n",
        "                \"type\": \"CocoDataset\",\n",
        "                \"ann_file\": str(val_json),\n",
        "                \"data_prefix\": {\"img\": str(val_img_dir)},\n",
        "                \"test_mode\": True\n",
        "            },\n",
        "        },\n",
        "        \n",
        "        # Test dataloader\n",
        "        \"test_dataloader\": {\n",
        "            \"batch_size\": cascade_config.val_batch_size,\n",
        "            \"num_workers\": 2,\n",
        "            \"persistent_workers\": True,\n",
        "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
        "            \"dataset\": {\n",
        "                \"type\": \"CocoDataset\",\n",
        "                \"ann_file\": str(test_json),\n",
        "                \"data_prefix\": {\"img\": str(test_img_dir)},\n",
        "                \"test_mode\": True\n",
        "            },\n",
        "        },\n",
        "        \n",
        "        # Data augmentation pipelines\n",
        "        \"train_pipeline\": [\n",
        "            {\"type\": \"LoadImageFromFile\"},\n",
        "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
        "            {\"type\": \"Resize\", \"scale\": (cascade_config.imgsz, cascade_config.imgsz), \"keep_ratio\": True},\n",
        "            {\"type\": \"RandomFlip\", \"prob\": 0.5},\n",
        "            {\"type\": \"PackDetInputs\"},\n",
        "        ],\n",
        "        \"test_pipeline\": [\n",
        "            {\"type\": \"LoadImageFromFile\"},\n",
        "            {\"type\": \"Resize\", \"scale\": (cascade_config.imgsz, cascade_config.imgsz), \"keep_ratio\": True},\n",
        "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
        "            {\"type\": \"PackDetInputs\"},\n",
        "        ],\n",
        "        \n",
        "        # Optimizer configuration\n",
        "        \"optim_wrapper\": {\n",
        "            \"type\": \"OptimWrapper\",\n",
        "            \"optimizer\": {\n",
        "                \"type\": \"SGD\", \n",
        "                \"lr\": 0.01, \n",
        "                \"momentum\": 0.9, \n",
        "                \"weight_decay\": 0.0001\n",
        "            },\n",
        "            \"clip_grad\": {\"max_norm\": 0.5, \"norm_type\": 2},\n",
        "        },\n",
        "        \n",
        "        # Learning rate scheduler\n",
        "        \"param_scheduler\": [\n",
        "            {\n",
        "                \"type\": \"LinearLR\", \n",
        "                \"start_factor\": 0.001, \n",
        "                \"by_epoch\": False, \n",
        "                \"begin\": 0, \n",
        "                \"end\": 500\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"MultiStepLR\", \n",
        "                \"begin\": 0, \n",
        "                \"end\": cascade_config.epochs, \n",
        "                \"by_epoch\": True, \n",
        "                \"milestones\": [int(cascade_config.epochs*0.67), int(cascade_config.epochs*0.89)], \n",
        "                \"gamma\": 0.1\n",
        "            },\n",
        "        ],\n",
        "        \n",
        "        # Training configuration\n",
        "        \"train_cfg\": {\"max_epochs\": cascade_config.epochs, \"val_interval\": 1},\n",
        "        \"val_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(val_json), \"metric\": \"bbox\"},\n",
        "        \"test_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(test_json), \"metric\": \"bbox\"},\n",
        "        \n",
        "        # Work directory and model loading\n",
        "        \"work_dir\": str(cascade_config.work_dir),\n",
        "        \"load_from\": pretrained,\n",
        "        \"resume\": False,\n",
        "        \n",
        "        # Visualization and logging\n",
        "        \"visualizer\": {\"type\": \"DetLocalVisualizer\"},\n",
        "        \"default_hooks\": {\n",
        "            \"checkpoint\": {\n",
        "                \"type\": \"CheckpointHook\", \n",
        "                \"interval\": 1, \n",
        "                \"max_keep_ckpts\": 3, \n",
        "                \"save_best\": \"coco/bbox_mAP\"\n",
        "            },\n",
        "            \"early_stopping\": {\n",
        "                \"type\": \"EarlyStoppingHook\",\n",
        "                \"patience\": cascade_config.patience,\n",
        "                \"monitor\": \"coco/bbox_mAP\",\n",
        "                \"min_delta\": 0.001,\n",
        "                \"mode\": \"max\"\n",
        "            },\n",
        "            \"logger\": {\"type\": \"LoggerHook\", \"interval\": 50},\n",
        "            \"param_scheduler\": {\"type\": \"ParamSchedulerHook\"},\n",
        "            \"timer\": {\"type\": \"IterTimerHook\"},\n",
        "            \"sampler_seed\": {\"type\": \"DistSamplerSeedHook\"},\n",
        "        },\n",
        "        \n",
        "        # Environment configuration\n",
        "        \"env_cfg\": {\"cudnn_benchmark\": True},\n",
        "        \"randomness\": {\"seed\": 42, \"deterministic\": False}\n",
        "    }\n",
        "    \n",
        "    return Config(config)\n",
        "\n",
        "print(\"‚úÖ Funci√≥n de configuraci√≥n del modelo creada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dbb932b",
      "metadata": {},
      "source": [
        "## üõë Early Stopping Implementado\n",
        "\n",
        "### ‚úÖ **Configuraci√≥n Agregada**\n",
        "- **Patience**: 5 √©pocas (configurable en `CascadeConfig`)\n",
        "- **M√©trica monitoreada**: `coco/bbox_mAP` (mean Average Precision)\n",
        "- **Modo**: `max` (busca maximizar la m√©trica)\n",
        "- **Delta m√≠nimo**: 0.001 (mejora m√≠nima requerida)\n",
        "\n",
        "### üéØ **¬øC√≥mo funciona?**\n",
        "- El entrenamiento se detendr√° autom√°ticamente si la m√©trica `coco/bbox_mAP` no mejora durante **5 √©pocas consecutivas**\n",
        "- Esto evita el sobreajuste y ahorra tiempo de entrenamiento\n",
        "- El modelo se guardar√° autom√°ticamente cuando se active el early stopping\n",
        "\n",
        "### ‚öôÔ∏è **Personalizaci√≥n**\n",
        "Para cambiar el comportamiento, modifica en la clase `CascadeConfig`:\n",
        "```python\n",
        "self.patience = 10  # M√°s paciencia (10 √©pocas)\n",
        "self.patience = 3   # Menos paciencia (3 √©pocas)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear configuraci√≥n del modelo y comenzar entrenamiento\n",
        "if train_json and val_json and test_json:\n",
        "    \n",
        "    # Crear directorio de trabajo\n",
        "    workdir = Path(cascade_config.work_dir)\n",
        "    workdir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(\"üèóÔ∏è Construyendo configuraci√≥n del modelo...\")\n",
        "    \n",
        "    # Construir configuraci√≥n\n",
        "    mmdet_config = build_cascade_config()\n",
        "    \n",
        "    print(\"‚úÖ Configuraci√≥n creada\")\n",
        "    print(f\"üìÅ Directorio de trabajo: {workdir}\")\n",
        "    \n",
        "    # Guardar configuraci√≥n\n",
        "    config_path = workdir / \"config.py\"\n",
        "    mmdet_config.dump(str(config_path))\n",
        "    print(f\"üíæ Configuraci√≥n guardada en: {config_path}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No se puede continuar sin datos v√°lidos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iniciar entrenamiento V1 con backup autom√°tico\n",
        "if train_json and val_json and test_json:\n",
        "    \n",
        "    print(\"üöÄ Iniciando entrenamiento Cascade R-CNN V1 (Optimizado)...\")\n",
        "    print(\"üî• Caracter√≠sticas V1:\")\n",
        "    print(f\"  - Batch size optimizado: {cascade_config.batch_size}\")\n",
        "    print(f\"  - Val batch size optimizado: {cascade_config.val_batch_size}\")\n",
        "    print(f\"  - Workers optimizados: {cascade_config.workers}\")\n",
        "    print(f\"  - Guardado cada {cascade_config.save_period} √©pocas\")\n",
        "    print(f\"  - Backup en Drive cada {cascade_config.drive_backup_period} √©pocas\")\n",
        "    print(f\"  - Recuperaci√≥n autom√°tica: {cascade_config.resume_training}\")\n",
        "    print(f\"‚è±Ô∏è  Tiempo estimado: {cascade_config.epochs * 5} minutos (aproximado)\")\n",
        "    \n",
        "    try:\n",
        "        # Verificar si hay checkpoint para reanudar\n",
        "        resume_path = None\n",
        "        if cascade_config.resume_training:\n",
        "            # Buscar en Colab primero\n",
        "            work_dir = Path(cascade_config.work_dir)\n",
        "            if work_dir.exists():\n",
        "                checkpoints = list(work_dir.glob(\"*.pth\"))\n",
        "                if checkpoints:\n",
        "                    resume_path = str(max(checkpoints, key=lambda x: x.stat().st_mtime))\n",
        "                    print(f\"üîÑ Reanudando desde checkpoint local: {resume_path}\")\n",
        "            else:\n",
        "                # Buscar en Drive\n",
        "                resume_path = resume_training_from_drive()\n",
        "                if resume_path:\n",
        "                    print(f\"üîÑ Reanudando desde checkpoint en Drive: {resume_path}\")\n",
        "        \n",
        "        # Configurar reanudaci√≥n si es necesario\n",
        "        if resume_path:\n",
        "            mmdet_config.resume = True\n",
        "            mmdet_config.load_from = resume_path\n",
        "        \n",
        "        # Crear runner\n",
        "        runner = Runner.from_cfg(mmdet_config)\n",
        "        \n",
        "        # Iniciar backup autom√°tico en segundo plano\n",
        "        print(\"üîÑ Iniciando backup autom√°tico en segundo plano...\")\n",
        "        start_auto_backup()\n",
        "        \n",
        "        # Iniciar entrenamiento\n",
        "        print(\"üöÄ Iniciando entrenamiento...\")\n",
        "        runner.train()\n",
        "        \n",
        "        # Detener backup autom√°tico\n",
        "        stop_auto_backup()\n",
        "        \n",
        "        print(\"‚úÖ Entrenamiento completado exitosamente!\")\n",
        "        print(f\"üìÅ Resultados guardados en: {cascade_config.work_dir}\")\n",
        "        \n",
        "        # Hacer backup final\n",
        "        print(\"üîÑ Realizando backup final...\")\n",
        "        backup_to_drive(epoch=\"final\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Detener backup autom√°tico en caso de error\n",
        "        stop_auto_backup()\n",
        "        print(f\"‚ùå Error durante el entrenamiento: {e}\")\n",
        "        print(\"üí° Verifica que los datos est√©n correctamente configurados\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No se puede entrenar sin datos v√°lidos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79356e2c",
      "metadata": {},
      "source": [
        "## üöÄ Instrucciones de Uso - V1 Optimizado\n",
        "\n",
        "### üìã **Antes de Ejecutar:**\n",
        "1. **Montar Google Drive** (se hace autom√°ticamente)\n",
        "2. **Verificar que tienes Colab Pro/Pro+** para mejor rendimiento\n",
        "3. **Configurar datos** en la estructura correcta\n",
        "\n",
        "### üî• **Caracter√≠sticas V1:**\n",
        "- **Backup autom√°tico**: Los checkpoints se guardan en Drive cada 5 √©pocas\n",
        "- **Recuperaci√≥n autom√°tica**: Si se interrumpe, reanuda desde el √∫ltimo checkpoint\n",
        "- **Configuraci√≥n optimizada**: Batch size 8, workers 8 para mayor velocidad\n",
        "- **Monitoreo en tiempo real**: Usa `show_training_status()` para ver progreso\n",
        "\n",
        "### üìä **Comandos √ötiles Durante el Entrenamiento:**\n",
        "```python\n",
        "# Ver estado del entrenamiento\n",
        "show_training_status()\n",
        "\n",
        "# Hacer backup manual\n",
        "backup_to_drive()\n",
        "\n",
        "# Verificar backups en Drive\n",
        "monitor_training_progress()\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è **Importante:**\n",
        "- **NO cierres la pesta√±a** de Colab durante el entrenamiento\n",
        "- **Los backups se hacen autom√°ticamente** cada 5 √©pocas\n",
        "- **Si se interrumpe**, simplemente ejecuta de nuevo y reanudar√° autom√°ticamente\n",
        "- **Los checkpoints se guardan en**: `/content/drive/MyDrive/aerial-wildlife-count-results/cascade_rcnn_v1/`\n",
        "\n",
        "### üéØ **Optimizaciones de Velocidad:**\n",
        "- **Batch size**: 8 (vs 4 original)\n",
        "- **Val batch size**: 4 (vs 2 original)\n",
        "- **Workers**: 8 (vs 4 original)\n",
        "- **Save period**: 1 √©poca (vs 10 original)\n",
        "- **Early stopping**: 5 √©pocas de paciencia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Evaluaci√≥n y Visualizaci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä C√°lculo de M√©tricas de Clasificaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mmdet_classification_metrics(log_file_path, classes):\n",
        "    \"\"\"\n",
        "    Calcular m√©tricas de clasificaci√≥n para MMDetection\n",
        "    \n",
        "    Args:\n",
        "        log_file_path: Ruta al archivo .log.json de MMDetection\n",
        "        classes: Lista de nombres de clases\n",
        "    \n",
        "    Returns:\n",
        "        dict: M√©tricas por clase y generales\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    \n",
        "    log_file = Path(log_file_path)\n",
        "    \n",
        "    if not log_file.exists():\n",
        "        print(f\"‚ùå No se encontr√≥ archivo de log: {log_file}\")\n",
        "        return None\n",
        "    \n",
        "    # Leer todas las l√≠neas del log\n",
        "    with open(log_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # Buscar la √∫ltima evaluaci√≥n (modo 'val')\n",
        "    val_metrics = None\n",
        "    for line in reversed(lines):\n",
        "        try:\n",
        "            data = json.loads(line.strip())\n",
        "            if data.get('mode') == 'val':\n",
        "                val_metrics = data\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if not val_metrics:\n",
        "        print(\"‚ùå No se encontraron m√©tricas de validaci√≥n en el log\")\n",
        "        return None\n",
        "    \n",
        "    metrics = {\n",
        "        'per_class': {},\n",
        "        'overall': {}\n",
        "    }\n",
        "    \n",
        "    # M√©tricas generales\n",
        "    metrics['overall']['bbox_mAP'] = val_metrics.get('coco/bbox_mAP', 'N/A')\n",
        "    metrics['overall']['bbox_mAP_50'] = val_metrics.get('coco/bbox_mAP_50', 'N/A')\n",
        "    metrics['overall']['bbox_mAP_75'] = val_metrics.get('coco/bbox_mAP_75', 'N/A')\n",
        "    \n",
        "    # M√©tricas por clase (si est√°n disponibles)\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_key = f'coco/bbox_mAP_{class_name}'\n",
        "        if class_key in val_metrics:\n",
        "            metrics['per_class'][class_name] = {\n",
        "                'mAP': val_metrics[class_key]\n",
        "            }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def print_mmdet_classification_metrics(metrics):\n",
        "    \"\"\"Imprimir m√©tricas de clasificaci√≥n de MMDetection\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"M√âTRICAS DE CLASIFICACI√ìN - MMDetection\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if 'per_class' in metrics and metrics['per_class']:\n",
        "        print(\"\\nüìä M√âTRICAS POR CLASE:\")\n",
        "        print(\"-\" * 80)\n",
        "        for class_name, class_metrics in metrics['per_class'].items():\n",
        "            print(f\"\\n{class_name}:\")\n",
        "            for metric_name, value in class_metrics.items():\n",
        "                print(f\"  {metric_name}: {value:.4f}\" if isinstance(value, float) else f\"  {metric_name}: {value}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä M√âTRICAS GENERALES:\")\n",
        "    print(\"-\" * 80)\n",
        "    for metric_name, value in metrics['overall'].items():\n",
        "        print(f\"{metric_name.upper():20s}: {value:.4f}\" if isinstance(value, float) else f\"{metric_name.upper():20s}: {value}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "def save_mmdet_classification_metrics(metrics, save_dir):\n",
        "    \"\"\"Guardar m√©tricas de MMDetection\"\"\"\n",
        "    import json\n",
        "    import pandas as pd\n",
        "    from pathlib import Path\n",
        "    \n",
        "    save_dir = Path(save_dir)\n",
        "    \n",
        "    # Guardar como JSON\n",
        "    json_path = save_dir / \"classification_metrics.json\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(f\"‚úÖ M√©tricas guardadas en JSON: {json_path}\")\n",
        "    \n",
        "    # Guardar m√©tricas generales como CSV\n",
        "    overall_df = pd.DataFrame([metrics['overall']])\n",
        "    csv_overall_path = save_dir / \"classification_metrics_overall.csv\"\n",
        "    overall_df.to_csv(csv_overall_path, index=False)\n",
        "    print(f\"‚úÖ M√©tricas generales guardadas en CSV: {csv_overall_path}\")\n",
        "    \n",
        "    if metrics.get('per_class'):\n",
        "        per_class_df = pd.DataFrame(metrics['per_class']).T\n",
        "        csv_path = save_dir / \"classification_metrics_per_class.csv\"\n",
        "        per_class_df.to_csv(csv_path)\n",
        "        print(f\"‚úÖ M√©tricas por clase guardadas en CSV: {csv_path}\")\n",
        "\n",
        "print(\"‚úÖ Funciones de m√©tricas de clasificaci√≥n MMDetection cargadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular y mostrar m√©tricas\n",
        "work_dir = Path(cascade_config.work_dir)\n",
        "log_files = list(work_dir.glob(\"*.log.json\"))\n",
        "\n",
        "if log_files:\n",
        "    latest_log = max(log_files, key=lambda x: x.stat().st_mtime)\n",
        "    print(f\"üîç Calculando m√©tricas desde: {latest_log.name}\")\n",
        "    metrics = calculate_mmdet_classification_metrics(latest_log, cascade_config.classes)\n",
        "    if metrics:\n",
        "        print_mmdet_classification_metrics(metrics)\n",
        "        save_mmdet_classification_metrics(metrics, work_dir)\n",
        "else:\n",
        "    print(\"‚ùå No se encontraron archivos de log\")\n",
        "    print(\"üí° Ejecuta el entrenamiento primero para generar m√©tricas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para evaluar el modelo entrenado\n",
        "def evaluate_model():\n",
        "    \"\"\"Evaluar el modelo en el conjunto de test\"\"\"\n",
        "    \n",
        "    # Buscar el mejor checkpoint\n",
        "    checkpoint_dir = Path(cascade_config.work_dir)\n",
        "    checkpoints = list(checkpoint_dir.glob(\"*.pth\"))\n",
        "    \n",
        "    if not checkpoints:\n",
        "        print(\"‚ùå No se encontraron checkpoints\")\n",
        "        return None\n",
        "    \n",
        "    # Usar el √∫ltimo checkpoint o el mejor\n",
        "    best_checkpoint = None\n",
        "    for ckpt in checkpoints:\n",
        "        if \"best\" in ckpt.name:\n",
        "            best_checkpoint = ckpt\n",
        "            break\n",
        "    \n",
        "    if not best_checkpoint:\n",
        "        best_checkpoint = sorted(checkpoints)[-1]  # √öltimo checkpoint\n",
        "    \n",
        "    print(f\"üìä Evaluando con checkpoint: {best_checkpoint}\")\n",
        "    \n",
        "    # Cargar modelo\n",
        "    model = init_detector(str(config_path), str(best_checkpoint), device=cascade_config.device)\n",
        "    \n",
        "    # Evaluar en test\n",
        "    test_results = model.test(\n",
        "        data=mmdet_config.test_dataloader.dataset,\n",
        "        metric='bbox'\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Evaluaci√≥n completada\")\n",
        "    return test_results, best_checkpoint\n",
        "\n",
        "# Ejecutar evaluaci√≥n si el entrenamiento fue exitoso\n",
        "if train_json and val_json and test_json:\n",
        "    try:\n",
        "        test_results, best_ckpt = evaluate_model()\n",
        "        print(f\"üéØ Mejor checkpoint: {best_ckpt}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en evaluaci√≥n: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para visualizar predicciones\n",
        "def visualize_predictions(model, image_path, save_path=None):\n",
        "    \"\"\"Visualizar predicciones en una imagen\"\"\"\n",
        "    \n",
        "    # Realizar inferencia\n",
        "    result = inference_detector(model, image_path)\n",
        "    \n",
        "    # Visualizar\n",
        "    from mmdet.visualization import DetLocalVisualizer\n",
        "    visualizer = DetLocalVisualizer()\n",
        "    \n",
        "    # Cargar imagen\n",
        "    img = cv2.imread(str(image_path))\n",
        "    \n",
        "    # Mostrar predicciones\n",
        "    visualizer.add_datasample(\n",
        "        'result',\n",
        "        img,\n",
        "        data_sample=result,\n",
        "        draw_gt=False,\n",
        "        wait_time=0,\n",
        "        out_file=save_path\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Funci√≥n para probar el modelo en im√°genes de ejemplo\n",
        "def test_model_on_samples():\n",
        "    \"\"\"Probar el modelo en algunas im√°genes de ejemplo\"\"\"\n",
        "    \n",
        "    if not (train_json and val_json and test_json):\n",
        "        print(\"‚ùå No hay modelo entrenado para probar\")\n",
        "        return\n",
        "    \n",
        "    # Usar el directorio de im√°genes de test ya determinado\n",
        "    if not test_img_dir.exists():\n",
        "        print(\"‚ùå No se encontr√≥ directorio de im√°genes de test\")\n",
        "        return\n",
        "    \n",
        "    # Tomar algunas im√°genes de ejemplo\n",
        "    image_files = list(test_img_dir.glob(\"*.jpg\"))[:3]  # Primeras 3 im√°genes\n",
        "    \n",
        "    if not image_files:\n",
        "        print(\"‚ùå No se encontraron im√°genes de test\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üñºÔ∏è  Probando modelo en {len(image_files)} im√°genes...\")\n",
        "    \n",
        "    # Cargar modelo\n",
        "    try:\n",
        "        model = init_detector(str(config_path), str(best_ckpt), device=cascade_config.device)\n",
        "        \n",
        "        for i, img_path in enumerate(image_files):\n",
        "            print(f\"  Procesando imagen {i+1}: {img_path.name}\")\n",
        "            \n",
        "            # Visualizar predicciones\n",
        "            result = visualize_predictions(model, img_path)\n",
        "            \n",
        "            # Mostrar estad√≠sticas\n",
        "            if hasattr(result, 'pred_instances'):\n",
        "                num_detections = len(result.pred_instances)\n",
        "                print(f\"    Detecciones: {num_detections}\")\n",
        "        \n",
        "        print(\"‚úÖ Pruebas completadas\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error probando modelo: {e}\")\n",
        "\n",
        "# Ejecutar pruebas si hay modelo entrenado\n",
        "test_model_on_samples()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Guardar y Descargar Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para guardar resultados en Google Drive\n",
        "def save_to_drive():\n",
        "    \"\"\"Guardar resultados del entrenamiento en Google Drive\"\"\"\n",
        "    \n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"‚ùå Google Drive no est√° montado\")\n",
        "        return False\n",
        "    \n",
        "    # Crear directorio en Drive\n",
        "    drive_results_dir = \"/content/drive/MyDrive/aerial-wildlife-count-results\"\n",
        "    os.makedirs(drive_results_dir, exist_ok=True)\n",
        "    \n",
        "    # Copiar directorio de trabajo\n",
        "    import shutil\n",
        "    try:\n",
        "        shutil.copytree(cascade_config.work_dir, f\"{drive_results_dir}/cascade_rcnn_{cascade_config.backbone}\", dirs_exist_ok=True)\n",
        "        print(f\"‚úÖ Resultados guardados en: {drive_results_dir}/cascade_rcnn_{cascade_config.backbone}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error guardando en Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "# Funci√≥n para descargar archivos\n",
        "def download_results():\n",
        "    \"\"\"Descargar archivos importantes del entrenamiento\"\"\"\n",
        "    \n",
        "    workdir = Path(cascade_config.work_dir)\n",
        "    \n",
        "    # Archivos importantes a descargar\n",
        "    important_files = [\n",
        "        \"config.py\",\n",
        "        \"*.log\",\n",
        "        \"*.pth\"\n",
        "    ]\n",
        "    \n",
        "    print(\"üì• Descargando archivos importantes...\")\n",
        "    \n",
        "    for pattern in important_files:\n",
        "        files_to_download = list(workdir.glob(pattern))\n",
        "        for file_path in files_to_download:\n",
        "            if file_path.is_file():\n",
        "                print(f\"  Descargando: {file_path.name}\")\n",
        "                files.download(str(file_path))\n",
        "\n",
        "# Opciones para guardar resultados\n",
        "print(\"üíæ Opciones para guardar resultados:\")\n",
        "print(\"1. Guardar en Google Drive\")\n",
        "print(\"2. Descargar archivos\")\n",
        "print(\"3. Ambas opciones\")\n",
        "\n",
        "choice = input(\"Selecciona opci√≥n (1, 2, o 3): \").strip()\n",
        "\n",
        "if choice in [\"1\", \"3\"]:\n",
        "    save_to_drive()\n",
        "\n",
        "if choice in [\"2\", \"3\"]:\n",
        "    download_results()\n",
        "\n",
        "if choice not in [\"1\", \"2\", \"3\"]:\n",
        "    print(\"‚ùå Opci√≥n inv√°lida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ ¬°Entrenamiento Completado!\n",
        "\n",
        "### üìã Resumen del Entrenamiento\n",
        "- **Modelo**: Cascade R-CNN con backbone {cascade_config.backbone}\n",
        "- **Tama√±o de imagen**: {cascade_config.imgsz}x{cascade_config.imgsz}\n",
        "- **√âpocas**: {cascade_config.epochs}\n",
        "- **Early Stopping**: {cascade_config.patience} √©pocas de paciencia\n",
        "- **Clases detectadas**: {len(cascade_config.classes)} especies de animales\n",
        "\n",
        "### üìä Pr√≥ximos Pasos\n",
        "1. **Evaluar m√©tricas**: Revisar mAP, precision, recall\n",
        "2. **Ajustar hiperpar√°metros**: Si es necesario mejorar el rendimiento\n",
        "3. **Exportar modelo**: Convertir a ONNX o TorchScript para deployment\n",
        "4. **Probar en nuevas im√°genes**: Validar en datos no vistos\n",
        "\n",
        "### üîß Configuraci√≥n Personalizada\n",
        "Para modificar par√°metros, edita la clase `CascadeConfig` en la celda de configuraci√≥n:\n",
        "- Cambiar backbone: `\"swin_t\"` o `\"resnext\"`\n",
        "- Ajustar √©pocas: `epochs = 100`\n",
        "- Modificar tama√±o de imagen: `imgsz = 1024`\n",
        "- Cambiar batch size: `batch_size = 8`\n",
        "- Ajustar early stopping: `patience = 10` (m√°s paciencia) o `patience = 3` (menos paciencia)\n",
        "\n",
        "### üìö Recursos Adicionales\n",
        "- [Documentaci√≥n MMDetection](https://mmdetection.readthedocs.io/)\n",
        "- [Cascade R-CNN Paper](https://arxiv.org/abs/1712.00726)\n",
        "- [Swin Transformer Paper](https://arxiv.org/abs/2103.14030)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}