{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Entrenamiento Cascade R-CNN en Google Colab\n",
    "\n",
    "Este notebook entrena un modelo Cascade R-CNN con MMDetection en Google Colab.\n",
    "\n",
    "## üìã Caracter√≠sticas\n",
    "- **Modelo**: Cascade R-CNN con backbone Swin-T o ResNeXt-101\n",
    "- **Framework**: MMDetection (OpenMMLab)\n",
    "- **Detecci√≥n autom√°tica de GPU**\n",
    "- **Configuraci√≥n autom√°tica de datasets**\n",
    "- **Visualizaci√≥n de resultados**\n",
    "- **Exportaci√≥n de modelos**\n",
    "\n",
    "## üéØ Clases de Animales\n",
    "- Buffalo\n",
    "- Elephant  \n",
    "- Kob\n",
    "- Alcelaphinae\n",
    "- Warthog\n",
    "- Waterbuck\n",
    "\n",
    "## ‚öôÔ∏è Configuraci√≥n por Defecto\n",
    "- **Backbone**: Swin-T (recomendado) o ResNeXt-101\n",
    "- **Tama√±o de imagen**: 896x896\n",
    "- **√âpocas**: 48\n",
    "- **Batch size**: 4 (train), 2 (val/test)\n",
    "- **Optimizador**: SGD con momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Instalaci√≥n de Dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias principales\n",
    "%pip install -q openmim\n",
    "!mim install -q mmcv-full\n",
    "!mim install -q mmdet\n",
    "%pip install -q pyyaml opencv-python pillow tqdm matplotlib seaborn pandas\n",
    "\n",
    "# Verificar instalaci√≥n\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Importar Librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MMDetection imports\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "import mmdet\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "\n",
    "# Google Colab\n",
    "from google.colab import files, drive\n",
    "\n",
    "# Configurar matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"MMDetection version: {mmdet.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Configuraci√≥n de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas y par√°metros\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Par√°metros del modelo\n",
    "        self.backbone = \"swin_t\"  # \"swin_t\" o \"resnext\"\n",
    "        self.imgsz = 896\n",
    "        self.epochs = 48\n",
    "        self.batch_size = 4\n",
    "        self.val_batch_size = 2\n",
    "        \n",
    "        # Rutas (ajustar seg√∫n tu estructura de datos)\n",
    "        self.data_root = \"/content/aerial-wildlife-count\"\n",
    "        self.work_dir = \"/content/work_dirs/cascade_rcnn\"\n",
    "        \n",
    "        # Clases del dataset\n",
    "        self.classes = [\n",
    "            \"Buffalo\", \"Elephant\", \"Kob\", \n",
    "            \"Alcelaphinae\", \"Warthog\", \"Waterbuck\"\n",
    "        ]\n",
    "        \n",
    "        # Configuraci√≥n de GPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def print_config(self):\n",
    "        print(\"üîß Configuraci√≥n:\")\n",
    "        print(f\"  Backbone: {self.backbone}\")\n",
    "        print(f\"  Tama√±o de imagen: {self.imgsz}\")\n",
    "        print(f\"  √âpocas: {self.epochs}\")\n",
    "        print(f\"  Batch size: {self.batch_size}\")\n",
    "        print(f\"  Dispositivo: {self.device}\")\n",
    "        print(f\"  Directorio de trabajo: {self.work_dir}\")\n",
    "\n",
    "# Crear instancia de configuraci√≥n\n",
    "cfg = Config()\n",
    "cfg.print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cargar y Preparar Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE RUTAS PRINCIPALES (Adaptado para pipelines)\n",
    "# ============================================================\n",
    "\n",
    "# Definir la ruta base principal (ajustar seg√∫n el entorno: Drive o local)\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/aerial-wildlife-count\")\n",
    "\n",
    "# ============================================================\n",
    "# RUTAS DE DATOS PROCESADOS (Outputs de pipelines)\n",
    "# ============================================================\n",
    "\n",
    "# Rutas de los archivos de anotaciones finales (despu√©s de quality + augmentation)\n",
    "TRAIN_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"train_final\" / \"train_final.json\"\n",
    "VAL_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"val_final\" / \"val_final.json\"\n",
    "TEST_ANN_FILE = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"test_final\" / \"test_final.json\"\n",
    "\n",
    "# Rutas alternativas si no existe la estructura final\n",
    "TRAIN_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"train_validated.json\"\n",
    "VAL_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"val_validated.json\"\n",
    "TEST_ANN_FILE_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"test_validated.json\"\n",
    "\n",
    "# Rutas de las carpetas de im√°genes correspondientes\n",
    "TRAIN_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"train_final\" / \"images\"\n",
    "VAL_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"val_final\" / \"images\"\n",
    "TEST_IMG_DIR = BASE_DIR / \"data\" / \"outputs\" / \"mirror_clean\" / \"test_final\" / \"images\"\n",
    "\n",
    "# Rutas alternativas para im√°genes\n",
    "TRAIN_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"train\" / \"images\"\n",
    "VAL_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"val\" / \"images\"\n",
    "TEST_IMG_DIR_ALT = BASE_DIR / \"data\" / \"outputs\" / \"verified\" / \"test\" / \"images\"\n",
    "\n",
    "# ============================================================\n",
    "# FUNCI√ìN PARA CONFIGURAR DATOS (Adaptada para pipelines)\n",
    "# ============================================================\n",
    "\n",
    "def setup_data():\n",
    "    \"\"\"Configurar datos desde Google Drive con detecci√≥n autom√°tica de pipelines\"\"\"\n",
    "    \n",
    "    # Montar Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive montado\")\n",
    "    \n",
    "    # Verificar si existe la estructura de pipelines\n",
    "    if TRAIN_ANN_FILE.exists() and TRAIN_IMG_DIR.exists():\n",
    "        print(\"‚úÖ Datos de pipelines encontrados (estructura final)\")\n",
    "        cfg.data_root = str(BASE_DIR)\n",
    "        return True, \"final\"\n",
    "    elif TRAIN_ANN_FILE_ALT.exists() and TRAIN_IMG_DIR_ALT.exists():\n",
    "        print(\"‚úÖ Datos de pipelines encontrados (estructura verificada)\")\n",
    "        cfg.data_root = str(BASE_DIR)\n",
    "        return True, \"verified\"\n",
    "    else:\n",
    "        # Buscar en ubicaciones alternativas\n",
    "        drive_path = \"/content/drive/MyDrive\"\n",
    "        possible_paths = [\n",
    "            f\"{drive_path}/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/datasets/aerial-wildlife-count\",\n",
    "            f\"{drive_path}/Colab Notebooks/aerial-wildlife-count\"\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                cfg.data_root = path\n",
    "                print(f\"‚úÖ Dataset encontrado en ubicaci√≥n alternativa: {path}\")\n",
    "                return True, \"legacy\"\n",
    "        \n",
    "        print(\"‚ùå No se encontr√≥ el dataset en Google Drive\")\n",
    "        return False, None\n",
    "\n",
    "# Configurar datos\n",
    "success, data_type = setup_data()\n",
    "if success:\n",
    "    print(f\"üìÅ Ruta de datos configurada: {cfg.data_root}\")\n",
    "    print(f\"üìä Tipo de datos: {data_type}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Configura los datos manualmente antes de continuar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para encontrar y validar datasets (Adaptada para pipelines)\n",
    "def find_datasets():\n",
    "    \"\"\"Encontrar datasets disponibles seg√∫n el tipo de datos\"\"\"\n",
    "    \n",
    "    datasets_found = []\n",
    "    \n",
    "    if data_type == \"final\":\n",
    "        # Estructura final de pipelines (augmentation + quality)\n",
    "        datasets_found = [TRAIN_ANN_FILE, VAL_ANN_FILE, TEST_ANN_FILE]\n",
    "        print(\"üìä Usando datasets finales de pipelines:\")\n",
    "        print(f\"  Train: {TRAIN_ANN_FILE}\")\n",
    "        print(f\"  Val: {VAL_ANN_FILE}\")\n",
    "        print(f\"  Test: {TEST_ANN_FILE}\")\n",
    "        \n",
    "    elif data_type == \"verified\":\n",
    "        # Estructura verificada de quality pipeline\n",
    "        datasets_found = [TRAIN_ANN_FILE_ALT, VAL_ANN_FILE_ALT, TEST_ANN_FILE_ALT]\n",
    "        print(\"üìä Usando datasets verificados de quality pipeline:\")\n",
    "        print(f\"  Train: {TRAIN_ANN_FILE_ALT}\")\n",
    "        print(f\"  Val: {VAL_ANN_FILE_ALT}\")\n",
    "        print(f\"  Test: {TEST_ANN_FILE_ALT}\")\n",
    "        \n",
    "    else:\n",
    "        # Estructura legacy - buscar archivos JSON\n",
    "        data_root = Path(cfg.data_root)\n",
    "        json_patterns = [\n",
    "            \"**/train_*.json\",\n",
    "            \"**/val_*.json\", \n",
    "            \"**/test_*.json\",\n",
    "            \"**/*_big_size_*.json\",\n",
    "            \"**/*_subframes_*.json\",\n",
    "            \"**/*_final.json\",\n",
    "            \"**/*_validated.json\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in json_patterns:\n",
    "            for json_file in data_root.glob(pattern):\n",
    "                if json_file.is_file():\n",
    "                    datasets_found.append(json_file)\n",
    "        \n",
    "        print(f\"üìä Datasets encontrados en estructura legacy ({len(datasets_found)}):\")\n",
    "        for i, dataset in enumerate(datasets_found):\n",
    "            print(f\"  {i+1}. {dataset}\")\n",
    "    \n",
    "    return datasets_found\n",
    "\n",
    "# Funci√≥n para analizar un dataset COCO\n",
    "def analyze_dataset(json_path):\n",
    "    \"\"\"Analizar estad√≠sticas de un dataset COCO\"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìà An√°lisis de {json_path.name}:\")\n",
    "    print(f\"  Im√°genes: {len(data['images'])}\")\n",
    "    print(f\"  Anotaciones: {len(data['annotations'])}\")\n",
    "    print(f\"  Categor√≠as: {len(data['categories'])}\")\n",
    "    \n",
    "    # Estad√≠sticas por categor√≠a\n",
    "    cat_counts = {}\n",
    "    for ann in data['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        cat_counts[cat_id] = cat_counts.get(cat_id, 0) + 1\n",
    "    \n",
    "    print(\"\\n  Distribuci√≥n por categor√≠a:\")\n",
    "    for cat in data['categories']:\n",
    "        count = cat_counts.get(cat['id'], 0)\n",
    "        print(f\"    {cat['name']}: {count}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Buscar y analizar datasets\n",
    "datasets = find_datasets()\n",
    "\n",
    "if datasets:\n",
    "    # Analizar el primer dataset encontrado como ejemplo\n",
    "    sample_data = analyze_dataset(datasets[0])\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron datasets. Verifica la estructura de directorios.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Configuraci√≥n del Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para construir configuraci√≥n de MMDetection\n",
    "def build_cascade_config(backbone, imgsz, epochs, train_json, val_json, test_json, workdir):\n",
    "    \"\"\"Construir configuraci√≥n de Cascade R-CNN para MMDetection\"\"\"\n",
    "    \n",
    "    # Configuraci√≥n base seg√∫n backbone\n",
    "    if backbone.lower().startswith(\"swin\"):\n",
    "        base_configs = [\n",
    "            \"mmdet::_base_/models/cascade-rcnn_swin-t-p4-w7_fpn.py\",\n",
    "            \"mmdet::_base_/datasets/coco_instance.py\", \n",
    "            \"mmdet::_base_/schedules/schedule_1x.py\",\n",
    "            \"mmdet::_base_/default_runtime.py\",\n",
    "        ]\n",
    "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/swin/cascade_rcnn_swin-t-p4-w7_fpn_1x_coco/cascade_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120925-64dfb01c.pth\"\n",
    "    else:\n",
    "        base_configs = [\n",
    "            \"mmdet::_base_/models/cascade_rcnn_x101_32x4d_fpn.py\",\n",
    "            \"mmdet::_base_/datasets/coco_instance.py\",\n",
    "            \"mmdet::_base_/schedules/schedule_1x.py\", \n",
    "            \"mmdet::_base_/default_runtime.py\",\n",
    "        ]\n",
    "        pretrained = \"https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco/cascade_rcnn_x101_32x4d_fpn_1x_coco_20200315-f6f0ac5e.pth\"\n",
    "    \n",
    "    # Determinar directorios de im√°genes seg√∫n el tipo de datos\n",
    "    if data_type == \"final\":\n",
    "        train_img_dir = TRAIN_IMG_DIR\n",
    "        val_img_dir = VAL_IMG_DIR\n",
    "        test_img_dir = TEST_IMG_DIR\n",
    "    elif data_type == \"verified\":\n",
    "        train_img_dir = TRAIN_IMG_DIR_ALT\n",
    "        val_img_dir = VAL_IMG_DIR_ALT\n",
    "        test_img_dir = TEST_IMG_DIR_ALT\n",
    "    else:\n",
    "        # Estructura legacy\n",
    "        train_img_dir = train_json.parent / \"images\" if (train_json.parent / \"images\").exists() else train_json.parent.parent / \"train\"\n",
    "        val_img_dir = val_json.parent.parent / \"val\"\n",
    "        test_img_dir = test_json.parent.parent / \"test\"\n",
    "    \n",
    "    config = {\n",
    "        \"_base_\": base_configs,\n",
    "        \"default_scope\": \"mmdet\",\n",
    "        \n",
    "        # Dataset configuration\n",
    "        \"dataset_type\": \"CocoDataset\",\n",
    "        \"data_root\": str(train_json.parent.parent),\n",
    "        \n",
    "        # Train dataloader\n",
    "        \"train_dataloader\": {\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "            \"num_workers\": 4,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": True},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(train_json),\n",
    "                \"data_prefix\": {\"img\": str(train_img_dir)},\n",
    "                \"filter_cfg\": {\"filter_empty\": True, \"min_size\": 1},\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Validation dataloader\n",
    "        \"val_dataloader\": {\n",
    "            \"batch_size\": cfg.val_batch_size,\n",
    "            \"num_workers\": 2,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(val_json),\n",
    "                \"data_prefix\": {\"img\": str(val_img_dir)},\n",
    "                \"test_mode\": True\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Test dataloader\n",
    "        \"test_dataloader\": {\n",
    "            \"batch_size\": cfg.val_batch_size,\n",
    "            \"num_workers\": 2,\n",
    "            \"persistent_workers\": True,\n",
    "            \"sampler\": {\"type\": \"DefaultSampler\", \"shuffle\": False},\n",
    "            \"dataset\": {\n",
    "                \"type\": \"CocoDataset\",\n",
    "                \"ann_file\": str(test_json),\n",
    "                \"data_prefix\": {\"img\": str(test_img_dir)},\n",
    "                \"test_mode\": True\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        # Data augmentation pipelines\n",
    "        \"train_pipeline\": [\n",
    "            {\"type\": \"LoadImageFromFile\"},\n",
    "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
    "            {\"type\": \"Resize\", \"scale\": (imgsz, imgsz), \"keep_ratio\": True},\n",
    "            {\"type\": \"RandomFlip\", \"prob\": 0.5},\n",
    "            {\"type\": \"PackDetInputs\"},\n",
    "        ],\n",
    "        \"test_pipeline\": [\n",
    "            {\"type\": \"LoadImageFromFile\"},\n",
    "            {\"type\": \"Resize\", \"scale\": (imgsz, imgsz), \"keep_ratio\": True},\n",
    "            {\"type\": \"LoadAnnotations\", \"with_bbox\": True},\n",
    "            {\"type\": \"PackDetInputs\"},\n",
    "        ],\n",
    "        \n",
    "        # Optimizer configuration\n",
    "        \"optim_wrapper\": {\n",
    "            \"type\": \"OptimWrapper\",\n",
    "            \"optimizer\": {\n",
    "                \"type\": \"SGD\", \n",
    "                \"lr\": 0.01, \n",
    "                \"momentum\": 0.9, \n",
    "                \"weight_decay\": 0.0001\n",
    "            },\n",
    "            \"clip_grad\": {\"max_norm\": 0.5, \"norm_type\": 2},\n",
    "        },\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        \"param_scheduler\": [\n",
    "            {\n",
    "                \"type\": \"LinearLR\", \n",
    "                \"start_factor\": 0.001, \n",
    "                \"by_epoch\": False, \n",
    "                \"begin\": 0, \n",
    "                \"end\": 500\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"MultiStepLR\", \n",
    "                \"begin\": 0, \n",
    "                \"end\": epochs, \n",
    "                \"by_epoch\": True, \n",
    "                \"milestones\": [int(epochs*0.67), int(epochs*0.89)], \n",
    "                \"gamma\": 0.1\n",
    "            },\n",
    "        ],\n",
    "        \n",
    "        # Training configuration\n",
    "        \"train_cfg\": {\"max_epochs\": epochs, \"val_interval\": 1},\n",
    "        \"val_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(val_json), \"metric\": \"bbox\"},\n",
    "        \"test_evaluator\": {\"type\": \"CocoMetric\", \"ann_file\": str(test_json), \"metric\": \"bbox\"},\n",
    "        \n",
    "        # Work directory and model loading\n",
    "        \"work_dir\": str(workdir),\n",
    "        \"load_from\": pretrained,\n",
    "        \"resume\": False,\n",
    "        \n",
    "        # Visualization and logging\n",
    "        \"visualizer\": {\"type\": \"DetLocalVisualizer\"},\n",
    "        \"default_hooks\": {\n",
    "            \"checkpoint\": {\n",
    "                \"type\": \"CheckpointHook\", \n",
    "                \"interval\": 1, \n",
    "                \"max_keep_ckpts\": 3, \n",
    "                \"save_best\": \"coco/bbox_mAP\"\n",
    "            },\n",
    "            \"logger\": {\"type\": \"LoggerHook\", \"interval\": 50},\n",
    "            \"param_scheduler\": {\"type\": \"ParamSchedulerHook\"},\n",
    "            \"timer\": {\"type\": \"IterTimerHook\"},\n",
    "            \"sampler_seed\": {\"type\": \"DistSamplerSeedHook\"},\n",
    "        },\n",
    "        \n",
    "        # Environment configuration\n",
    "        \"env_cfg\": {\"cudnn_benchmark\": True},\n",
    "        \"randomness\": {\"seed\": 42, \"deterministic\": False}\n",
    "    }\n",
    "    \n",
    "    return Config(config)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de configuraci√≥n del modelo creada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entrenamiento del Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para entrenamiento (Adaptada para pipelines)\n",
    "def prepare_training_data():\n",
    "    \"\"\"Preparar y validar datos para entrenamiento seg√∫n el tipo de datos\"\"\"\n",
    "    \n",
    "    if data_type == \"final\":\n",
    "        # Usar rutas finales de pipelines\n",
    "        train_json = TRAIN_ANN_FILE\n",
    "        val_json = VAL_ANN_FILE\n",
    "        test_json = TEST_ANN_FILE\n",
    "        \n",
    "        # Verificar que existan\n",
    "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
    "            print(\"‚ùå Faltan archivos de datos finales de pipelines\")\n",
    "            return None, None, None\n",
    "            \n",
    "    elif data_type == \"verified\":\n",
    "        # Usar rutas verificadas de quality pipeline\n",
    "        train_json = TRAIN_ANN_FILE_ALT\n",
    "        val_json = VAL_ANN_FILE_ALT\n",
    "        test_json = TEST_ANN_FILE_ALT\n",
    "        \n",
    "        # Verificar que existan\n",
    "        if not all([train_json.exists(), val_json.exists(), test_json.exists()]):\n",
    "            print(\"‚ùå Faltan archivos de datos verificados de quality pipeline\")\n",
    "            return None, None, None\n",
    "            \n",
    "    else:\n",
    "        # Estructura legacy - buscar en datasets encontrados\n",
    "        train_files = [d for d in datasets if 'train' in d.name.lower()]\n",
    "        val_files = [d for d in datasets if 'val' in d.name.lower()]\n",
    "        test_files = [d for d in datasets if 'test' in d.name.lower()]\n",
    "        \n",
    "        print(\"üìä Archivos de datos encontrados:\")\n",
    "        print(f\"  Train: {len(train_files)} archivos\")\n",
    "        print(f\"  Val: {len(val_files)} archivos\") \n",
    "        print(f\"  Test: {len(test_files)} archivos\")\n",
    "        \n",
    "        # Seleccionar archivos (usar los primeros encontrados)\n",
    "        train_json = train_files[0] if train_files else None\n",
    "        val_json = val_files[0] if val_files else None\n",
    "        test_json = test_files[0] if test_files else None\n",
    "        \n",
    "        if not all([train_json, val_json, test_json]):\n",
    "            print(\"‚ùå Faltan archivos de datos. Necesitas train, val y test JSON files.\")\n",
    "            return None, None, None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Archivos seleccionados:\")\n",
    "    print(f\"  Train: {train_json}\")\n",
    "    print(f\"  Val: {val_json}\")\n",
    "    print(f\"  Test: {test_json}\")\n",
    "    \n",
    "    return train_json, val_json, test_json\n",
    "\n",
    "# Preparar datos\n",
    "train_json, val_json, test_json = prepare_training_data()\n",
    "\n",
    "if train_json and val_json and test_json:\n",
    "    print(\"‚úÖ Datos preparados correctamente\")\n",
    "else:\n",
    "    print(\"‚ùå Error preparando datos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear configuraci√≥n del modelo y comenzar entrenamiento\n",
    "if train_json and val_json and test_json:\n",
    "    \n",
    "    # Crear directorio de trabajo\n",
    "    workdir = Path(cfg.work_dir)\n",
    "    workdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"üèóÔ∏è Construyendo configuraci√≥n del modelo...\")\n",
    "    \n",
    "    # Construir configuraci√≥n\n",
    "    mmdet_config = build_cascade_config(\n",
    "        backbone=cfg.backbone,\n",
    "        imgsz=cfg.imgsz, \n",
    "        epochs=cfg.epochs,\n",
    "        train_json=train_json,\n",
    "        val_json=val_json,\n",
    "        test_json=test_json,\n",
    "        workdir=workdir\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Configuraci√≥n creada\")\n",
    "    print(f\"üìÅ Directorio de trabajo: {workdir}\")\n",
    "    \n",
    "    # Guardar configuraci√≥n\n",
    "    config_path = workdir / \"config.py\"\n",
    "    mmdet_config.dump(str(config_path))\n",
    "    print(f\"üíæ Configuraci√≥n guardada en: {config_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se puede continuar sin datos v√°lidos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar entrenamiento\n",
    "if train_json and val_json and test_json:\n",
    "    \n",
    "    print(\"üöÄ Iniciando entrenamiento...\")\n",
    "    print(f\"‚è±Ô∏è  Tiempo estimado: {cfg.epochs * 2} minutos (aproximado)\")\n",
    "    \n",
    "    try:\n",
    "        # Crear runner\n",
    "        runner = Runner.from_cfg(mmdet_config)\n",
    "        \n",
    "        # Iniciar entrenamiento\n",
    "        runner.train()\n",
    "        \n",
    "        print(\"‚úÖ Entrenamiento completado exitosamente!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante el entrenamiento: {e}\")\n",
    "        print(\"üí° Verifica que los datos est√©n correctamente configurados\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No se puede entrenar sin datos v√°lidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluaci√≥n y Visualizaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para evaluar el modelo entrenado\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluar el modelo en el conjunto de test\"\"\"\n",
    "    \n",
    "    # Buscar el mejor checkpoint\n",
    "    checkpoint_dir = Path(cfg.work_dir)\n",
    "    checkpoints = list(checkpoint_dir.glob(\"*.pth\"))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"‚ùå No se encontraron checkpoints\")\n",
    "        return None\n",
    "    \n",
    "    # Usar el √∫ltimo checkpoint o el mejor\n",
    "    best_checkpoint = None\n",
    "    for ckpt in checkpoints:\n",
    "        if \"best\" in ckpt.name:\n",
    "            best_checkpoint = ckpt\n",
    "            break\n",
    "    \n",
    "    if not best_checkpoint:\n",
    "        best_checkpoint = sorted(checkpoints)[-1]  # √öltimo checkpoint\n",
    "    \n",
    "    print(f\"üìä Evaluando con checkpoint: {best_checkpoint}\")\n",
    "    \n",
    "    # Cargar modelo\n",
    "    model = init_detector(str(config_path), str(best_checkpoint), device=cfg.device)\n",
    "    \n",
    "    # Evaluar en test\n",
    "    test_results = model.test(\n",
    "        data=mmdet_config.test_dataloader.dataset,\n",
    "        metric='bbox'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Evaluaci√≥n completada\")\n",
    "    return test_results, best_checkpoint\n",
    "\n",
    "# Ejecutar evaluaci√≥n si el entrenamiento fue exitoso\n",
    "if train_json and val_json and test_json:\n",
    "    try:\n",
    "        test_results, best_ckpt = evaluate_model()\n",
    "        print(f\"üéØ Mejor checkpoint: {best_ckpt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en evaluaci√≥n: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para visualizar predicciones\n",
    "def visualize_predictions(model, image_path, save_path=None):\n",
    "    \"\"\"Visualizar predicciones en una imagen\"\"\"\n",
    "    \n",
    "    # Realizar inferencia\n",
    "    result = inference_detector(model, image_path)\n",
    "    \n",
    "    # Visualizar\n",
    "    from mmdet.visualization import DetLocalVisualizer\n",
    "    visualizer = DetLocalVisualizer()\n",
    "    \n",
    "    # Cargar imagen\n",
    "    img = cv2.imread(str(image_path))\n",
    "    \n",
    "    # Mostrar predicciones\n",
    "    visualizer.add_datasample(\n",
    "        'result',\n",
    "        img,\n",
    "        data_sample=result,\n",
    "        draw_gt=False,\n",
    "        wait_time=0,\n",
    "        out_file=save_path\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Funci√≥n para probar el modelo en im√°genes de ejemplo\n",
    "def test_model_on_samples():\n",
    "    \"\"\"Probar el modelo en algunas im√°genes de ejemplo\"\"\"\n",
    "    \n",
    "    if not train_json and val_json and test_json:\n",
    "        print(\"‚ùå No hay modelo entrenado para probar\")\n",
    "        return\n",
    "    \n",
    "    # Buscar algunas im√°genes de test\n",
    "    test_img_dir = test_json.parent.parent / \"test\"\n",
    "    if not test_img_dir.exists():\n",
    "        print(\"‚ùå No se encontr√≥ directorio de im√°genes de test\")\n",
    "        return\n",
    "    \n",
    "    # Tomar algunas im√°genes de ejemplo\n",
    "    image_files = list(test_img_dir.glob(\"*.jpg\"))[:3]  # Primeras 3 im√°genes\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"‚ùå No se encontraron im√°genes de test\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üñºÔ∏è  Probando modelo en {len(image_files)} im√°genes...\")\n",
    "    \n",
    "    # Cargar modelo\n",
    "    try:\n",
    "        model = init_detector(str(config_path), str(best_ckpt), device=cfg.device)\n",
    "        \n",
    "        for i, img_path in enumerate(image_files):\n",
    "            print(f\"  Procesando imagen {i+1}: {img_path.name}\")\n",
    "            \n",
    "            # Visualizar predicciones\n",
    "            result = visualize_predictions(model, img_path)\n",
    "            \n",
    "            # Mostrar estad√≠sticas\n",
    "            if hasattr(result, 'pred_instances'):\n",
    "                num_detections = len(result.pred_instances)\n",
    "                print(f\"    Detecciones: {num_detections}\")\n",
    "        \n",
    "        print(\"‚úÖ Pruebas completadas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error probando modelo: {e}\")\n",
    "\n",
    "# Ejecutar pruebas si hay modelo entrenado\n",
    "test_model_on_samples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Guardar y Descargar Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para guardar resultados en Google Drive\n",
    "def save_to_drive():\n",
    "    \"\"\"Guardar resultados del entrenamiento en Google Drive\"\"\"\n",
    "    \n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"‚ùå Google Drive no est√° montado\")\n",
    "        return False\n",
    "    \n",
    "    # Crear directorio en Drive\n",
    "    drive_results_dir = \"/content/drive/MyDrive/aerial-wildlife-count-results\"\n",
    "    os.makedirs(drive_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Copiar directorio de trabajo\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.copytree(cfg.work_dir, f\"{drive_results_dir}/cascade_rcnn_{cfg.backbone}\", dirs_exist_ok=True)\n",
    "        print(f\"‚úÖ Resultados guardados en: {drive_results_dir}/cascade_rcnn_{cfg.backbone}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error guardando en Drive: {e}\")\n",
    "        return False\n",
    "\n",
    "# Funci√≥n para descargar archivos\n",
    "def download_results():\n",
    "    \"\"\"Descargar archivos importantes del entrenamiento\"\"\"\n",
    "    \n",
    "    workdir = Path(cfg.work_dir)\n",
    "    \n",
    "    # Archivos importantes a descargar\n",
    "    important_files = [\n",
    "        \"config.py\",\n",
    "        \"*.log\",\n",
    "        \"*.pth\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üì• Descargando archivos importantes...\")\n",
    "    \n",
    "    for pattern in important_files:\n",
    "        files_to_download = list(workdir.glob(pattern))\n",
    "        for file_path in files_to_download:\n",
    "            if file_path.is_file():\n",
    "                print(f\"  Descargando: {file_path.name}\")\n",
    "                files.download(str(file_path))\n",
    "\n",
    "# Opciones para guardar resultados\n",
    "print(\"üíæ Opciones para guardar resultados:\")\n",
    "print(\"1. Guardar en Google Drive\")\n",
    "print(\"2. Descargar archivos\")\n",
    "print(\"3. Ambas opciones\")\n",
    "\n",
    "choice = input(\"Selecciona opci√≥n (1, 2, o 3): \").strip()\n",
    "\n",
    "if choice in [\"1\", \"3\"]:\n",
    "    save_to_drive()\n",
    "\n",
    "if choice in [\"2\", \"3\"]:\n",
    "    download_results()\n",
    "\n",
    "if choice not in [\"1\", \"2\", \"3\"]:\n",
    "    print(\"‚ùå Opci√≥n inv√°lida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ ¬°Entrenamiento Completado!\n",
    "\n",
    "### üìã Resumen del Entrenamiento\n",
    "- **Modelo**: Cascade R-CNN con backbone {cfg.backbone}\n",
    "- **Tama√±o de imagen**: {cfg.imgsz}x{cfg.imgsz}\n",
    "- **√âpocas**: {cfg.epochs}\n",
    "- **Clases detectadas**: {len(cfg.classes)} especies de animales\n",
    "\n",
    "### üìä Pr√≥ximos Pasos\n",
    "1. **Evaluar m√©tricas**: Revisar mAP, precision, recall\n",
    "2. **Ajustar hiperpar√°metros**: Si es necesario mejorar el rendimiento\n",
    "3. **Exportar modelo**: Convertir a ONNX o TorchScript para deployment\n",
    "4. **Probar en nuevas im√°genes**: Validar en datos no vistos\n",
    "\n",
    "### üîß Configuraci√≥n Personalizada\n",
    "Para modificar par√°metros, edita la clase `Config` en la celda de configuraci√≥n:\n",
    "- Cambiar backbone: `\"swin_t\"` o `\"resnext\"`\n",
    "- Ajustar √©pocas: `epochs = 100`\n",
    "- Modificar tama√±o de imagen: `imgsz = 1024`\n",
    "- Cambiar batch size: `batch_size = 8`\n",
    "\n",
    "### üìö Recursos Adicionales\n",
    "- [Documentaci√≥n MMDetection](https://mmdetection.readthedocs.io/)\n",
    "- [Cascade R-CNN Paper](https://arxiv.org/abs/1712.00726)\n",
    "- [Swin Transformer Paper](https://arxiv.org/abs/2103.14030)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
